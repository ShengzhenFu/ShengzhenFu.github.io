[
{
	"uri": "https://shengzhenfu.github.io/content-organisation/logo/",
	"title": "Logo placeholder",
	"tags": [],
	"description": "",
	"content": "Create a _header.md page in content folder. Its content is what you get in the logo placeholder (top left of the screen).\nTip : you can add a image, a combobox with links to other documentation\u0026hellip;. Tip 2 : look at extra static menu if you want to add links to other website in this sidebar "
},
{
	"uri": "https://shengzhenfu.github.io/java-for-beginners/",
	"title": "A Java Tutorial for Beginners",
	"tags": [],
	"description": "",
	"content": " JAVA tutorial Visit my github for you to get started with Java programming  it’s that simple.\nPlease get your self understand what is JDK and IDE (Eclipse or IEDA). Cause we will need these basic knowledge before you started this tutorial.\n Day 01 **Read** input from console and **print** to console Day 01\n  Day 02 Working with **numbers**, Day 02\n  Day 03 **Switch** statement**, Day 03\n  Day 04 **Equal** statement**, Day 04\n  "
},
{
	"uri": "https://shengzhenfu.github.io/it-infra/",
	"title": "About IT - Infrastructure",
	"tags": [],
	"description": "",
	"content": " Docs about IT infra Let\u0026rsquo;s get started to explore IT infrastructure knowledge  it’s that simple\n Reset - Mysql - password-ubuntu Reset MySQL root password in ubuntu edit config file by sudo gedit /etc/mysql/mysql.conf.d/mysqld.cnf in [mysqld] section, below \u0026lsquo;skip-external-locking\u0026rsquo; add line skip-grant-tables restart MySQL sudo systemctl restart mysql enter mysql mysql -u root -p switch to mysql database use mysql; change root password to \u0026lsquo;passwd\u0026rsquo;： UPDATE mysql.user SET authentication_string=password(\u0026#39;passwd\u0026#39;) WHERE User=\u0026#39;root\u0026#39; AND Host =\u0026#39;localhost\u0026#39;; update plugin field： UPDATE user SET plugin=\u0026#34;mysql_native_password\u0026#34;; （plugin field is used to authenticate user, if it is empty, server will use built-in method)\n  Set - Env - JDK - Maven - ubuntu Set env for JDK and Maven ubuntu Edit /etc/profile sudo vi /etc/profile add below lines to the bottom #set maven env export M2_HOME=/usr/local/apache-maven-3.6.1 export CLASSPATH=$CLASSPATH:$M2_HOME/lib export PATH=$PATH:$M2_HOME/bin #set Java environment export JAVA_HOME=/usr/lib/jdk1.8.0_191 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH activate the changes source /etc/profile /etc/profile Edit bashrc sudo vi ~/.bashrc add below lines in the front #set maven env export M2_HOME=/usr/local/apache-maven-3.\n  k8s - CentOS7 - Installation kubernetes installation on Centos7 介绍下环境，4个节点的情况如下： 节点名 IP OS 安装软件 Master 10.211.55.6 Centos7 kubeadm，kubelet，kubectl，docker Node1 10.211.55.7 Centos7 kubeadm，kubelet，kubectl，docker Node2 10.211.55.8 Centos7 kubeadm，kubelet，kubectl，docker Node3 10.211.55.9 Centos7 kubeadm，kubelet，kubectl，docker 其中kubeadm,kubectl,kubelet的版本为v1.10.0，docker的版本为1.13.1。 一.各节点前期的准备工作： 1.关闭并停用防火墙 ​ systemctl stop firewalld.service ​ systemctl disable firewalld.service 2.永久关闭SELinux ​ vim /etc/selinux/config ​ SELINUX=disabled 3.同步集群系统时间 ​ yum -y install ntp ​ *ntpdate 0.*asia.pool.ntp.org 4.重启机器 ​ reboot\n  "
},
{
	"uri": "https://shengzhenfu.github.io/java-for-beginners/day01/",
	"title": "Day 01",
	"tags": [],
	"description": "**Read** input from console and **print** to console Day 01",
	"content": " Please Install JAVA IDE and setup JDK in your computer before you begin this tutorial.If you don’t know How to at all, we strongly suggest you to train by following this Eclipse Installation for beginners. \n The following steps are here to help you to understand how to print data in console and use Scanner to read data from console.\nPre-requisites We assume you already have JDK and Eclipse installed on your computer.\nBefore start real work:\n Install Jave JDK Download Eclipse https://www.eclipse.org/downloads/ or IEDA http://www.jetbrains.com/idea/download/#section=linux and Install. Copy \u0026amp; paste below example to your Eclipse or IEDA  Before we begin  create a project in Eclipse\n create package in Eclipse\n create class (name it profile_print) in Eclipse\n Open the class in Eclipse\n  Copy below code to your class import java.util.Scanner; /* * this example is to let you know * how to read input from console * how to print it in the console * */ public class profile_print { public String name; public String country; public String hobby; public String wechat; public int age; public profile_print(String name, String country, String hobby, String wechat, int age){ this.name = name; this.country = country; this.hobby = hobby; this.wechat = wechat; this.age = age; } public void run(){ System.out.println(\u0026#34;your profile_print:\\nname: \u0026#34;+name+\u0026#34;\\ncountry: \u0026#34;+country+\u0026#34;\\nhobby: \u0026#34;+hobby+\u0026#34;\\nwechat: \u0026#34;+wechat+\u0026#34;\\nage: \u0026#34;+age); } public static void main(String[] arg){ Scanner reader = new Scanner(System.in); String name = null; String country = null; String hobby = null; String wechat = null; int age = 0; System.out.println(\u0026#34;what is your name: \u0026#34;); while(reader.hasNext()) { name = reader.nextLine(); System.out.println(\u0026#34;which country you are: \u0026#34;); country = reader.nextLine(); System.out.println(\u0026#34;what is your hobby: \u0026#34;); hobby = reader.nextLine(); System.out.println(\u0026#34;your wechat id: \u0026#34;); wechat = reader.nextLine(); System.out.println(\u0026#34;how old are you\u0026#34;); age = reader.nextInt(); break; } profile_print show = new profile_print(name, country, hobby, wechat, age); if(age \u0026gt;0 \u0026amp; age \u0026lt; 150){ show.run(); } else { System.out.println(\u0026#34;the age you input is not correct, please try again\u0026#34;); } } } Day 02 Day 02\n"
},
{
	"uri": "https://shengzhenfu.github.io/java-for-beginners/day02/",
	"title": "Day 02",
	"tags": [],
	"description": "Working with **numbers**, Day 02",
	"content": " Day 02 Working with numbers Before we begin  create a project in Eclipse\n create package in Eclipse\n create class in (name it numbers) Eclipse\n Open the class in Eclipse\n  Copy code to your class public class numbers { public static void main(String[] args) { /* * +add+ -minus- *multiply* /divide/ calculation * */ int a = 5; int b = 2; int c = -2; a += 5; //a=5, 5 + 5 = 10, a has a new value of 10 \tSystem.out.println(a); // 10 \ta *= 10; // a=10, 10 * 10 =100, a has a new value of 100 \tSystem.out.println(a); // 100 \ta /= 5; // a = 100, 100 / 5 = 20, a has new value of 20 \tSystem.out.println(a); // 20 \tSystem.out.println(b); // 2 \tSystem.out.println(b+2); // 2 + 2 =4 , b has a new value of 4 \tSystem.out.println(c); // -2 \tSystem.out.println(c+2); // -2 + 2 =0, c has a new value of 0 \t/* * mod calculation * */ System.out.println(10 % 4); // 10 mod 4 = 2 \tSystem.out.println(15 % 4); // 15 mod 4 = 3 \tSystem.out.println(16 % 4); // 16 mod 4 = 0 \t/* * increamental \u0026amp; decreamental numbers * */ System.out.println(\u0026#34;increamental \u0026amp; decreamental numbers\u0026#34;); int x =3; x++; System.out.println(x); x--; System.out.println(x); ++x; System.out.println(x); --x; System.out.println(x); int y = 5; int z = ++y; //y increased, then add to z, so z value is 6 \tSystem.out.println(y); // 6 \tSystem.out.println(z); // 6 \tint h = 3; int j = h++; // h value give to j, then h increased \tSystem.out.println(h); // 4 \tSystem.out.println(j); // 3 \t} } Day 03 Day 03\n"
},
{
	"uri": "https://shengzhenfu.github.io/java-for-beginners/day03/",
	"title": "Day 03",
	"tags": [],
	"description": "**Switch** statement**, Day 03",
	"content": " Day 03 Switch statement Before we begin  create a project in Eclipse\n create package in Eclipse\n create class (name it switcher) in Eclipse\n Open the class in Eclipse\n  Copy code to your class import java.util.Scanner; public class switcher { public int x; public char y; public String z; void switch_numbers(){ int x = 0; System.out.println(\u0026#34;please input a number \u0026lt;= 9: \u0026#34;); Scanner scan = new Scanner(System.in); while(scan.hasNext()){ x = scan.nextInt(); break; } System.out.println(\u0026#34;your input number is \u0026#34;+x); switch (x){ case 3: case 5: case 7: System.out.println(x + \u0026#34; is ODD number\u0026#34;); break; case 2: case 4: case 6: case 8: System.out.println(x+\u0026#34; is even number\u0026#34;); break; case 1: System.out.println(x+\u0026#34; is minimum number\u0026#34;); case 9: System.out.println(x+\u0026#34; is maximum number\u0026#34;);default: System.out.println(\u0026#34;your input is not in range 1 ~ 9\u0026#34;); break; } } void switch_string() { String y = \u0026#34;\u0026#34;; System.out.println(\u0026#34;Please input one string\u0026#34;); Scanner scan = new Scanner(System.in); while(scan.hasNext()) { y = scan.nextLine(); break; } System.out.println(\u0026#34;your input string is \u0026#34;+y); switch(y) { case \u0026#34;mj\u0026#34;: System.out.println(\u0026#34;mj matched Michael Jordan!\u0026#34;); break; case \u0026#34;kb\u0026#34;: System.out.println(\u0026#34;kb matched Kobe Bryant!\u0026#34;); break; case \u0026#34;kg\u0026#34;: System.out.println(\u0026#34;kg matched Kevin Garnet!\u0026#34;); break;default: System.out.println(\u0026#34;None matched !\u0026#34;); break; } } void switch_char() { char z =\u0026#39;f\u0026#39;; System.out.println(\u0026#34;please input one letter a ~ z or A ~ Z\u0026#34;); Scanner scan = new Scanner(System.in); while(scan.hasNext()) { z = scan.next().charAt(0); break; } System.out.println(\u0026#34;your input letter is \u0026#34;+z); switch(z) { case \u0026#39;m\u0026#39;: System.out.println(\u0026#34;m matched, Mondy !\u0026#34;); break; case \u0026#39;t\u0026#39;: System.out.println(\u0026#34;t matched Tuesday!\u0026#34;); break; case \u0026#39;w\u0026#39;: System.out.println(\u0026#34;w matched Wednesday!\u0026#34;); break;default: System.out.println(\u0026#34;None matched !\u0026#34;); break; } } public static void main(String[] args){ switcher swn = new switcher(); swn.switch_numbers(); // call switch_numbers function  swn.switch_string(); // call switch_string function  swn.switch_char(); // call switch_char function  } } Day 04 Day 04\n"
},
{
	"uri": "https://shengzhenfu.github.io/java-for-beginners/day04/",
	"title": "Day 04",
	"tags": [],
	"description": "**Equal** statement**, Day 04",
	"content": " Day 04 Equal statement Before we begin  create a project in Eclipse\n create package in Eclipse\n create class (name it equal) in Eclipse\n Open the class in Eclipse\n  Copy code to your class public class equal { public static void main(String[] args) { System.out.println(1 == 2); //false \tSystem.out.println(2 == 2); //true \tint a = 1; int b = 1; boolean c = a == b; // boolean c = true; \tSystem.out.println(c); //true \tSystem.out.println(a == b); //true \tSystem.out.println(\u0026#34;**********************\u0026#34;); System.out.println(1\u0026gt;2); //false \tSystem.out.println(1\u0026lt;2); // true \tSystem.out.println(2\u0026gt;=1); // true \tSystem.out.println(1!=2); //true \tSystem.out.println(\u0026#34;**********************\u0026#34;); int i = 10; System.out.println(true || ++i == 11); System.out.println(i); //10 \tSystem.out.println(false \u0026amp;\u0026amp; i++ == 11); System.out.println(i); int j = 20; System.out.println(true \u0026amp; ++j == 21); System.out.println(false | ++j == 21); System.out.println(j); System.out.println(\u0026#34;**********************\u0026#34;); System.out.println(true ^ false); //true \tSystem.out.println(\u0026#34;**********************\u0026#34;); System.out.println(!true); System.out.println(!false);\t} } Day 04 Day 04\n"
},
{
	"uri": "https://shengzhenfu.github.io/getting-start/",
	"title": "Getting started - Doc hub",
	"tags": [],
	"description": "",
	"content": " Requirements Download Hugo binary for your OS (Windows, Linux, Mac) : it’s that simple\n Installation HUGO v0.32 minimum required to use this theme The following steps are here to help you initialize your new website. If you don’t know Hugo at all, we strongly suggest you to train by following this great documentation for beginners.   Configuration When building the website, you can set a theme by using --theme option. We suggest you to edit your configuration file and set the theme by default. Example with config.toml format.   "
},
{
	"uri": "https://shengzhenfu.github.io/getting-start/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "HUGO v0.32 minimum required to use this theme The following steps are here to help you initialize your new website. If you don’t know Hugo at all, we strongly suggest you to train by following this great documentation for beginners. Installation We assume that all changes to Hugo content and customizations are going to be tracked by git (GitHub, Bitbucket etc.). Develop locally, build on remote system.\nBefore start real work:\n Initialize Hugo Install DocDock theme Configure DocDock and Hugo  Prepare empty Hugo site Create empty directory, which will be root of your Hugo project. Navigate there and let Hugo to create minimal required directory structure:\n$ hugo new site .  AFTER that, initialize this as git directory where to track further changes\n$ git init  Next, there are at least three ways to install DocDock (first recommended):\n As git submodule As git clone As direct copy (from ZIP)  Navigate to your themes folder in your Hugo site and use perform one of following scenarios.\n1. Install DocDock as git submodule DocDock will be added like a dependency repo to original project. When using CI tools like Netlify, Jenkins etc., submodule method is required, or you will get theme not found issues. Same applies when building site on remote server trough SSH.\nIf submodule is no-go, use 3rd option.\nOn your root of Hugo execute:\n$ git submodule add https://github.com/vjeantet/hugo-theme-docdock.git themes/docdock  Next initialize submodule for parent git repo:\n$ git submodule init $ git submodule update  Now you are ready to add content and customize looks. Do not change any file inside theme directory.\nIf you want to freeze changes to DocDock theme itself and use still submodules, fork private copy of DocDock and use that as submodule. When you are ready to update theme, just pull changes from origin to your private fork.\n2. Install DocDock simply as git clone This method results that files are checked out locally, but won\u0026rsquo;t be visible from parent git repo. Probably you will build site locally with hugo command and use result from public/ on your own.\n$ git clone https://github.com/vjeantet/hugo-theme-docdock.git themes/docdock  3. Install DocDock from ZIP All files from theme will be tracked inside parent repo, to update it, have to override files in theme.  download following zip and extract inside themes/.\nhttps://github.com/vjeantet/hugo-theme-docdock/archive/master.zip  Name of theme in next step will be hugo-theme-docdock-master, can rename as you wish.\nConfiguration Follow instructions here\n"
},
{
	"uri": "https://shengzhenfu.github.io/it-infra/mysql-password-reset-ubuntu/",
	"title": "Reset - Mysql - password-ubuntu",
	"tags": [],
	"description": "",
	"content": " Reset MySQL root password in ubuntu  edit config file by  sudo gedit /etc/mysql/mysql.conf.d/mysqld.cnf   in [mysqld] section, below \u0026lsquo;skip-external-locking\u0026rsquo; add line  skip-grant-tables  restart MySQL  sudo systemctl restart mysql  enter mysql  mysql -u root -p  switch to mysql database  use mysql;  change root password to \u0026lsquo;passwd\u0026rsquo;：  UPDATE mysql.user SET authentication_string=password(\u0026#39;passwd\u0026#39;) WHERE User=\u0026#39;root\u0026#39; AND Host =\u0026#39;localhost\u0026#39;;  update plugin field：  UPDATE user SET plugin=\u0026#34;mysql_native_password\u0026#34;; （plugin field is used to authenticate user, if it is empty, server will use built-in method)\n flush privileges; quit; revert back the config file /etc/mysql/mysql.conf.d/mysqld.cnf by remove or comment out the line has been added.\n start mysql again and login with new passwd\nsudo systemctl start mysql K8s installation on Centos k8s installation Centos7\n  "
},
{
	"uri": "https://shengzhenfu.github.io/it-infra/jdk_maven_env_ubuntu/",
	"title": "Set - Env - JDK - Maven - ubuntu",
	"tags": [],
	"description": "",
	"content": " Set env for JDK and Maven ubuntu  Edit /etc/profile  sudo vi /etc/profile  add below lines to the bottom  #set maven env  export M2_HOME=/usr/local/apache-maven-3.6.1 export CLASSPATH=$CLASSPATH:$M2_HOME/lib export PATH=$PATH:$M2_HOME/bin #set Java environment  export JAVA_HOME=/usr/lib/jdk1.8.0_191 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH   activate the changes  source /etc/profile /etc/profile   Edit bashrc  sudo vi ~/.bashrc  add below lines in the front  #set maven env  export M2_HOME=/usr/local/apache-maven-3.6.1 export CLASSPATH=$CLASSPATH:$M2_HOME/lib export PATH=$PATH:$M2_HOME/bin #set Java environment  export JAVA_HOME=/usr/lib/jdk1.8.0_191 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH  activate the changes  source ~/.bashrc"
},
{
	"uri": "https://shengzhenfu.github.io/it-infra/kubernetes-on-centos7/",
	"title": "k8s - CentOS7 - Installation",
	"tags": [],
	"description": "",
	"content": " kubernetes installation on Centos7 介绍下环境，4个节点的情况如下：\n   节点名 IP OS 安装软件     Master 10.211.55.6 Centos7 kubeadm，kubelet，kubectl，docker   Node1 10.211.55.7 Centos7 kubeadm，kubelet，kubectl，docker   Node2 10.211.55.8 Centos7 kubeadm，kubelet，kubectl，docker   Node3 10.211.55.9 Centos7 kubeadm，kubelet，kubectl，docker    其中kubeadm,kubectl,kubelet的版本为v1.10.0，docker的版本为1.13.1。\n一.各节点前期的准备工作：\n1.关闭并停用防火墙\n​ systemctl stop firewalld.service\n​ systemctl disable firewalld.service\n2.永久关闭SELinux\n​ vim /etc/selinux/config\n​ SELINUX=disabled\n3.同步集群系统时间\n​ yum -y install ntp\n​ *ntpdate 0.*asia.pool.ntp.org\n4.重启机器\n​ reboot\n二.软件安装与配置：\n注意⚠️：软件源按需配置，下面给出3个源，其中kubernetes yum源必须配置，docker源如果需要安装docker-ce版本则需要安装，否则最高支持1.13.1版本。\n   #阿里云yum源： wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum clean all yum makecache     #docker yum源 cat \u0026gt;\u0026gt; /etc/yum.repos.d/docker.repo \u0026lt;\u0026lt;EOF [docker-repo] name=Docker Repository baseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7 enabled=1 gpgcheck=0 EOF   #kubernetes yum源 cat \u0026gt;\u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt;EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=0 EOF    配置完源，安装软件：\n​ yum -y install docker kubeadm kubelet kubectl\n关闭SWAP\n​ swapoff -a\n启动docker并设为开机启动：\n​ systemctl start docker\n​ systemctl enable docker\n参数配置：\nkubelet的cgroup驱动参数需要和docker使用的一致，先查询下docker的cgroup驱动参数：\n​ docker info |grep cgroup\n在docker v1.13.1下，该参数默认为systemd，所以更改kubelet的配置参数：\n​ sed -i \u0026ldquo;s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g\u0026rdquo; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n载入配置，启动kubelet：\n​ systemctl daemon-reload\n​ systemctl start kubelet\n注意⚠️：在启动kubeadm之前，一定要先启动kubelet，否则会显示连接不上。\n下面开始，分别操作Master节点和Node节点：\n启动Master节点：\n​ kubeadm init \u0026ndash;kubernetes-version=1.10.0 \u0026ndash;token-ttl 0 \u0026ndash;pod-network-cidr=10.244.0.0/16\n该命令表示kubenetes集群版本号为v1.10.0，token的有效时间为0表示永久有效，容器的网络段为10.244.0.0/16，由于kubeadm安装方式只能用于建立最小可用集群，所以很多addon是没有集成的，包括网络插件，需要之后安装，但网段参数需要先行配置。\n注意⚠️*：kubenetes目前的版本较之老版本，最大区别在于核心组件都已经容器化，所以安装的过程是会自动pull镜像的，但是由于镜像基本都存放于谷歌的服务器，墙内用户是无法下载，导致安装进程卡在*[init] This often takes around a minute; or longer if the control plane images have to be pulled ，这里我提供两个思路：\n1.有个墙外的代理服务器，对docker配置代理，需修改/etc/sysconfig/docker文件，添加：\n​ *HTTP_PROXY=*http://proxy_ip:port\n​ http_proxy=$HTTP_PROXY\n重启docker：systemctl restart docker\n*2.事先下载好所有镜像，下面我给出v1.10.0版本基本安装下所需要的所有镜像（其他版本所需的镜像版本可能不同，以官方文档为准）：*\n   Master节点所需镜像：     k8s.gcr.io/kube-apiserver-amd64:v1.10.0k8s.gcr.io/kube-scheduler-amd64:v1.10.0k8s.gcr.io/kube-controller-manager-amd64:v1.10.0k8s.gcr.io/kube-proxy-amd64:v1.10.0k8s.gcr.io/etcd-amd64:3.1.12k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.8k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.8k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.8k8s.gcr.io/pause-amd64:3.1quay.io/coreos/flannel:v0.9.1-amd64 （为网络插件的镜像，这里选择flannel为网络插件）   Node节点所需镜像：   k8s.gcr.io/kube-proxy-amd64:v1.10.0k8s.gcr.io/pause-amd64:3.1quay.io/coreos/flannel:v0.9.1-amd64（为网络插件的镜像，这里选择flannel为网络插件）    Master节点安装成功会输出如下内容：\n[root@master kubelet.service.d]# kubeadm reset\n[preflight] Running pre-flight checks.\n[reset] Stopping the kubelet service.\n[reset] Unmounting mounted directories in \u0026ldquo;/var/lib/kubelet\u0026rdquo;\n[reset] Removing kubernetes-managed containers.\n[reset] Deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes /var/lib/etcd]\n[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]\n[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n[root@master kubelet.service.d]# kubeadm init \u0026ndash;kubernetes-version=1.10.1 \u0026ndash;pod-network-cidr=10.244.0.0/16 \u0026ndash;skip-preflight-checks\nFlag \u0026ndash;skip-preflight-checks has been deprecated, it is now equivalent to \u0026ndash;ignore-preflight-errors=all\n[init] Using Kubernetes version: v1.10.1\n[init] Using Authorization modes: [Node RBAC]\n[preflight] Running pre-flight checks.\n​ [WARNING Service-Kubelet]: kubelet service is not enabled, please run \u0026lsquo;systemctl enable kubelet.service\u0026rsquo;\n​ [WARNING FileExisting-crictl]: crictl not found in system path\nSuggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl\n[certificates] Generated ca certificate and key.\n[certificates] Generated apiserver certificate and key.\n[certificates] apiserver serving cert is signed for DNS names [master.kube kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.130]\n[certificates] Generated apiserver-kubelet-client certificate and key.\n[certificates] Generated etcd/ca certificate and key.\n[certificates] Generated etcd/server certificate and key.\n[certificates] etcd/server serving cert is signed for DNS names [localhost] and IPs [127.0.0.1]\n[certificates] Generated etcd/peer certificate and key.\n[certificates] etcd/peer serving cert is signed for DNS names [master.kube] and IPs [192.168.0.130]\n[certificates] Generated etcd/healthcheck-client certificate and key.\n[certificates] Generated apiserver-etcd-client certificate and key.\n[certificates] Generated sa key and public key.\n[certificates] Generated front-proxy-ca certificate and key.\n[certificates] Generated front-proxy-client certificate and key.\n[certificates] Valid certificates and keys now exist in \u0026ldquo;/etc/kubernetes/pki\u0026rdquo;\n[kubeconfig] Wrote KubeConfig file to disk: \u0026ldquo;/etc/kubernetes/admin.conf\u0026rdquo;\n[kubeconfig] Wrote KubeConfig file to disk: \u0026ldquo;/etc/kubernetes/kubelet.conf\u0026rdquo;\n[kubeconfig] Wrote KubeConfig file to disk: \u0026ldquo;/etc/kubernetes/controller-manager.conf\u0026rdquo;\n[kubeconfig] Wrote KubeConfig file to disk: \u0026ldquo;/etc/kubernetes/scheduler.conf\u0026rdquo;\n[controlplane] Wrote Static Pod manifest for component kube-apiserver to \u0026ldquo;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026rdquo;\n[controlplane] Wrote Static Pod manifest for component kube-controller-manager to \u0026ldquo;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026rdquo;\n[controlplane] Wrote Static Pod manifest for component kube-scheduler to \u0026ldquo;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026rdquo;\n[etcd] Wrote Static Pod manifest for a local etcd instance to \u0026ldquo;/etc/kubernetes/manifests/etcd.yaml\u0026rdquo;\n[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory \u0026ldquo;/etc/kubernetes/manifests\u0026rdquo;.\n[init] This might take a minute or longer if the control plane images have to be pulled.\n[apiclient] All control plane components are healthy after 55.004934 seconds\n[uploadconfig] Storing the configuration used in ConfigMap \u0026ldquo;kubeadm-config\u0026rdquo; in the \u0026ldquo;kube-system\u0026rdquo; Namespace\n[markmaster] Will mark node master.kube as master by adding a label and a taint\n[markmaster] Master master.kube tainted and labelled with key/value: node-role.kubernetes.io/master=\u0026ldquo;\u0026rdquo;\n[bootstraptoken] Using token: e9m9do.jvfdl5dti279cf59\n[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstraptoken] Creating the \u0026ldquo;cluster-info\u0026rdquo; ConfigMap in the \u0026ldquo;kube-public\u0026rdquo; namespace\n[addons] Applied essential addon: kube-dns\n[addons] Applied essential addon: kube-proxy\nYour Kubernetes master has initialized successfully!\nTo start using your cluster, you need to run the following as a regular user:\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\nYou should now deploy a pod network to the cluster.\nRun \u0026ldquo;kubectl apply -f [podnetwork].yaml\u0026rdquo; with one of the options listed at:\nhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\nYou can now join any number of machines by running the following on each node\nas root:\nkubeadm join 192.168.0.130:6443 \u0026ndash;token gzyfvx.kt9bt4tbt9rexbo8 \u0026ndash;discovery-token-ca-cert-hash sha256:8d7124791b48c73a5d232759edfa84a9ab9dceeb738db08cfd066e92d0fd7549\n其中\n​ kubeadm join 10.211.55.6:6443 \u0026ndash;token 63nuhu.quu72c0hl95hc82m \u0026ndash;discovery-token-ca-cert-hash sha256:3971ae49e7e5884bf191851096e39d8e28c0b77718bb2a413638057da66ed30a\n*是后续节点加入集群的启动命令，由于设置了\u0026ndash;token-ttl 0，所以该命令永久有效，需保存好，kubeadm token list命令可以输出token，但不能输出完整命令，需要做hash转换。*\n注意⚠️*：集群启动后要获取集群的使用权限，否则在master节点执行kubectl get nodes命令，会反馈localhost:8080* connection *refused,获取权限方法如下：*\n   Root用户： export KUBECONFIG=/etc/kubernetes/admin.conf     非Root用户： mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config    三.安装网络插件Pod:\n在成功启动Master节点后，在添加node节点之前，需要先安装网络管理插件，kubernetes可供选择的网络插件有很多，\n*如*Calico，Canal，flannel，Kube-router,Romana,Weave Net\n*各种安装教程可以参考官方文档，*点击这里\n本文选择flannel作为网络插件：\n​ vim /etc/sysctl.conf，添加以下内容\n​ net.ipv4.ip_forward=1\n​ net.bridge.bridge-nf-call-iptables=1\n​ net.bridge.bridge-nf-call-ip6tables=1\n​ 修改后，及时生效\n​ sysctl -p\n执行安装：\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml\n安装完成后，执行：\n​ kubectl get pods \u0026ndash;all-namespaces\n查看Pod的启动状态，一旦kube-dns Pod的启动状态为UP或者Running，集群就可以开始添加节点了。\n四.添加Node节点：\n启动Node节点加入集群只需启动kubelet，然后执行之前保存的命令：\n​ systemctl start kubelet\n​ kubeadm join 10.211.55.6:6443 \u0026ndash;token 63nuhu.quu72c0hl95hc82m \u0026ndash;discovery-token-ca-cert-hash sha256:3971ae49e7e5884bf191851096e39d8e28c0b77718bb2a413638057da66ed30a\n节点成功加入集群。\n*在主节点执行kubectl get nodes，验证集群状态，显示如下：*\n[root@master ~]# kubectl get nodes\nNAME STATUS ROLES AGE VERSION\nmaster Ready master 7h v1.10.0\nnode1 Ready  6h v1.10.0\nnode2 Ready  2h v1.10.0\nnode3 Ready  4h v1.10.0\nKubenetes v1.10.0 集群构建完成！\n五.Kubernetes-Dashboard（WebUI）的安装：\n和网络插件的用法一样，dashboard也是一个容器应用，同样执行安装yaml：\n​ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml\n*可以参考官方文档，点击这里。*\n安装完成后，执行：\nkubectl get pods \u0026ndash;all-namespaces\n查看Pod的启动状态，kubernetes-dashboard启动完成后，执行：\n​ kubectl proxy \u0026ndash;address=10.211.55.6 \u0026ndash;accept-hosts=\u0026lsquo;^*$\u0026rsquo;\n基本参数是address为master节点的IP，access-host如果不填，打开web页面会返回：\n​ **unauthorized\n启动后控制台输出：\n​ Starting to serve on 10.211.55.6:8001\n打开WebUI：\n​ http://10.211.55.6:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview?namespace=default\n见如下页面：\n这是需要用一个可用的ClusterRole进行登录，该账户需要有相关的集群操作权限，如果跳过，则是用默认的系统角色kubernetes-dashboard（该角色在创建该容器时生成），初始状态下该角色没有任何权限，需要在系统中进行配置，角色绑定：\n在主节点上任意位置创建一个文件xxx.yaml，名字随意：\n​ vim ClusterRoleBinding.yaml\n​ 编辑文件：\n​ kind: ClusterRoleBinding\n​ apiVersion: rbac.authorization.k8s.io/v1beta1\n​ metadata:\n​ name: kubernetes-dashboard\n​ subjects:\n​ - kind: ServiceAccount\n​ name: kubernetes-dashboard\n​ namespace: kube-system\n​ roleRef:\n​ kind: ClusterRole\n​ name: cluster-admin\n​ apiGroup: rbac.authorization.k8s.io\n保存，退出，执行该文件：\n​ kubectl create -f ClusterRoleBinding.yaml\n再次打开WebUI，成功显示集群信息：\n*注意⚠️*：给kubernetes-dashboard角色赋予cluster-admin权限仅供测试使用，本身这种方式并不安全，建议新建一个系统角色，分配有限的集群操作权限，方法如下：\n新建一个yaml文件，写入：\nkind: ClusterRole #创建集群角色\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\nname: dashboard #角色名称\nrules:\n*- apiGroups: [*\u0026ldquo;*\u0026rdquo;]\n*resources: [\u0026rdquo;**”] #所有资源\n*verbs: [\u0026ldquo;get\u0026rdquo;,* *\u0026ldquo;watch\u0026rdquo;,* *\u0026ldquo;list\u0026rdquo;,* *\u0026ldquo;create\u0026rdquo;,\u0026ldquo;proxy\u0026rdquo;,\u0026ldquo;update”]* #赋予获取，监听，列表，创建，代理，更新的权限\n*- apiGroups: [*\u0026ldquo;*\u0026rdquo;]\n*resources: [\u0026ldquo;pods*”] #容器资源单独配置（在所有资源配置的基础上）\n*verbs: [\u0026ldquo;delete*”] #提供删除权限\n\u0026mdash;\napiVersion: v1\nkind: ServiceAccount #创建ServiceAccount\nmetadata:\nname: dashboard\nnamespace: kube-system\n\u0026mdash;\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n*name: dashboard-extended*\nsubjects:\n*-*kind: ServiceAccount\n​ name: dashboard\n​ namespace: kube-system\nroleRef:\nkind: ClusterRole\n*name:*dashboard #填写cluster-admin代表开放全部权限\napiGroup: rbac.authorization.k8s.io\n执行该文件，查看角色是否生成：\n​ kubectl get serviceaccount \u0026ndash;all-namespaces\n查询该账户的密钥名：\n​ kubectl get secret -n kube-system\n根据密钥名找到token：\n​ kubectl discribe secret dashboard-token-wd9rz -n kube-system\n输出一段信息：\n将此token用于登陆WebUI即可。\n以上便是Kubeadm安装K8S v1.10.0版本的全记录，本文用于总结与梳理，参考于官方文档，如有错漏，望予指正。\nJDK Maven setup env on Ubuntu jdk maven env setup on Ubuntu\n"
},
{
	"uri": "https://shengzhenfu.github.io/content-organisation/extramenu/",
	"title": "Extra menu entries",
	"tags": [],
	"description": "",
	"content": "You can define additional menu entries in the navigation menu without any link to content.\nEdit the website configuration config.toml and add a [[menu.shortcuts]] entry for each link your want to add.\nExample from the current website, note the pre param which allows you to insert HTML code and used here to separate content\u0026rsquo;s menu from this \u0026ldquo;static\u0026rdquo; menu\n[[menu.shortcuts]] pre = \u0026quot;\u0026lt;h3\u0026gt;More\u0026lt;/h3\u0026gt;\u0026quot; name = \u0026quot;\u0026lt;i class='fa fa-github'\u0026gt;\u0026lt;/i\u0026gt; Github repo\u0026quot; identifier = \u0026quot;ds\u0026quot; url = \u0026quot;https://github.com/vjeantet/hugo-theme-docdock\u0026quot; weight = 1 [[menu.shortcuts]] name = \u0026quot;\u0026lt;i class='fa fa-bookmark'\u0026gt;\u0026lt;/i\u0026gt; Hugo Documentation\u0026quot; identifier = \u0026quot;hugodoc\u0026quot; url = \u0026quot;https://gohugo.io/\u0026quot; weight = 2   Read more about hugo and menu here\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/01-intro/",
	"title": "01 Introduction - BigData",
	"tags": [],
	"description": "",
	"content": " What is Data? The quantities, characters, or symbols on which operations are performed by a computer, which may be stored and transmitted in the form of electrical signals and recorded on magnetic, optical, or mechanical recording media.\nWhat is Big Data? Big Data is also data but with a huge size. Big Data is a term used to describe a collection of data that is huge in size and yet growing exponentially with time. In short such data is so large and complex that none of the traditional data management tools are able to store it or process it efficiently.\nIn this tutorial, you will learn,\n Examples Of Big Data Types Of Big Data Characteristics Of Big Data Advantages Of Big Data Processing  Examples Of Big Data Following are some the examples of Big Data-\nThe New York Stock Exchange generates about one terabyte of new trade data per day.\nSocial Media\nThe statistic shows that 500+terabytes of new data get ingested into the databases of social media site Facebook, every day. This data is mainly generated in terms of photo and video uploads, message exchanges, putting comments etc.\nA single Jet engine can generate 10+terabytes of data in 30 minutes of flight time. With many thousand flights per day, generation of data reaches up to many Petabytes.\nTypes Of Big Data BigData\u0026rsquo; could be found in three forms:\n Structured Unstructured Semi-structured  Structured Any data that can be stored, accessed and processed in the form of fixed format is termed as a \u0026lsquo;structured\u0026rsquo; data. Over the period of time, talent in computer science has achieved greater success in developing techniques for working with such kind of data (where the format is well known in advance) and also deriving value out of it. However, nowadays, we are foreseeing issues when a size of such data grows to a huge extent, typical sizes are being in the rage of multiple zettabytes.\nDo you know? 1021 bytes equal to 1 zettabyte or one billion terabytes forms a zettabyte.\nLooking at these figures one can easily understand why the name Big Data is given and imagine the challenges involved in its storage and processing.\nDo you know? Data stored in a relational database management system is one example of a \u0026lsquo;structured\u0026rsquo; data.\nExamples Of Structured Data\nAn \u0026lsquo;Employee\u0026rsquo; table in a database is an example of Structured Data\n   Employee_ID Employee_Name Gender Department Salary_In_lacs     2365 Rajesh Kulkarni Male Finance 650000   3398 Pratibha Joshi Female Admin 650000   7465 Shushil Roy Male Admin 500000   7500 Shubhojit Das Male Finance 500000   7699 Priya Sane Female Finance 550000    Unstructured Any data with unknown form or the structure is classified as unstructured data. In addition to the size being huge, un-structured data poses multiple challenges in terms of its processing for deriving value out of it. A typical example of unstructured data is a heterogeneous data source containing a combination of simple text files, images, videos etc. Now day organizations have wealth of data available with them but unfortunately, they don\u0026rsquo;t know how to derive value out of it since this data is in its raw form or unstructured format.\nExamples Of Un-structured Data\nThe output returned by \u0026lsquo;Google Search\u0026rsquo;\nSemi-structured Semi-structured data can contain both the forms of data. We can see semi-structured data as a structured in form but it is actually not defined with e.g. a table definition in relational DBMS. Example of semi-structured data is a data represented in an XML file.\nExamples Of Semi-structured Data\nPersonal data stored in an XML file-\nPrashant RaoMale35 Seema R.Female41 Satish ManeMale29 Subrato RoyMale26 Jeremiah J.Male35\nData Growth over the years Please note that web application data, which is unstructured, consists of log files, transaction history files etc. OLTP systems are built to work with structured data wherein data is stored in relations (tables).\nCharacteristics Of Big Data (i) Volume – The name Big Data itself is related to a size which is enormous. Size of data plays a very crucial role in determining value out of data. Also, whether a particular data can actually be considered as a Big Data or not, is dependent upon the volume of data. Hence, \u0026lsquo;Volume\u0026rsquo; is one characteristic which needs to be considered while dealing with Big Data.\n(ii) Variety – The next aspect of Big Data is its variety.\nVariety refers to heterogeneous sources and the nature of data, both structured and unstructured. During earlier days, spreadsheets and databases were the only sources of data considered by most of the applications. Nowadays, data in the form of emails, photos, videos, monitoring devices, PDFs, audio, etc. are also being considered in the analysis applications. This variety of unstructured data poses certain issues for storage, mining and analyzing data.\n(iii) Velocity – The term \u0026lsquo;velocity\u0026rsquo; refers to the speed of generation of data. How fast the data is generated and processed to meet the demands, determines real potential in the data.\nBig Data Velocity deals with the speed at which data flows in from sources like business processes, application logs, networks, and social media sites, sensors,Mobile devices, etc. The flow of data is massive and continuous.\n(iv) Variability – This refers to the inconsistency which can be shown by the data at times, thus hampering the process of being able to handle and manage the data effectively.\nBenefits of Big Data Processing Ability to process Big Data brings in multiple benefits, such as-\n - Businesses can utilize outside intelligence while taking decisions  Access to social data from search engines and sites like facebook, twitter are enabling organizations to fine tune their business strategies.\n - Improved customer service  Traditional customer feedback systems are getting replaced by new systems designed with Big Data technologies. In these new systems, Big Data and natural language processing technologies are being used to read and evaluate consumer responses.\n - Early identification of risk to the product/services, if any  Better operational efficiency   Big Data technologies can be used for creating a staging area or landing zone for new data before identifying what data should be moved to the data warehouse. In addition, such integration of Big Data technologies and data warehouse helps an organization to offload infrequently accessed data.\nSummary  Big Data is defined as data that is huge in size. Bigdata is a term used to describe a collection of data that is huge in size and yet growing exponentially with time. Examples of Big Data generation includes stock exchanges, social media sites, jet engines, etc. Big Data could be 1) Structured, 2) Unstructured, 3) Semi-structured Volume, Variety, Velocity, and Variability are few Characteristics of Bigdata Improved customer service, better operational efficiency, Better Decision Making are few advantages of Bigdata  02 Hadoop Intro 02 Hadoop Introduction\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/02-hadoop/",
	"title": "02 Hadoop-introduction",
	"tags": [],
	"description": "",
	"content": " What is Hadoop? Apache Hadoop is an open source software framework used to develop data processing applications which are executed in a distributed computing environment.\nApplications built using HADOOP are run on large data sets distributed across clusters of commodity computers. Commodity computers are cheap and widely available. These are mainly useful for achieving greater computational power at low cost.\nSimilar to data residing in a local file system of a personal computer system, in Hadoop, data resides in a distributed file system which is called as a Hadoop Distributed File system. The processing model is based on \u0026lsquo;Data Locality\u0026rsquo; concept wherein computational logic is sent to cluster nodes(server) containing data. This computational logic is nothing, but a compiled version of a program written in a high-level language such as Java. Such a program, processes data stored in Hadoop HDFS.\nDo you know? Computer cluster consists of a set of multiple processing units (storage disk + processor) which are connected to each other and acts as a single system.\nIn this tutorial, you will learn,\n Hadoop EcoSystem and Components Hadoop Architecture Features Of \u0026lsquo;Hadoop\u0026rsquo; Network Topology In Hadoop  Hadoop EcoSystem and Components Below diagram shows various components in the Hadoop ecosystem-\nApache Hadoop consists of two sub-projects –\n Hadoop MapReduce: MapReduce is a computational model and software framework for writing applications which are run on Hadoop. These MapReduce programs are capable of processing enormous data in parallel on large clusters of computation nodes. HDFS (Hadoop Distributed File System): HDFS takes care of the storage part of Hadoop applications. MapReduce applications consume data from HDFS. HDFS creates multiple replicas of data blocks and distributes them on compute nodes in a cluster. This distribution enables reliable and extremely rapid computations.  Although Hadoop is best known for MapReduce and its distributed file system- HDFS, the term is also used for a family of related projects that fall under the umbrella of distributed computing and large-scale data processing. Other Hadoop-related projects atApache include are Hive, HBase, Mahout, Sqoop, Flume, and ZooKeeper.\nHadoop Architecture\nHigh Level Hadoop Architecture\nHadoop has a Master-Slave Architecture for data storage and distributed data processing using MapReduce and HDFS methods.\nNameNode:\nNameNode represented every files and directory which is used in the namespace\nDataNode:\nDataNode helps you to manage the state of an HDFS node and allows you to interacts with the blocks\nMasterNode:\nThe master node allows you to conduct parallel processing of data using Hadoop MapReduce.\nSlave node:\nThe slave nodes are the additional machines in the Hadoop cluster which allows you to store data to conduct complex calculations. Moreover, all the slave node comes with Task Tracker and a DataNode. This allows you to synchronize the processes with the NameNode and Job Tracker respectively.\nIn Hadoop, master or slave system can be set up in the cloud or on-premise\nFeatures Of \u0026lsquo;Hadoop\u0026rsquo;\n• Suitable for Big Data Analysis\nAs Big Data tends to be distributed and unstructured in nature, HADOOP clusters are best suited for analysis of Big Data. Since it is processing logic (not the actual data) that flows to the computing nodes, less network bandwidth is consumed. This concept is called as data locality concept which helps increase the efficiency of Hadoop based applications.\n• Scalability\nHADOOP clusters can easily be scaled to any extent by adding additional cluster nodes and thus allows for the growth of Big Data. Also, scaling does not require modifications to application logic.\n• Fault Tolerance\nHADOOP ecosystem has a provision to replicate the input data on to other cluster nodes. That way, in the event of a cluster node failure, data processing can still proceed by using data stored on another cluster node.\nNetwork Topology In Hadoop Topology (Arrangment) of the network, affects the performance of the Hadoop cluster when the size of the Hadoop cluster grows. In addition to the performance, one also needs to care about the high availability and handling of failures. In order to achieve this Hadoop, cluster formation makes use of network topology.\nTypically, network bandwidth is an important factor to consider while forming any network. However, as measuring bandwidth could be difficult, in Hadoop, a network is represented as a tree and distance between nodes of this tree (number of hops) is considered as an important factor in the formation of Hadoop cluster. Here, the distance between two nodes is equal to sum of their distance to their closest common ancestor.\nHadoop cluster consists of a data center, the rack and the node which actually executes jobs. Here, data center consists of racks and rack consists of nodes. Network bandwidth available to processes varies depending upon the location of the processes. That is, the bandwidth available becomes lesser as we go away from-\n Processes on the same node Different nodes on the same rack Nodes on different racks of the same data center Nodes in different data centers  03 Hadoop Installation 03 Hadoop Installation\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/03-hadoop-on-ubuntu/",
	"title": "03 Hadoop-installation(ubuntu)",
	"tags": [],
	"description": "",
	"content": " Hadoop installation on Ubuntu  In this tutorial, we will take you through step by step process to install Apache Hadoop on a Linux box (Ubuntu). This is 2 part process\n Part 1) Download and Install Hadoop Part 2) Configure Hadoop   There are 2 Prerequisites\n You must have Ubuntu installed and running You must have Java Installed.  Part 1) Download and Install Hadoop\nStep 1) Add a Hadoop system user using below command\nsudo addgroup hadoop_\nsudo adduser \u0026ndash;ingroup hadoop_ hduser_\nEnter your password, name and other details.\nNOTE: There is a possibility of below-mentioned error in this setup and installation process.\n\u0026ldquo;hduser is not in the sudoers file. This incident will be reported.\u0026rdquo;\nThis error can be resolved by Login as a root user\nExecute the command\nsudo adduser hduser_ sudo\nRe-login as hduser_\nStep 2) Configure SSH\nIn order to manage nodes in a cluster, Hadoop requires SSH access\nFirst, switch user, enter the following command\nsu - hduser_\nThis command will create a new key.\nssh-keygen -t rsa -P \u0026ldquo;\u0026rdquo;\nEnable SSH access to local machine using this key.\ncat $HOME/.ssh/id_rsa.pub \u0026gt;\u0026gt; $HOME/.ssh/authorized_keys\nNow test SSH setup by connecting to localhost as \u0026lsquo;hduser\u0026rsquo; user.\nssh localhost\nNote: Please note, if you see below error in response to \u0026lsquo;ssh localhost\u0026rsquo;, then there is a possibility that SSH is not available on this system-\nTo resolve this -\nPurge SSH using,\nsudo apt-get purge openssh-server\nIt is good practice to purge before the start of installation\nInstall SSH using the command-\nsudo apt-get install openssh-server\nStep 3) Next step is to Download Hadoop\nSelect Stable\nSelect the tar.gz file ( not the file with src)\nOnce a download is complete, navigate to the directory containing the tar file\nEnter,\nsudo tar xzf hadoop-2.2.0.tar.gz\nNow, rename hadoop-2.2.0 as hadoop\nsudo mv hadoop-2.2.0 hadoop\nsudo chown -R hduser:hadoop hadoop\nPart 2) Configure Hadoop\nStep 1) Modify ~/.bashrc file\nAdd following lines to end of file ~/.bashrc\n#Set HADOOP_HOME export HADOOP_HOME=#Set JAVA_HOME export JAVA_HOME=# Add bin/ directory of Hadoop to PATH export PATH=$PATH:$HADOOP_HOME/bin\nNow, source this environment configuration using below command\n. ~/.bashrc\nStep 2) Configurations related to HDFS\nSet JAVA_HOME inside file $HADOOP_HOME/etc/hadoop/hadoop-env.sh\nWith\nThere are two parameters in $HADOOP_HOME/etc/hadoop/core-site.xml which need to be set-\n1. \u0026lsquo;hadoop.tmp.dir\u0026rsquo; - Used to specify a directory which will be used by Hadoop to store its data files.\n2. \u0026lsquo;fs.default.name\u0026rsquo; - This specifies the default file system.\nTo set these parameters, open core-site.xml\nsudo gedit $HADOOP_HOME/etc/hadoop/core-site.xml\nCopy below line in between tags \n hadoop.tmp.dir /app/hadoop/tmp Parent directory for other temporary directories.   fs.defaultFS  hdfs://localhost:54310 The name of the default file system.  \nNavigate to the directory $HADOOP_HOME/etc/Hadoop\nNow, create the directory mentioned in core-site.xml\nsudo mkdir -p Grant permissions to the directory\nsudo chown -R hduser:Hadoop sudo chmod 750 Step 3) Map Reduce Configuration\nBefore you begin with these configurations, lets set HADOOP_HOME path\nsudo gedit /etc/profile.d/hadoop.sh\nAnd Enter\nexport HADOOP_HOME=/home/guru99/Downloads/Hadoop\nNext enter\nsudo chmod +x /etc/profile.d/hadoop.sh\nExit the Terminal and restart again\nType echo $HADOOP_HOME. To verify the path\nNow copy files\nsudo cp $HADOOP_HOME/etc/hadoop/mapred-site.xml.template $HADOOP_HOME/etc/hadoop/mapred-site.xml\nOpen the mapred-site.xml file\nsudo gedit $HADOOP_HOME/etc/hadoop/mapred-site.xml\nAdd below lines of setting in between tags  and \n mapreduce.jobtracker.address localhost:54311 MapReduce job tracker runs at this host and port.  \nOpen $HADOOP_HOME/etc/hadoop/hdfs-site.xml as below,\nsudo gedit $HADOOP_HOME/etc/hadoop/hdfs-site.xml\nAdd below lines of setting between tags  and \n dfs.replication 1 Default block replication.   dfs.datanode.data.dir /home/hduser_/hdfs \nCreate a directory specified in above setting-\nsudo mkdir -p sudo mkdir -p /home/hduser_/hdfs\nsudo chown -R hduser:hadoop sudo chown -R hduser:hadoop /home/hduser_/hdfs\nsudo chmod 750 sudo chmod 750 /home/hduser_/hdfs\nStep 4) Before we start Hadoop for the first time, format HDFS using below command\n$HADOOP_HOME/bin/hdfs namenode -format\nStep 5) Start Hadoop single node cluster using below command\n$HADOOP_HOME/sbin/start-dfs.sh\nAn output of above command\n$HADOOP_HOME/sbin/start-yarn.sh\nUsing \u0026lsquo;jps\u0026rsquo; tool/command, verify whether all the Hadoop related processes are running or not.\nIf Hadoop has started successfully then an output of jps should show NameNode, NodeManager, ResourceManager, SecondaryNameNode, DataNode.\nStep 6) Stopping Hadoop\n$HADOOP_HOME/sbin/stop-dfs.sh\n$HADOOP_HOME/sbin/stop-yarn.sh\n04 HDFS Intro 04 HDFS Introduction\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/04-hdfs/",
	"title": "04 HDFS",
	"tags": [],
	"description": "",
	"content": " What is HDFS? HDFS is a distributed file system for storing very large data files, running on clusters of commodity hardware. It is fault tolerant, scalable, and extremely simple to expand. Hadoop comes bundled with HDFS (Hadoop Distributed File Systems).\nWhen data exceeds the capacity of storage on a single physical machine, it becomes essential to divide it across a number of separate machines. A file system that manages storage specific operations across a network of machines is called a distributed file system. HDFS is one such software.\nIn this tutorial, we will learn,\n What is HDFS? HDFS Architecture Read Operation Write Operation Access HDFS using JAVA API Access HDFS Using COMMAND-LINE INTERFACE  HDFS Architecture\nHDFS cluster primarily consists of a NameNode that manages the file system Metadata and a DataNodes that stores the actual data.\n NameNode: NameNode can be considered as a master of the system. It maintains the file system tree and the metadata for all the files and directories present in the system. Two files \u0026lsquo;Namespace image\u0026rsquo; and the \u0026lsquo;edit log\u0026rsquo; are used to store metadata information. Namenode has knowledge of all the datanodes containing data blocks for a given file, however, it does not store block locations persistently. This information is reconstructed every time from datanodes when the system starts. DataNode: DataNodes are slaves which reside on each machine in a cluster and provide the actual storage. It is responsible for serving, read and write requests for the clients.  Read/write operations in HDFS operate at a block level. Data files in HDFS are broken into block-sized chunks, which are stored as independent units. Default block-size is 64 MB.\nHDFS operates on a concept of data replication wherein multiple replicas of data blocks are created and are distributed on nodes throughout a cluster to enable high availability of data in the event of node failure.\nDo you know? A file in HDFS, which is smaller than a single block, does not occupy a block\u0026rsquo;s full storage.\nRead Operation In HDFS\nData read request is served by HDFS, NameNode, and DataNode. Let\u0026rsquo;s call the reader as a \u0026lsquo;client\u0026rsquo;. Below diagram depicts file read operation in Hadoop.\n A client initiates read request by calling \u0026lsquo;open()\u0026rsquo; method of FileSystem object; it is an object of type DistributedFileSystem. This object connects to namenode using RPC and gets metadata information such as the locations of the blocks of the file. Please note that these addresses are of first few blocks of a file. In response to this metadata request, addresses of the DataNodes having a copy of that block is returned back. Once addresses of DataNodes are received, an object of type FSDataInputStream is returned to the client. FSDataInputStream contains DFSInputStream which takes care of interactions with DataNode and NameNode. In step 4 shown in the above diagram, a client invokes \u0026lsquo;read()\u0026rsquo; method which causes DFSInputStream to establish a connection with the first DataNode with the first block of a file. Data is read in the form of streams wherein client invokes \u0026lsquo;read()\u0026rsquo; method repeatedly. This process of read() operation continues till it reaches the end of block. Once the end of a block is reached, DFSInputStream closes the connection and moves on to locate the next DataNode for the next block Once a client has done with the reading, it calls a close() method.  Write Operation In HDFS\nIn this section, we will understand how data is written into HDFS through files.\n A client initiates write operation by calling \u0026lsquo;create()\u0026rsquo; method of DistributedFileSystem object which creates a new file - Step no. 1 in the above diagram. DistributedFileSystem object connects to the NameNode using RPC call and initiates new file creation. However, this file creates operation does not associate any blocks with the file. It is the responsibility of NameNode to verify that the file (which is being created) does not exist already and a client has correct permissions to create a new file. If a file already exists or client does not have sufficient permission to create a new file, then IOException is thrown to the client. Otherwise, the operation succeeds and a new record for the file is created by the NameNode. Once a new record in NameNode is created, an object of type FSDataOutputStream is returned to the client. A client uses it to write data into the HDFS. Data write method is invoked (step 3 in the diagram). FSDataOutputStream contains DFSOutputStream object which looks after communication with DataNodes and NameNode. While the client continues writing data, DFSOutputStream continues creating packets with this data. These packets are enqueued into a queue which is called as DataQueue. There is one more component called DataStreamer which consumes this DataQueue. DataStreamer also asks NameNode for allocation of new blocks thereby picking desirable DataNodes to be used for replication. Now, the process of replication starts by creating a pipeline using DataNodes. In our case, we have chosen a replication level of 3 and hence there are 3 DataNodes in the pipeline. The DataStreamer pours packets into the first DataNode in the pipeline. Every DataNode in a pipeline stores packet received by it and forwards the same to the second DataNode in a pipeline. Another queue, \u0026lsquo;Ack Queue\u0026rsquo; is maintained by DFSOutputStream to store packets which are waiting for acknowledgment from DataNodes. Once acknowledgment for a packet in the queue is received from all DataNodes in the pipeline, it is removed from the \u0026lsquo;Ack Queue\u0026rsquo;. In the event of any DataNode failure, packets from this queue are used to reinitiate the operation. After a client is done with the writing data, it calls a close() method (Step 9 in the diagram) Call to close(), results into flushing remaining data packets to the pipeline followed by waiting for acknowledgment. Once a final acknowledgment is received, NameNode is contacted to tell it that the file write operation is complete.  Access HDFS using JAVA API\nIn this section, we try to understandJava interface used for accessing Hadoop\u0026rsquo;s file system.\nIn order to interact with Hadoop\u0026rsquo;s filesystem programmatically, Hadoop provides multiple JAVA classes. Package named org.apache.hadoop.fs contains classes useful in manipulation of a file in Hadoop\u0026rsquo;s filesystem. These operations include, open, read, write, and close. Actually, file API for Hadoop is generic and can be extended to interact with other filesystems other than HDFS.\nReading a file from HDFS, programmatically\nObject java.net.URL is used for reading contents of a file. To begin with, we need to make Java recognize Hadoop\u0026rsquo;s hdfs URL scheme. This is done by calling setURLStreamHandlerFactory method on URL object and an instance of FsUrlStreamHandlerFactory is passed to it. This method needs to be executed only once per JVM, hence it is enclosed in a static block.\nAn example code is-\npublic class URLCat { static { URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory()); } public static void main(String[] args) throws Exception { InputStream in = null; try { in = new URL(args[0]).openStream(); IOUtils.copyBytes(in, System.out, 4096, false); } finally { IOUtils.closeStream(in); } } }\nThis code opens and reads contents of a file. Path of this file on HDFS is passed to the program as a command line argument.\nAccess HDFS Using COMMAND-LINE INTERFACE\nThis is one of the simplest ways to interact with HDFS. Command-line interface has support for filesystem operations like read the file, create directories, moving files, deleting data, and listing directories.\nWe can run \u0026rsquo;$HADOOP_HOME/bin/hdfs dfs -help\u0026rsquo; to get detailed help on every command. Here, \u0026lsquo;dfs\u0026rsquo; is a shell command of HDFS which supports multiple subcommands.\nSome of the widely used commands are listed below along with some details of each one.\n\\1. Copy a file from the local filesystem to HDFS\n$HADOOP_HOME/bin/hdfs dfs -copyFromLocal temp.txt /\nThis command copies file temp.txt from the local filesystem to HDFS.\n\\2. We can list files present in a directory using -ls\n$HADOOP_HOME/bin/hdfs dfs -ls /\nWe can see a file \u0026lsquo;temp.txt\u0026rsquo; (copied earlier) being listed under \u0026rsquo; / \u0026lsquo; directory.\n\\3. Command to copy a file to the local filesystem from HDFS\n$HADOOP_HOME/bin/hdfs dfs -copyToLocal /temp.txt\nWe can see temp.txt copied to a local filesystem.\n\\4. Command to create a new directory\n$HADOOP_HOME/bin/hdfs dfs -mkdir /mydirectory\nCheck whether a directory is created or not. Now, you should know how to do it ;-)\n05 MapReduce Intro 05 MapReduce\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/05-mapreduce/",
	"title": "05 MapReduce",
	"tags": [],
	"description": "",
	"content": " What is MapReduce in Hadoop? MapReduce is a programming model suitable for processing of huge data. Hadoop is capable of running MapReduce programs written in various languages: Java, Ruby, Python, and C++. MapReduce programs are parallel in nature, thus are very useful for performing large-scale data analysis using multiple machines in the cluster.\nMapReduce programs work in two phases:\n Map phase Reduce phase.  An input to each phase is key-value pairs. In addition, every programmer needs to specify two functions: map function and reduce function.\nIn this beginner training, you will learn-\n What is MapReduce in Hadoop? How MapReduce Works? Complete Process MapReduce Architecture explained in detail How MapReduce Organizes Work?  How MapReduce Works? Complete Process\nThe whole process goes through four phases of execution namely, splitting, mapping, shuffling, and reducing.\nLet\u0026rsquo;s understand this with an example –\nConsider you have following input data for your Map Reduce Program\nWelcome to Hadoop Class Hadoop is good Hadoop is bad\nMapReduce Architecture\nThe final output of the MapReduce task is\n   bad 1     Class 1   good 1   Hadoop 3   is 2   to 1   Welcome 1    The data goes through the following phases\nInput Splits:\nAn input to a MapReduce job is divided into fixed-size pieces called input splits Input split is a chunk of the input that is consumed by a single map\nMapping\nThis is the very first phase in the execution of map-reduce program. In this phase data in each split is passed to a mapping function to produce output values. In our example, a job of mapping phase is to count a number of occurrences of each word from input splits (more details about input-split is given below) and prepare a list in the form of Shuffling\nThis phase consumes the output of Mapping phase. Its task is to consolidate the relevant records from Mapping phase output. In our example, the same words are clubed together along with their respective frequency.\nReducing\nIn this phase, output values from the Shuffling phase are aggregated. This phase combines values from Shuffling phase and returns a single output value. In short, this phase summarizes the complete dataset.\nIn our example, this phase aggregates the values from Shuffling phase i.e., calculates total occurrences of each word.\nMapReduce Architecture explained in detail\n One map task is created for each split which then executes map function for each record in the split. It is always beneficial to have multiple splits because the time taken to process a split is small as compared to the time taken for processing of the whole input. When the splits are smaller, the processing is better to load balanced since we are processing the splits in parallel. However, it is also not desirable to have splits too small in size. When splits are too small, the overload of managing the splits and map task creation begins to dominate the total job execution time. For most jobs, it is better to make a split size equal to the size of an HDFS block (which is 64 MB, by default). Execution of map tasks results into writing output to a local disk on the respective node and not to HDFS. Reason for choosing local disk over HDFS is, to avoid replication which takes place in case of HDFS store operation. Map output is intermediate output which is processed by reduce tasks to produce the final output. Once the job is complete, the map output can be thrown away. So, storing it in HDFS with replication becomes overkill. In the event of node failure, before the map output is consumed by the reduce task, Hadoop reruns the map task on another node and re-creates the map output. Reduce task doesn\u0026rsquo;t work on the concept of data locality. An output of every map task is fed to the reduce task. Map output is transferred to the machine where reduce task is running. On this machine, the output is merged and then passed to the user-defined reduce function. Unlike the map output, reduce output is stored in HDFS (the first replica is stored on the local node and other replicas are stored on off-rack nodes). So, writing the reduce output  How MapReduce Organizes Work?\nHadoop divides the job into tasks. There are two types of tasks:\n Map tasks (Splits \u0026amp; Mapping) Reduce tasks (Shuffling, Reducing)  as mentioned above.\nThe complete execution process (execution of Map and Reduce tasks, both) is controlled by two types of entities called a\n Jobtracker: Acts like a master (responsible for complete execution of submitted job) Multiple Task Trackers: Acts like slaves, each of them performing the job  For every job submitted for execution in the system, there is one Jobtracker that resides on Namenode and there are multiple tasktrackers which reside on Datanode.\n A job is divided into multiple tasks which are then run onto multiple data nodes in a cluster. It is the responsibility of job tracker to coordinate the activity by scheduling tasks to run on different data nodes. Execution of individual task is then to look after by task tracker, which resides on every data node executing part of the job. Task tracker\u0026rsquo;s responsibility is to send the progress report to the job tracker. In addition, task tracker periodically sends \u0026lsquo;heartbeat\u0026rsquo; signal to the Jobtracker so as to notify him of the current state of the system. Thus job tracker keeps track of the overall progress of each job. In the event of task failure, the job tracker can reschedule it on a different task tracker.  06 MapReduce example 06 MapReduce example\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/06-mapreduce-example/",
	"title": "06 MapReduce-example",
	"tags": [],
	"description": "",
	"content": " In this tutorial, you will learn to use Hadoop and MapReduce with Example. The input data used is SalesJan2009.csv. It contains Sales related information like Product name, price, payment mode, city, country of client etc. The goal is to Find out Number of Products Sold in Each Country.\nIn this tutorial, you will learn-\n First Hadoop MapReduce Program Explanation of SalesMapper Class Explanation of SalesCountryReducer Class Explanation of SalesCountryDriver Class  First Hadoop MapReduce Program\nData of SalesJan2009\nEnsure you have Hadoop installed. Before you start with the actual process, change user to \u0026lsquo;hduser\u0026rsquo; (id used while Hadoop configuration, you can switch to the userid used during your Hadoop config ).\nsu - hduser_\nStep 1)\nCreate a new directory with name MapReduceTutorial\nsudo mkdir MapReduceTutorial\nGive permissions\nsudo chmod -R 777 MapReduceTutorial\nSalesMapper.java\npackage SalesCountry; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapred.*; public class SalesMapper extends MapReduceBase implements Mapper { private final static IntWritable one = new IntWritable(1); public void map(LongWritable key, Text value, OutputCollector output, Reporter reporter) throws IOException { String valueString = value.toString(); String[] SingleCountryData = valueString.split(\u0026ldquo;,\u0026rdquo;); output.collect(new Text(SingleCountryData[7]), one); } }\nSalesCountryReducer.java\npackage SalesCountry; import java.io.IOException; import java.util.; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapred.; public class SalesCountryReducer extends MapReduceBase implements Reducer{ public void reduce(Text t_key, Iterator values, OutputCollector output, Reporter reporter) throws IOException { Text key = t_key; int frequencyForCountry = 0; while (values.hasNext()) { // replace type of value with the actual type of our value IntWritable value = (IntWritable) values.next(); frequencyForCountry += value.get(); } output.collect(key, new IntWritable(frequencyForCountry)); } }\nSalesCountryDriver.java\npackage SalesCountry; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.; import org.apache.hadoop.mapred.; public class SalesCountryDriver { public static void main(String[] args) { JobClient my_client = new JobClient(); // Create a configuration object for the job JobConf job_conf = new JobConf(SalesCountryDriver.class); // Set a name of the Job job_conf.setJobName(\u0026ldquo;SalePerCountry\u0026rdquo;); // Specify data type of output key and value job_conf.setOutputKeyClass(Text.class); job_conf.setOutputValueClass(IntWritable.class); // Specify names of Mapper and Reducer Class job_conf.setMapperClass(SalesCountry.SalesMapper.class); job_conf.setReducerClass(SalesCountry.SalesCountryReducer.class); // Specify formats of the data type of Input and output job_conf.setInputFormat(TextInputFormat.class); job_conf.setOutputFormat(TextOutputFormat.class); // Set input and output directories using command line arguments, //arg[0] = name of input directory on HDFS, and arg[1] = name of output directory to be created to store the output file. FileInputFormat.setInputPaths(job_conf, new Path(args[0])); FileOutputFormat.setOutputPath(job_conf, new Path(args[1])); my_client.setConf(job_conf); try { // Run the job JobClient.runJob(job_conf); } catch (Exception e) { e.printStackTrace(); } } }\nDownload Files Here\nCheck the file permissions of all these files\nand if \u0026lsquo;read\u0026rsquo; permissions are missing then grant the same-\nStep 2)\nExport classpath\nexport CLASSPATH=\u0026ldquo;$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.2.0.jar:$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.2.0.jar:$HADOOP_HOME/share/hadoop/common/hadoop-common-2.2.0.jar:~/MapReduceTutorial/SalesCountry/:$HADOOP_HOME/lib/\u0026ldquo;\nStep 3)\nCompileJava files (these files are present in directory Final-MapReduceHandsOn). Its class files will be put in the package directory\njavac -d . SalesMapper.java SalesCountryReducer.java SalesCountryDriver.java\nThis warning can be safely ignored.\nThis compilation will create a directory in a current directory named with package name specified in the java source file (i.e. SalesCountry in our case) and put all compiled class files in it.\nStep 4)\nCreate a new file Manifest.txt\nsudo gedit Manifest.txt\nadd following lines to it,\nMain-Class: SalesCountry.SalesCountryDriver\nSalesCountry.SalesCountryDriver is the name of main class. Please note that you have to hit enter key at end of this line.\nStep 5)\nCreate a Jar file\njar cfm ProductSalePerCountry.jar Manifest.txt SalesCountry/*.class\nCheck that the jar file is created\nStep 6)\nStart Hadoop\n$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\nStep 7)\nCopy the File SalesJan2009.csv into ~/inputMapReduce\nNow Use below command to copy ~/inputMapReduce to HDFS.\n$HADOOP_HOME/bin/hdfs dfs -copyFromLocal ~/inputMapReduce /\nWe can safely ignore this warning.\nVerify whether a file is actually copied or not.\n$HADOOP_HOME/bin/hdfs dfs -ls /inputMapReduce\nStep 8)\nRun MapReduce job\n$HADOOP_HOME/bin/hadoop jar ProductSalePerCountry.jar /inputMapReduce /mapreduce_output_sales\nThis will create an output directory named mapreduce_output_sales on HDFS. Contents of this directory will be a file containing product sales per country.\nStep 9)\nThe result can be seen through command interface as,\n$HADOOP_HOME/bin/hdfs dfs -cat /mapreduce_output_sales/part-00000\nResults can also be seen via a web interface as-\nOpen r in a web browser.\nNow select \u0026lsquo;Browse the filesystem\u0026rsquo; and navigate to /mapreduce_output_sales\nOpen part-r-00000\nExplanation of SalesMapper Class\nIn this section, we will understand the implementation of SalesMapper class.\n\\1. We begin by specifying a name of package for our class. SalesCountry is a name of our package. Please note that output of compilation, SalesMapper.class will go into a directory named by this package name: SalesCountry.\nFollowed by this, we import library packages.\nBelow snapshot shows an implementation of SalesMapper class-\nSample Code Explanation:\n1. SalesMapper Class Definition-\npublic class SalesMapper extends MapReduceBase implements Mapper{\nEvery mapper class must be extended from MapReduceBase class and it must implement Mapper interface.\n2. Defining \u0026lsquo;map\u0026rsquo; function-\npublic void map(LongWritable key, Text value, OutputCollectoroutput, Reporter reporter) throws IOException\nThe main part of Mapper class is a \u0026lsquo;map()\u0026rsquo; method which accepts four arguments.\nAt every call to \u0026lsquo;map()\u0026rsquo; method, a key-value pair (\u0026lsquo;key\u0026rsquo; and \u0026lsquo;value\u0026rsquo; in this code) is passed.\n\u0026lsquo;map()\u0026rsquo; method begins by splitting input text which is received as an argument. It uses the tokenizer to split these lines into words.\nString valueString = value.toString(); String[] SingleCountryData = valueString.split(\u0026ldquo;,\u0026rdquo;);\nHere, \u0026rsquo;,\u0026rsquo; is used as a delimiter.\nAfter this, a pair is formed using a record at 7th index of array \u0026lsquo;SingleCountryData\u0026rsquo; and a value \u0026lsquo;1\u0026rsquo;.\n​ output.collect(new Text(SingleCountryData[7]), one);\nWe are choosing record at 7th index because we need Country data and it is located at 7th index in array \u0026lsquo;SingleCountryData\u0026rsquo;.\nPlease note that our input data is in the below format (where Country is at 7th index, with 0 as a starting index)-\nTransaction_date,Product,Price,Payment_Type,Name,City,State,Country,Account_Created,Last_Login,Latitude,Longitude\nAn output of mapper is again a key-value pair which is outputted using \u0026lsquo;collect()\u0026rsquo; method of \u0026lsquo;OutputCollector\u0026rsquo;.\nExplanation of SalesCountryReducer Class\nIn this section, we will understand the implementation of SalesCountryReducer class.\n\\1. We begin by specifying a name of the package for our class. SalesCountry is a name of out package. Please note that output of compilation, SalesCountryReducer.class will go into a directory named by this package name: SalesCountry.\nFollowed by this, we import library packages.\nBelow snapshot shows an implementation of SalesCountryReducer class-\nCode Explanation:\n1. SalesCountryReducer Class Definition-\npublic class SalesCountryReducer extends MapReduceBase implements Reducer{\nHere, the first two data types, \u0026lsquo;Text\u0026rsquo; and \u0026lsquo;IntWritable\u0026rsquo; are data type of input key-value to the reducer.\nOutput of mapper is in the form of , . This output of mapper becomes input to the reducer. So, to align with its data type, Text and IntWritable are used as data type here.\nThe last two data types, \u0026lsquo;Text\u0026rsquo; and \u0026lsquo;IntWritable\u0026rsquo; are data type of output generated by reducer in the form of key-value pair.\nEvery reducer class must be extended from MapReduceBase class and it must implement Reducer interface.\n2. Defining \u0026lsquo;reduce\u0026rsquo; function-\npublic void reduce( Text t_key, Iterator values, OutputCollector output, Reporter reporter) throws IOException {\nAn input to the reduce() method is a key with a list of multiple values.\nFor example, in our case, it will be-\n, , ,, , .\nThis is given to reducer as \nSo, to accept arguments of this form, first two data types are used, viz., Text and Iterator. Text is a data type of key and Iterator is a data type for list of values for that key.\nThe next argument is of type OutputCollector which collects the output of reducer phase.\nreduce() method begins by copying key value and initializing frequency count to 0.\n​ Text key = t_key;\n​ int frequencyForCountry = 0;\nThen, using \u0026lsquo;while\u0026rsquo; loop, we iterate through the list of values associated with the key and calculate the final frequency by summing up all the values.\n​\nwhile (values.hasNext()) { // replace type of value with the actual type of our value IntWritable value = (IntWritable) values.next(); frequencyForCountry += value.get(); }\nNow, we push the result to the output collector in the form of key and obtained frequency count.\nBelow code does this-\noutput.collect(key, new IntWritable(frequencyForCountry));\nExplanation of SalesCountryDriver Class\nIn this section, we will understand the implementation of SalesCountryDriver class\n\\1. We begin by specifying a name of package for our class. SalesCountry is a name of out package. Please note that output of compilation, SalesCountryDriver.class will go into directory named by this package name: SalesCountry.\nHere is a line specifying package name followed by code to import library packages.\n\\2. Define a driver class which will create a new client job, configuration object and advertise Mapper and Reducer classes.\nThe driver class is responsible for setting our MapReduce job to run in Hadoop. In this class, we specify job name, data type of input/output and names of mapper and reducer classes.\n\\3. In below code snippet, we set input and output directories which are used to consume input dataset and produce output, respectively.\narg[0] and arg[1] are the command-line arguments passed with a command given in MapReduce hands-on, i.e.,\n$HADOOP_HOME/bin/hadoop jar ProductSalePerCountry.jar /inputMapReduce /mapreduce_output_sales\n\\4. Trigger our job\nBelow code start execution of MapReduce job-\ntry { // Run the job JobClient.runJob(job_conf); } catch (Exception e) { e.printStackTrace(); }\n07 Join MapReduce 07 Join MapReduce\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/07-join-mapreduce/",
	"title": "07 Join-MapReduce",
	"tags": [],
	"description": "",
	"content": " What is a Join in MapReduce?\nA join operation is used to combine two large datasets in MapReduce. However, this process involves writing lots of code to perform the actual join operation.\nJoining of two datasets begins by comparing the size of each dataset. If one dataset is smaller as compared to the other dataset then smaller dataset is distributed to every data node in the cluster. Once it is distributed, either Mapper or Reducer uses the smaller dataset to perform a lookup for matching records from the large dataset and then combine those records to form output records.\nIn this tutorial, you will learn-\n What is a Join in MapReduce? Types of Join How to Join two DataSets: MapReduce Example What is Counter in MapReduce? Types of MapReduce Counters Counters Example  Types of Join\nDepending upon the place where the actual join is performed, this join is classified into-\n1. Map-side join - When the join is performed by the mapper, it is called as map-side join. In this type, the join is performed before data is actually consumed by the map function. It is mandatory that the input to each map is in the form of a partition and is in sorted order. Also, there must be an equal number of partitions and it must be sorted by the join key.\n2. Reduce-side join - When the join is performed by the reducer, it is called as reduce-side join. There is no necessity in this join to have a dataset in a structured form (or partitioned).\nHere, map side processing emits join key and corresponding tuples of both the tables. As an effect of this processing, all the tuples with same join key fall into the same reducer which then joins the records with same join key.\nAn overall process flow is depicted in below diagram.\nHow to Join two DataSets: MapReduce Example\nThere are two Sets of Data in two Different Files (shown below). The Key Dept_ID is common in both files. The goal is to use MapReduce Join to combine these files\nFile 1\nFile 2\nInput: The input data set is a txt file, DeptName.txt \u0026amp; DepStrength.txt\nDownload Input Files From Here\nEnsure you have Hadoop installed. Before you start with the actual process, change user to \u0026lsquo;hduser\u0026rsquo; (id used while Hadoop configuration, you can switch to the userid used during your Hadoop config ).\nsu - hduser_\nStep 1) Copy the zip file to the location of your choice\nStep 2) Uncompress the Zip File\nsudo tar -xvf MapReduceJoin.tar.gz\nStep 3) Go to directory MapReduceJoin/\ncd MapReduceJoin/\nStep 4) Start Hadoop\n$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\nStep 5) DeptStrength.txt and DeptName.txt are the input files used for this program.\nThese file needs to be copied to HDFS using below command-\n$HADOOP_HOME/bin/hdfs dfs -copyFromLocal DeptStrength.txt DeptName.txt /\nStep 6) Run the program using below command-\n$HADOOP_HOME/bin/hadoop jar MapReduceJoin.jar MapReduceJoin/JoinDriver/DeptStrength.txt /DeptName.txt /output_mapreducejoin\nStep 7) After execution, output file (named \u0026lsquo;part-00000\u0026rsquo;) will stored in the directory /output_mapreducejoin on HDFS\nResults can be seen using the command line interface\n$HADOOP_HOME/bin/hdfs dfs -cat /output_mapreducejoin/part-00000\nResults can also be seen via a web interface as-\nNow select \u0026lsquo;Browse the filesystem\u0026rsquo; and navigate upto /output_mapreducejoin\nOpen part-r-00000\nResults are shown\nNOTE: Please note that before running this program for the next time, you will need to delete output directory /output_mapreducejoin\n$HADOOP_HOME/bin/hdfs dfs -rm -r /output_mapreducejoin\nAlternative is to use a different name for the output directory.\nWhat is Counter in MapReduce?\nA counter in MapReduce is a mechanism used for collecting statistical information about the MapReduce job. This information could be useful for diagnosis of a problem in MapReduce job processing. Counters are similar to putting a log message in the code for a map or reduce.\nTypically, these counters are defined in a program (map or reduce) and are incremented during execution when a particular event or condition (specific to that counter) occurs. A very good application of counters is to track valid and invalid records from an input dataset.\nTypes of MapReduce Counters\nThere are basically 2 types of MapReduce Counters\n  Hadoop Built-In counters:There are some built-in counters which exist per job. Below are built-in counter groups-   - MapReduce Task Counters - Collects task specific information (e.g., number of input records) during its execution time.   FileSystem Counters - Collects information like number of bytes read or written by a task FileInputFormat Counters - Collects information of a number of bytes read through FileInputFormat FileOutputFormat Counters - Collects information of a number of bytes written through FileOutputFormat Job Counters - These counters are used by JobTracker. Statistics collected by them include e.g., the number of task launched for a job.   User Defined Counters   In addition to built-in counters, a user can define his own counters using similar functionalities provided by programming languages. For example, inJava \u0026lsquo;enum\u0026rsquo; are used to define user defined counters.\nCounters Example\nAn example MapClass with Counters to count the number of missing and invalid values. Input data file used in this tutorial Our input data set is a CSV file, SalesJan2009.csv\npublic static class MapClass extends MapReduceBase implements Mapper{ static enum SalesCounters { MISSING, INVALID }; public void map ( LongWritable key, Text value, OutputCollectoroutput, Reporter reporter) throws IOException { //Input string is split using \u0026lsquo;,\u0026rsquo; and stored in \u0026lsquo;fields\u0026rsquo; array String fields[] = value.toString().split(\u0026ldquo;,\u0026rdquo;, -20); //Value at 4th index is country. It is stored in \u0026lsquo;country\u0026rsquo; variable String country = fields[4]; //Value at 8th index is sales data. It is stored in \u0026lsquo;sales\u0026rsquo; variable String sales = fields[8]; if (country.length() == 0) { reporter.incrCounter(SalesCounters.MISSING, 1); } else if (sales.startsWith(\u0026rdquo;\\\u0026ldquo;\u0026rdquo;)) { reporter.incrCounter(SalesCounters.INVALID, 1); } else { output.collect(new Text(country), new Text(sales + \u0026ldquo;,1\u0026rdquo;)); } } }\nAbove code snippet shows an example implementation of counters in Map Reduce.\nHere, SalesCounters is a counter defined using \u0026lsquo;enum\u0026rsquo;. It is used to count MISSING and INVALID input records.\nIn the code snippet, if \u0026lsquo;country\u0026rsquo; field has zero length then its value is missing and hence corresponding counter SalesCounters.MISSING is incremented.\nNext, if \u0026lsquo;sales\u0026rsquo; field starts with a \u0026ldquo; then the record is considered INVALID. This is indicated by incrementing counter SalesCounters.INVALID.\n08 SQOOP 08 Sqoop\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/08-sqoop/",
	"title": "08 SQOOP",
	"tags": [],
	"description": "",
	"content": " What is SQOOP in Hadoop?\nApache Sqoop (SQL-to-Hadoop) is designed to support bulk import of data into HDFS from structured data stores such as relational databases, enterprise data warehouses, and NoSQL systems. Sqoop is based upon a connector architecture which supports plugins to provide connectivity to new external systems.\nAn example use case of Sqoop is an enterprise that runs a nightly Sqoop import to load the day\u0026rsquo;s data from a production transactional RDBMS into aHive data warehouse for further analysis.\nSqoop Architecture\nAll the existing Database Management Systems are designed withSQL standard in mind. However, each DBMS differs with respect to dialect to some extent. So, this difference poses challenges when it comes to data transfers across the systems. Sqoop Connectors are components which help overcome these challenges.\nData transfer between Sqoop and external storage system is made possible with the help of Sqoop\u0026rsquo;s connectors.\nSqoop has connectors for working with a range of popular relational databases, including MySQL, PostgreSQL, Oracle, SQL Server, and DB2. Each of these connectors knows how to interact with its associated DBMS. There is also a generic JDBC connector for connecting to any database that supports Java\u0026rsquo;s JDBC protocol. In addition, Sqoop provides optimized MySQL and PostgreSQL connectors that use database-specific APIs to perform bulk transfers efficiently.\nSqoop Architecture\nIn addition to this, Sqoop has various third-party connectors for data stores,\nranging from enterprise data warehouses (including Netezza, Teradata, and Oracle) to NoSQL stores (such as Couchbase). However, these connectors do not come with Sqoop bundle; those need to be downloaded separately and can be added easily to an existing Sqoop installation.\nWhy do we need Sqoop?\nAnalytical processing using Hadoop requires loading of huge amounts of data from diverse sources into Hadoop clusters. This process of bulk data load into Hadoop, from heterogeneous sources and then processing it, comes with a certain set of challenges. Maintaining and ensuring data consistency and ensuring efficient utilization of resources, are some factors to consider before selecting the right approach for data load.\nMajor Issues:\n1. Data load using Scripts\nThe traditional approach of using scripts to load data is not suitable for bulk data load into Hadoop; this approach is inefficient and very time-consuming.\n2. Direct access to external data via Map-Reduce application\nProviding direct access to the data residing at external systems(without loading into Hadoop) for map-reduce applications complicates these applications. So, this approach is not feasible.\n\\3. In addition to having the ability to work with enormous data, Hadoop can work with data in several different forms. So, to load such heterogeneous data into Hadoop, different tools have been developed. Sqoop and Flume are two such data loading tools.\nSqoop vs Flume vs HDFS in Hadoop\n   Sqoop Flume HDFS     Sqoop is used for importing data from structured data sources such as RDBMS. Flume is used for moving bulk streaming data into HDFS. HDFS is a distributed file system used by Hadoop ecosystem to store data.   Sqoop has a connector based architecture. Connectors know how to connect to the respective data source and fetch the data. Flume has an agent-based architecture. Here, a code is written (which is called as \u0026lsquo;agent\u0026rsquo;) which takes care of fetching data. HDFS has a distributed architecture where data is distributed across multiple data nodes.   HDFS is a destination for data import using Sqoop. Data flows to HDFS through zero or more channels. HDFS is an ultimate destination for data storage.   Sqoop data load is not event-driven. Flume data load can be driven by an event. HDFS just stores data provided to it by whatsoever means.   In order to import data from structured data sources, one has to use Sqoop only, because its connectors know how to interact with structured data sources and fetch data from them. In order to load streaming data such as tweets generated on Twitter or log files of a web server, Flume should be used. Flume agents are built for fetching streaming data. HDFS has its own built-in shell commands to store data into it. HDFS cannot import streaming data    09 Flume 09 Flume\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/09-flume/",
	"title": "09 Flume",
	"tags": [],
	"description": "",
	"content": " What is FLUME in Hadoop? Apache Flume is a system used for moving massive quantities of streaming data into HDFS. Collecting log data present in log files from web servers and aggregating it in HDFS for analysis, is one common example use case of Flume.\nFlume supports multiple sources like –\n \u0026lsquo;tail\u0026rsquo; (which pipes data from a local file and write into HDFS via Flume, similar to Unix command \u0026lsquo;tail\u0026rsquo;) System logs Apache log4j (enable Java applications to write events to files in HDFS via Flume).  In this tutorial, you will learn-\n What is FLUME in Hadoop? Flume Architecture Some Important features of FLUME Flume, library and source code setup Load data from Twitter using Flume Creating a Twitter Application Modify \u0026lsquo;flume.conf\u0026rsquo; File Example: Streaming Twitter Data using Flume  Flume Architecture\nA Flume agent is a JVM process which has 3 components -Flume Source, Flume Channel and Flume Sink- through which events propagate after initiated at an external source.\nFlume Architecture\n In the above diagram, the events generated by external source (WebServer) are consumed by Flume Data Source. The external source sends events to Flume source in a format that is recognized by the target source. Flume Source receives an event and stores it into one or more channels. The channel acts as a store which keeps the event until it is consumed by the flume sink. This channel may use a local file system in order to store these events. Flume sink removes the event from a channel and stores it into an external repository like e.g., HDFS. There could be multiple flume agents, in which case flume sink forwards the event to the flume source of next flume agent in the flow.  Some Important features of FLUME\n Flume has a flexible design based upon streaming data flows. It is fault tolerant and robust with multiple failovers and recovery mechanisms. Flume has different levels of reliability to offer which includes \u0026lsquo;best-effort delivery\u0026rsquo; and an \u0026lsquo;end-to-end delivery\u0026rsquo;. Best-effort delivery does not tolerate any Flume node failure whereas \u0026lsquo;end-to-end delivery\u0026rsquo; mode guarantees delivery even in the event of multiple node failures. Flume carries data between sources and sinks. This gathering of data can either be scheduled or event-driven. Flume has its own query processing engine which makes it easy to transform each new batch of data before it is moved to the intended sink. Possible Flume sinks include HDFS and HBase. Flume can also be used to transport event data including but not limited to network traffic data, data generated by social media websites and email messages.  Flume, library and source code setup\nBefore we start with the actual process, ensure you have Hadoop installed. Change user to \u0026lsquo;hduser\u0026rsquo; (id used while Hadoop configuration, you can switch to the userid used during your Hadoop config)\nStep 1) Create a new directory with the name \u0026lsquo;FlumeTutorial\u0026rsquo;\nsudo mkdir FlumeTutorial\n Give a read, write and execute permissions  sudo chmod -R 777 FlumeTutorial\n Copy files MyTwitterSource.java and MyTwitterSourceForFlume.java in this directory.  Download Input Files From Here\nCheck the file permissions of all these files and if \u0026lsquo;read\u0026rsquo; permissions are missing then grant the same-\nStep 2) Download \u0026lsquo;Apache Flume\u0026rsquo; from a site- https://flume.apache.org/download.html\nApache Flume 1.4.0 has been used in this tutorial.\nNext Click\nStep 3) Copy the downloaded tarball in the directory of your choice and extract contents using the following command\nsudo tar -xvf apache-flume-1.4.0-bin.tar.gz\nThis command will create a new directory named apache-flume-1.4.0-bin and extract files into it. This directory will be referred to as  in rest of the article.\nStep 4) Flume library setup\nCopy twitter4j-core-4.0.1.jar, flume-ng-configuration-1.4.0.jar, flume-ng-core-1.4.0.jar, flume-ng-sdk-1.4.0.jar to\n/lib/\nIt is possible that either or all of the copied JAR will have to execute permission. This may cause an issue with the compilation of code. So, revoke execute permission on such JAR.\nIn my case, twitter4j-core-4.0.1.jar was having to execute permission. I revoked it as below-\nsudo chmod -x twitter4j-core-4.0.1.jar\nAfter this command gives \u0026lsquo;read\u0026rsquo; permission on twitter4j-core-4.0.1.jar to all.\nsudo chmod +rrr /usr/local/apache-flume-1.4.0-bin/lib/twitter4j-core-4.0.1.jar\nPlease note that I have downloaded-\n- twitter4j-core-4.0.1.jar from http://mvnrepository.com/artifact/org.twitter4j/twitter4j-core\n- All flame JARs i.e., flume-ng-*-1.4.0.jar from http://mvnrepository.com/artifact/org.apache.flume\nLoad data from Twitter using Flume\nStep 1) Go to the directory containing source code files in it.\nStep 2) Set CLASSPATH to contain /lib/* and ~/FlumeTutorial/flume/mytwittersource/*\nexport CLASSPATH=\u0026ldquo;/usr/local/apache-flume-1.4.0-bin/lib/:~/FlumeTutorial/flume/mytwittersource/\u0026ldquo;\nStep 3) Compile source code using the command-\njavac -d . MyTwitterSourceForFlume.java MyTwitterSource.java\nStep 4)Create a jar\nFirst, create Manifest.txt file using a text editor of your choice and add below line in it-\nMain-Class: flume.mytwittersource.MyTwitterSourceForFlume\n.. here flume.mytwittersource.MyTwitterSourceForFlume is the name of the main class. Please note that you have to hit enter key at end of this line.\nNow, create JAR \u0026lsquo;MyTwitterSourceForFlume.jar\u0026rsquo; as-\njar cfm MyTwitterSourceForFlume.jar Manifest.txt flume/mytwittersource/*.class\nStep 5) Copy this jar to /lib/\nsudo cp MyTwitterSourceForFlume.jar /lib/\nStep 6) Go to the configuration directory of Flume, /conf\nIf flume.conf does not exist, then copy flume-conf.properties.template and rename it to flume.conf\nsudo cp flume-conf.properties.template flume.conf\nIf flume-env.sh does not exist, then copy flume-env.sh.template and rename it to flume-env.sh\nsudo cp flume-env.sh.template flume-env.sh\nCreating a Twitter Application\nStep 1) Create a Twitter application by signing in to https://developer.twitter.com/\nStep 2) Go to \u0026lsquo;My applications\u0026rsquo; (This option gets dropped down when \u0026lsquo;Egg\u0026rsquo; button at the top right corner is clicked)\nStep 3) Create a new application by clicking \u0026lsquo;Create New App\u0026rsquo;\nStep 4) Fill up application details by specifying the name of application, description, and website. You may refer to the notes given underneath each input box.\nStep 5) Scroll down the page and accept terms by marking \u0026lsquo;Yes, I agree\u0026rsquo; and click on button\u0026lsquo;Create your Twitter application\u0026rsquo;\nStep 6) On the window of a newly created application, go to the tab, \u0026lsquo;API Keys\u0026rsquo; scroll down the page and click button \u0026lsquo;Create my access token\u0026rsquo;\nStep 7) Refresh the page.\nStep 8) Click on \u0026lsquo;Test OAuth\u0026rsquo;. This will display \u0026lsquo;OAuth\u0026rsquo; settings of the application.\nStep 9) Modify \u0026lsquo;flume.conf\u0026rsquo; using these OAuth settings. Steps to modify \u0026lsquo;flume.conf\u0026rsquo; are given below.\nWe need to copy Consumer key, Consumer secret, Access token and Access token secret to updating \u0026lsquo;flume.conf\u0026rsquo;.\nNote: These values belong to the user and hence are confidential, so should not be shared.\nModify \u0026lsquo;flume.conf\u0026rsquo; File\nStep 1) Open \u0026lsquo;flume.conf\u0026rsquo; in write mode and set values for below parameters-\nsudo gedit flume.conf\nCopy below contents-\nMyTwitAgent.sources = Twitter MyTwitAgent.channels = MemChannel MyTwitAgent.sinks = HDFS MyTwitAgent.sources.Twitter.type = flume.mytwittersource.MyTwitterSourceForFlume MyTwitAgent.sources.Twitter.channels = MemChannel MyTwitAgent.sources.Twitter.consumerKey = MyTwitAgent.sources.Twitter.consumerSecret = MyTwitAgent.sources.Twitter.accessToken = MyTwitAgent.sources.Twitter.accessTokenSecret = MyTwitAgent.sources.Twitter.keywords = guru99 MyTwitAgent.sinks.HDFS.channel = MemChannel MyTwitAgent.sinks.HDFS.type = hdfs MyTwitAgent.sinks.HDFS.hdfs.path = hdfs://localhost:54310/user/hduser/flume/tweets/ MyTwitAgent.sinks.HDFS.hdfs.fileType = DataStream MyTwitAgent.sinks.HDFS.hdfs.writeFormat = Text MyTwitAgent.sinks.HDFS.hdfs.batchSize = 1000 MyTwitAgent.sinks.HDFS.hdfs.rollSize = 0 MyTwitAgent.sinks.HDFS.hdfs.rollCount = 10000 MyTwitAgent.channels.MemChannel.type = memory MyTwitAgent.channels.MemChannel.capacity = 10000 MyTwitAgent.channels.MemChannel.transactionCapacity = 1000\nStep 2) Also, set TwitterAgent.sinks.HDFS.hdfs.path as below,\nTwitterAgent.sinks.HDFS.hdfs.path = hdfs://://flume/tweets/\nTo know ,  and  , see value of parameter \u0026lsquo;fs.defaultFS\u0026rsquo; set in $HADOOP_HOME/etc/hadoop/core-site.xml\nStep 3) In order to flush the data to HDFS, as an when it comes, delete below entry if it exists,\nTwitterAgent.sinks.HDFS.hdfs.rollInterval = 600\nExample: Streaming Twitter Data using Flume\nStep 1) Open \u0026lsquo;flume-env.sh\u0026rsquo; in write mode and set values for below parameters,\nJAVA_HOME=FLUME_CLASSPATH=\u0026rdquo;/lib/MyTwitterSourceForFlume.jar\u0026rdquo;\nStep 2) Start Hadoop\n$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\nStep 3) Two of the JAR files from the Flume tarball are not compatible with Hadoop 2.2.0. So, we will need to follow below steps to make Flume compatible with Hadoop 2.2.0.\na. Move protobuf-java-2.4.1.jar out of \u0026lsquo;/lib\u0026rsquo;.\nGo to \u0026lsquo;/lib\u0026rsquo;\ncd /lib\nsudo mv protobuf-java-2.4.1.jar ~/\nb. Find for JAR file \u0026lsquo;guava\u0026rsquo; as below\nfind . -name \u0026ldquo;guava*\u0026rdquo;\nMove guava-10.0.1.jar out of \u0026lsquo;/lib\u0026rsquo;.\nsudo mv guava-10.0.1.jar ~/\nc. Download guava-17.0.jar from http://mvnrepository.com/artifact/com.google.guava/guava/17.0\nNow, copy this downloaded jar file to \u0026lsquo;/lib\u0026rsquo;\nStep 4) Go to \u0026lsquo;/bin\u0026rsquo; and start Flume as-\n./flume-ng agent -n MyTwitAgent -c conf -f /conf/flume.conf\nCommand prompt window where flume is fetching Tweets-\nFrom command window message we can see that the output is written to /user/hduser/flume/tweets/ directory.\nNow, open this directory using a web browser.\nStep 5) To see the result of data load, using a browser open http://localhost:50070/ and browse the file system, then go to the directory where data has been loaded, that is-\n/flume/tweets/\n10 Pig 10 Pig Introduction\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/10-pig/",
	"title": "10 Pig",
	"tags": [],
	"description": "",
	"content": " What is PIG? Pig is a high-level programming language useful for analyzing large data sets. A pig was a result of development effort at Yahoo!\nIn a MapReduce framework, programs need to be translated into a series of Map and Reduce stages. However, this is not a programming model which data analysts are familiar with. So, in order to bridge this gap, an abstraction called Pig was built on top of Hadoop.\nApache Pig enables people to focus more on analyzing bulk data sets and to spend less time writing Map-Reduce programs. Similar to Pigs, who eat anything, the Pig programming language is designed to work upon any kind of data. That\u0026rsquo;s why the name, Pig!\nIn this beginner\u0026rsquo;s Big Data tutorial, you will learn-\n What is PIG? Pig Architecture Prerequisites How to Download and Install Pig Example Pig Script  Pig Architecture\nPig consists of two components:\n Pig Latin, which is a language A runtime environment, for running PigLatin programs.  A Pig Latin program consists of a series of operations or transformations which are applied to the input data to produce output. These operations describe a data flow which is translated into an executable representation, by Pig execution environment. Underneath, results of these transformations are series of MapReduce jobs which a programmer is unaware of. So, in a way, Pig allows the programmer to focus on data rather than the nature of execution.\nPigLatin is a relatively stiffened language which uses familiar keywords from data processing e.g., Join, Group and Filter.\nPIG Architecture\nExecution modes:\nPig has two execution modes:\n Local mode: In this mode, Pig runs in a single JVM and makes use of local file system. This mode is suitable only for analysis of small datasets using Pig Map Reduce mode: In this mode, queries written in Pig Latin are translated into MapReduce jobs and are run on a Hadoop cluster (cluster may be pseudo or fully distributed). MapReduce mode with the fully distributed cluster is useful of running Pig on large datasets.  How to Download and Install Pig\nBefore we start with the actual process, ensure you have Hadoop installed. Change user to \u0026lsquo;hduser\u0026rsquo; (id used while Hadoop configuration, you can switch to the userid used during your Hadoop config)\nStep 1) Download the stable latest release of Pig from any one of the mirrors sites available at\nhttp://pig.apache.org/releases.html\nSelect tar.gz (and not src.tar.gz) file to download.\nStep 2) Once a download is complete, navigate to the directory containing the downloaded tar file and move the tar to the location where you want to setup Pig. In this case, we will move to /usr/local\nMove to a directory containing Pig Files\ncd /usr/local\nExtract contents of tar file as below\nsudo tar -xvf pig-0.12.1.tar.gz\nStep 3). Modify ~/.bashrc to add Pig related environment variables\nOpen ~/.bashrc file in any text editor of your choice and do below modifications-\nexport PIG_HOME=export PATH=$PIG_HOME/bin:$HADOOP_HOME/bin:$PATH\nStep 4) Now, source this environment configuration using below command\n. ~/.bashrc\nStep 5) We need to recompile PIG to support Hadoop 2.2.0\nHere are the steps to do this-\nGo to PIG home directory\ncd $PIG_HOME\nInstall Ant\nsudo apt-get install ant\nNote: Download will start and will consume time as per your internet speed.\nRecompile PIG\nsudo ant clean jar-all -Dhadoopversion=23\nPlease note that in this recompilation process multiple components are downloaded. So, a system should be connected to the internet.\nAlso, in case this process stuck somewhere and you don\u0026rsquo;t see any movement on command prompt for more than 20 minutes then press Ctrl + c and rerun the same command.\nIn our case, it takes 20 minutes\nStep 6) Test the Pig installation using the command\npig -help\nExample Pig Script\nWe will use PIG to find the Number of Products Sold in Each Country.\nInput: Our input data set is a CSV file, SalesJan2009.csv\nStep 1) Start Hadoop\n$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\nStep 2) Pig takes a file from HDFS in MapReduce mode and stores the results back to HDFS.\nCopy file SalesJan2009.csv (stored on local file system, ~/input/SalesJan2009.csv) to HDFS (Hadoop Distributed File System) Home Directory\nHere the file is in Folder input. If the file is stored in some other location give that name\n$HADOOP_HOME/bin/hdfs dfs -copyFromLocal ~/input/SalesJan2009.csv /\nVerify whether a file is actually copied or not.\n$HADOOP_HOME/bin/hdfs dfs -ls /\nStep 3) Pig Configuration\nFirst, navigate to $PIG_HOME/conf\ncd $PIG_HOME/conf\nsudo cp pig.properties pig.properties.original\nOpen pig.properties using a text editor of your choice, and specify log file path using pig.logfile\nsudo gedit pig.properties\nLoger will make use of this file to log errors.\nStep 4) Run command \u0026lsquo;pig\u0026rsquo; which will start Pig command prompt which is an interactive shell Pig queries.\npig\nStep 5)In Grunt command prompt for Pig, execute below Pig commands in order.\n\u0026ndash; A. Load the file containing data.\nsalesTable = LOAD \u0026lsquo;/SalesJan2009.csv\u0026rsquo; USING PigStorage(\u0026lsquo;,\u0026rsquo;) AS (Transaction_date:chararray,Product:chararray,Price:chararray,Payment_Type:chararray,Name:chararray,City:chararray,State:chararray,Country:chararray,Account_Created:chararray,Last_Login:chararray,Latitude:chararray,Longitude:chararray);\nPress Enter after this command.\n\u0026ndash; B. Group data by field Country\nGroupByCountry = GROUP salesTable BY Country;\n\u0026ndash; C. For each tuple in \u0026lsquo;GroupByCountry\u0026rsquo;, generate the resulting string of the form-\u0026gt; Name of Country: No. of products sold\nCountByCountry = FOREACH GroupByCountry GENERATE CONCAT((chararray)$0,CONCAT(\u0026rsquo;:\u0026lsquo;,(chararray)COUNT($1)));\nPress Enter after this command.\n\u0026ndash; D. Store the results of Data Flow in the directory \u0026lsquo;pig_output_sales\u0026rsquo; on HDFS\nSTORE CountByCountry INTO \u0026lsquo;pig_output_sales\u0026rsquo; USING PigStorage(\u0026rsquo;\\t\u0026rsquo;);\nThis command will take some time to execute. Once done, you should see the following screen\nStep 6) Result can be seen through command interface as,\n$HADOOP_HOME/bin/hdfs dfs -cat pig_output_sales/part-r-00000\nResults can also be seen via a web interface as-\nResults through a web interface-\nOpen http://localhost:50070/ in a web browser.\nNow select \u0026lsquo;Browse the filesystem\u0026rsquo; and navigate upto /user/hduser/pig_output_sales\nOpen part-r-00000\n11 OOZIE 11 OOZIE\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/11-oozie/",
	"title": "11 OOZIE",
	"tags": [],
	"description": "",
	"content": " What is OOZIE?\nApache Oozie is a workflow scheduler for Hadoop. It is a system which runs the workflow of dependent jobs. Here, users are permitted to create Directed Acyclic Graphs of workflows, which can be run in parallel and sequentially in Hadoop.\nIn this tutorial, you will learn,\n How does OOZIE work? Example Workflow Diagram Packaging and deploying an Oozie workflow application Why use Oozie? Features of Oozie  It consists of two parts:\n Workflow engine: Responsibility of a workflow engine is to store and run workflows composed of Hadoop jobs e.g., MapReduce, Pig, Hive. Coordinator engine: It runs workflow jobs based on predefined schedules and availability of data.  Oozie is scalable and can manage the timely execution of thousands of workflows (each consisting of dozens of jobs) in a Hadoop cluster.\nOozie is very much flexible, as well. One can easily start, stop, suspend and rerun jobs. Oozie makes it very easy to rerun failed workflows. One can easily understand how difficult it can be to catch up missed or failed jobs due to downtime or failure. It is even possible to skip a specific failed node.\nHow does OOZIE work?\nOozie runs as a service in the cluster and clients submit workflow definitions for immediate or later processing.\nOozie workflow consists of action nodes and control-flow nodes.\nAn action node represents a workflow task, e.g., moving files into HDFS, running a MapReduce, Pig orHive jobs, importing data using Sqoop or running a shell script of a program written in Java.\nA control-flow node controls the workflow execution between actions by allowing constructs like conditional logic wherein different branches may be followed depending on the result of earlier action node.\nStart Node, End Node, and Error Node fall under this category of nodes.\nStart Node, designates the start of the workflow job.\nEnd Node, signals end of the job.\nError Node designates the occurrence of an error and corresponding error message to be printed.\nAt the end of execution of a workflow, HTTP callback is used by Oozie to update the client with the workflow status. Entry-to or exit from an action node may also trigger the callback.\nExample Workflow Diagram\nPackaging and deploying an Oozie workflow application\nA workflow application consists of the workflow definition and all the associated resources such as MapReduce Jar files, Pig scripts etc. Applications need to follow a simple directory structure and are deployed to HDFS so that Oozie can access them.\nAn example directory structure is shown below-\n/ ??? lib/ ? ??? hadoop-examples.jar ??? workflow.xml\nIt is necessary to keep workflow.xml (a workflow definition file) in the top level directory (parent directory with workflow name). Lib directory contains Jar files containing MapReduce classes. Workflow application conforming to this layout can be built with any build tool e.g., Ant or Maven.\nSuch a build need to be copied to HDFS using a command, for example -\n% hadoop fs -put hadoop-examples/target/name of workflow\nSteps for Running an Oozie workflow job\nIn this section, we will see how to run a workflow job. To run this, we will use the Oozie command-line tool (a client program which communicates with the Oozie server).\n1. Export OOZIE_URL environment variable which tells the oozie command which Oozie server to use (here we’re using one running locally):\n% export OOZIE_URL=\u0026ldquo;http://localhost:11000/oozie\u0026quot;\n2. Run workflow job using-\n% oozie job -config ch05/src/main/resources/max-temp-workflow.properties -run\nThe -config option refers to a localJava properties file containing definitions for the parameters in the workflow XML file, as well as oozie.wf.application.path, which tells Oozie the location of the workflow application in HDFS.\nExample contents of the properties file:\nnameNode=hdfs://localhost:8020 jobTracker=localhost:8021 oozie.wf.application.path=${nameNode}/user/${user.name}/3. Get the status of workflow job-\nStatus of workflow job can be seen using subcommand \u0026lsquo;job\u0026rsquo; with \u0026lsquo;-info\u0026rsquo; option and specifying job id after \u0026lsquo;-info\u0026rsquo;.\ne.g., % oozie job -info Output shows status which is one of RUNNING, KILLED or SUCCEEDED.\n4. Results of successful workflow execution can be seen using Hadoop command like-\n% hadoop fs -cat Why use Oozie?\nThe main purpose of using Oozie is to manage different type of jobs being processed in Hadoop system.\nDependencies between jobs are specified by a user in the form of Directed Acyclic Graphs. Oozie consumes this information and takes care of their execution in the correct order as specified in a workflow. That way user\u0026rsquo;s time to manage complete workflow is saved. In addition, Oozie has a provision to specify the frequency of execution of a particular job.\nFeatures of Oozie\n Oozie has client API and command line interface which can be used to launch, control and monitor job from Java application. Using its Web Service APIs one can control jobs from anywhere. Oozie has provision to execute jobs which are scheduled to run periodically. Oozie has provision to send email notifications upon completion of jobs.  12 Testing 12 BigData Testing\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/12-testing-bigdata/",
	"title": "12 Testing-BigData",
	"tags": [],
	"description": "",
	"content": " What is Big Data Testing?\nBigData testing is defined as testing of Bigdata applications. Big data is a collection of large datasets that cannot be processed using traditional computing techniques.Testing of these datasets involves various tools, techniques, and frameworks to process. Big data relates to data creation, storage, retrieval and analysis that is remarkable in terms of volume, variety, and velocity. You can learn more about Big Data, Hadoop and MapReduce here\nIn this tutorial, you will learn-\n Big Data Testing Strategy How to test Hadoop Applications Architecture Testing Performance Testing Performance Testing Approach Parameters for Performance Testing Test Environment Needs Big data Testing Vs. Traditional database Testing Tools used in Big Data Scenarios Challenges in Big Data Testing  Big Data Testing Strategy\nTesting Big Data application is more verification of its data processing rather than testing the individual features of the software product. When it comes to Big data testing, performance and functional testing are the keys.\nIn Big data testing, QA engineers verify the successful processing of terabytes of data using commodity cluster and other supportive components. It demands a high level of testing skills as the processing is very fast. Processing may be of three types\nAlong with this, data quality is also an important factor in Hadoop testing. Before testing the application, it is necessary to check the quality of data and should be considered as a part of database testing. It involves checking various characteristics like conformity, accuracy, duplication, consistency, validity, data completeness, etc.\nHow to test Hadoop Applications\nThe following figure gives a high-level overview of phases in Testing Big Data Applications\nBig Data Testing can be broadly divided into three steps\nStep 1: Data Staging Validation\nThe first step of big data testing also referred as pre-Hadoop stage involves process validation.\n Data from various source like RDBMS, weblogs, social media, etc. should be validated to make sure that correct data is pulled into the system Comparing source data with the data pushed into the Hadoop system to make sure they match Verify the right data is extracted and loaded into the correct HDFS location  Tools like Talend, Datameer, can be used for data staging validation\nStep 2: \u0026ldquo;MapReduce\u0026rdquo; Validation\nThe second step is a validation of \u0026ldquo;MapReduce\u0026rdquo;. In this stage, the tester verifies the business logic validation on every node and then validating them after running against multiple nodes, ensuring that the\n Map Reduce process works correctly Data aggregation or segregation rules are implemented on the data Key value pairs are generated Validating the data after the Map-Reduce process  Step 3: Output Validation Phase\nThe final or third stage of Big Data testing is the output validation process. The output data files are generated and ready to be moved to an EDW (Enterprise Data Warehouse) or any other system based on the requirement.\nActivities in the third stage include\n To check the transformation rules are correctly applied To check the data integrity and successful data load into the target system To check that there is no data corruption by comparing the target data with the HDFS file system data  Architecture Testing\nHadoop processes very large volumes of data and is highly resource intensive. Hence, architectural testing is crucial to ensure the success of your Big Data project. A poorly or improper designed system may lead to performance degradation, and the system could fail to meet the requirement. At least, Performance and Failover test services should be done in a Hadoop environment.\nPerformance testing includes testing of job completion time, memory utilization, data throughput, and similar system metrics. While the motive of Failover test service is to verify that data processing occurs seamlessly in case of failure of data nodes\nPerformance Testing\nPerformance Testing for Big Data includes two main action\n Data ingestion and Throughout: In this stage, the tester verifies how the fast system can consume data from various data source. Testing involves identifying a different message that the queue can process in a given time frame. It also includes how quickly data can be inserted into the underlying data store for example insertion rate into a Mongo and Cassandra database. Data Processing: It involves verifying the speed with which the queries or map reduce jobs are executed. It also includes testing the data processing in isolation when the underlying data store is populated within the data sets. For example, running Map Reduce jobs on the underlying HDFS Sub-Component Performance: These systems are made up of multiple components, and it is essential to test each of these components in isolation. For example, how quickly the message is indexed and consumed, MapReduce jobs, query performance, search, etc.  Performance Testing Approach\nPerformance testing for big data application involves testing of huge volumes of structured and unstructured data, and it requires a specific testing approach to test such massive data.\nPerformance Testing is executed in this order\n The process begins with the setting of the Big data cluster which is to be tested for performance Identify and design corresponding workloads Prepare individual clients (Custom Scripts are created) Execute the test and analyzes the result (If objectives are not met then tune the component and re-execute) Optimum Configuration  Parameters for Performance Testing\nVarious parameters to be verified for performance testing are\n Data Storage: How data is stored in different nodes Commit logs: How large the commit log is allowed to grow Concurrency: How many threads can perform write and read operation Caching: Tune the cache setting \u0026ldquo;row cache\u0026rdquo; and \u0026ldquo;key cache.\u0026rdquo; Timeouts: Values for connection timeout, query timeout, etc. JVM Parameters: Heap size, GC collection algorithms, etc. Map reduce performance: Sorts, merge, etc. Message queue: Message rate, size, etc.  Test Environment Needs\nTest Environment needs to depend on the type of application you are testing. For Big data testing, the test environment should encompass\n It should have enough space for storage and process a large amount of data It should have a cluster with distributed nodes and data It should have minimum CPU and memory utilization to keep performance high  Big data Testing Vs. Traditional database Testing\n   Properties Traditional database testing Big data testing     Data Tester work with structured data Tester works with both structured as well as unstructured data    Testing approach is well defined and time-tested The testing approach requires focused R\u0026amp;D efforts    Tester has the option of \u0026ldquo;Sampling\u0026rdquo; strategy doing manually or \u0026ldquo;Exhaustive Verification\u0026rdquo; strategy by the automation tool \u0026ldquo;Sampling\u0026rdquo; strategy in Big data is a challenge   Infrastructure It does not require a special test environment as the file size is limited It requires a special test environment due to large data size and files (HDFS)   Validation Tools Tester uses either the Excel-based macros or UI based automation tools No defined tools, the range is vast from programming tools like MapReduce to HIVEQL    Testing Tools can be used with basic operating knowledge and less training. It requires a specific set of skills and training to operate a testing tool. Also, the tools are in their nascent stage and over time it may come up with new features.    Tools used in Big Data Scenarios\n   Big Data Cluster Big Data Tools     NoSQL: CouchDB, DatabasesMongoDB, Cassandra, Redis, ZooKeeper, HBase   MapReduce: Hadoop, Hive, Pig, Cascading, Oozie, Kafka, S4, MapR, Flume   Storage: S3, HDFS ( Hadoop Distributed File System)   Servers: Elastic, Heroku, Elastic, Google App Engine, EC2   Processing R, Yahoo! Pipes, Mechanical Turk, BigSheets, Datameer    Challenges in Big Data Testing\n Automation  Automation testing for Big data requires someone with technical expertise. Also, automated tools are not equipped to handle unexpected problems that arise during testing\n Virtualization  It is one of the integral phases of testing. Virtual machine latency creates timing problems in real time big data testing. Also managing images in Big data is a hassle.\n Large Dataset\n  Need to verify more data and need to do it faster   Need to automate the testing effort Need to be able to test across different platform   Performance testing challenges\n Diverse set of technologies: Each sub-component belongs to different technology and requires testing in isolation Unavailability of specific tools: No single tool can perform the end-to-end testing. For example, NoSQL might not fit for message queues Test Scripting: A high degree of scripting is needed to design test scenarios and test cases Test environment: It needs a special test environment due to the large data size Monitoring Solution: Limited solutions exists that can monitor the entire environment Diagnostic Solution: a Custom solution is required to develop to drill down the performance bottleneck areas  Summary\n As data engineering and data analytics advances to a next level, Big data testing is inevitable. Big data processing could be Batch, Real-Time, or Interactive 3 stages of Testing Big Data applications are\n  Data staging validation   \u0026ldquo;MapReduce\u0026rdquo; validation Output validation phase  Architecture Testing is the important phase of Big data testing, as poorly designed system may lead to unprecedented errors and degradation of performance\n Performance testing for Big data includes verifying\n  Data throughput   Data processing Sub-component performance  Big data testing is very different from Traditional data testing in terms of Data, Infrastructure \u0026amp; Validation Tools\n Big Data Testing challenges include virtualization, test automation and dealing with large dataset. Performance testing of Big Data applications is also an issue.\n  13 FAQ 13 FAQ\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/13-faq/",
	"title": "13 FAQ",
	"tags": [],
	"description": "",
	"content": " Following are frequently asked questions in interviews for freshers as well experienced developer.\n1) What is Hadoop Map Reduce?\nFor processing large data sets in parallel across a Hadoop cluster, Hadoop MapReduce framework is used. Data analysis uses a two-step map and reduce process.\n2) How Hadoop MapReduce works?\nIn MapReduce, during the map phase, it counts the words in each document, while in the reduce phase it aggregates the data as per the document spanning the entire collection. During the map phase, the input data is divided into splits for analysis by map tasks running in parallel across Hadoop framework.\n3) Explain what is shuffling in MapReduce?\nThe process by which the system performs the sort and transfers the map outputs to the reducer as inputs is known as the shuffle\n4) Explain what is distributed Cache in MapReduce Framework?\nDistributed Cache is an important feature provided by the MapReduce framework. When you want to share some files across all nodes in Hadoop Cluster, Distributed Cache is used. The files could be an executable jar files or simple properties file.\n5) Explain what is NameNode in Hadoop?\nNameNode in Hadoop is the node, where Hadoop stores all the file location information in HDFS (Hadoop Distributed File System). In other words, NameNode is the centerpiece of an HDFS file system. It keeps the record of all the files in the file system and tracks the file data across the cluster or multiple machines\n6) Explain what is JobTracker in Hadoop? What are the actions followed by Hadoop?\nIn Hadoop for submitting and tracking MapReduce jobs, JobTracker is used. Job tracker run on its own JVM process\nJob Tracker performs following actions in Hadoop\n Client application submit jobs to the job tracker JobTracker communicates to the Name mode to determine data location Near the data or with available slots JobTracker locates TaskTracker nodes On chosen TaskTracker Nodes, it submits the work When a task fails, Job tracker notifies and decides what to do then. The TaskTracker nodes are monitored by JobTracker  7) Explain what is heartbeat in HDFS?\nHeartbeat is referred to a signal used between a data node and Name node, and between task tracker and job tracker, if the Name node or job tracker does not respond to the signal, then it is considered there is some issues with data node or task tracker\n8) Explain what combiners are and when you should use a combiner in a MapReduce Job?\nTo increase the efficiency of MapReduce Program, Combiners are used. The amount of data can be reduced with the help of combiner’s that need to be transferred across to the reducers. If the operation performed is commutative and associative you can use your reducer code as a combiner. The execution of combiner is not guaranteed in Hadoop\n9) What happens when a data node fails?\nWhen a data node fails\n Jobtracker and namenode detect the failure On the failed node all tasks are re-scheduled Namenode replicates the user\u0026rsquo;s data to another node  10) Explain what is Speculative Execution?\nIn Hadoop during Speculative Execution, a certain number of duplicate tasks are launched. On a different slave node, multiple copies of the same map or reduce task can be executed using Speculative Execution. In simple words, if a particular drive is taking a long time to complete a task, Hadoop will create a duplicate task on another disk. A disk that finishes the task first is retained and disks that do not finish first are killed.\n11) Explain what are the basic parameters of a Mapper?\nThe basic parameters of a Mapper are\n LongWritable and Text Text and IntWritable  12) Explain what is the function of MapReduce partitioner?\nThe function of MapReduce partitioner is to make sure that all the value of a single key goes to the same reducer, eventually which helps even distribution of the map output over the reducers\n13) Explain what is a difference between an Input Split and HDFS Block?\nThe logical division of data is known as Split while a physical division of data is known as HDFS Block\n14) Explain what happens in text format?\nIn text input format, each line in the text file is a record. Value is the content of the line while Key is the byte offset of the line. For instance, Key: longWritable, Value: text\n15) Mention what are the main configuration parameters that user need to specify to run MapReduce Job?\nThe user of the MapReduce framework needs to specify\n Job’s input locations in the distributed file system Job’s output location in the distributed file system Input format Output format Class containing the map function Class containing the reduce function JAR file containing the mapper, reducer and driver classes  16) Explain what is WebDAV in Hadoop?\nTo support editing and updating files WebDAV is a set of extensions to HTTP. On most operating system WebDAV shares can be mounted as filesystems, so it is possible to access HDFS as a standard filesystem by exposing HDFS over WebDAV.\n17) Explain what is Sqoop in Hadoop?\nTo transfer the data between Relational database management (RDBMS) and Hadoop HDFS a tool is used known as Sqoop. Using Sqoop data can be transferred from RDMS like MySQL or Oracle into HDFS as well as exporting data from HDFS file to RDBMS\n18) Explain how JobTracker schedules a task?\nThe task tracker sends out heartbeat messages to Jobtracker usually every few minutes to make sure that JobTracker is active and functioning. The message also informs JobTracker about the number of available slots, so the JobTracker can stay up to date with wherein the cluster work can be delegated\n19) Explain what is Sequencefileinputformat?\nSequencefileinputformat is used for reading files in sequence. It is a specific compressed binary file format which is optimized for passing data between the output of one MapReduce job to the input of some other MapReduce job.\n20) Explain what does the conf.setMapper Class do?\nConf.setMapperclass sets the mapper class and all the stuff related to map job such as reading data and generating a key-value pair out of the mapper\n21) Explain what is Hadoop?\nIt is an open-source software framework for storing data and running applications on clusters of commodity hardware. It provides enormous processing power and massive storage for any type of data.\n22) Mention what is the difference between an RDBMS and Hadoop?\n   RDBMS Hadoop     RDBMS is a relational database management system Hadoop is a node based flat structure   It used for OLTP processing whereas Hadoop It is currently used for analytical and for BIG DATA processing   In RDBMS, the database cluster uses the same data files stored in a shared storage In Hadoop, the storage data can be stored independently in each processing node.   You need to preprocess data before storing it you don’t need to preprocess data before storing it    23) Mention Hadoop core components?\nHadoop core components include,\n HDFS MapReduce  24) What is NameNode in Hadoop?\nNameNode in Hadoop is where Hadoop stores all the file location information in HDFS. It is the master node on which job tracker runs and consists of metadata.\n25) Mention what are the data components used by Hadoop?\nData components used by Hadoop are\n Pig Hive  26) Mention what is the data storage component used by Hadoop?\nThe data storage component used by Hadoop is HBase.\n27) Mention what are the most common input formats defined in Hadoop?\nThe most common input formats defined in Hadoop are;\n TextInputFormat KeyValueInputFormat SequenceFileInputFormat  28) In Hadoop what is InputSplit?\nIt splits input files into chunks and assigns each split to a mapper for processing.\n29) For a Hadoop job, how will you write a custom partitioner?\nYou write a custom partitioner for a Hadoop job, you follow the following path\n Create a new class that extends Partitioner Class Override method getPartition In the wrapper that runs the MapReduce Add the custom partitioner to the job by using method set Partitioner Class or – add the custom partitioner to the job as a config file  30) For a job in Hadoop, is it possible to change the number of mappers to be created?\nNo, it is not possible to change the number of mappers to be created. The number of mappers is determined by the number of input splits.\n31) Explain what is a sequence file in Hadoop?\nTo store binary key/value pairs, sequence file is used. Unlike regular compressed file, sequence file support splitting even when the data inside the file is compressed.\n32) When Namenode is down what happens to job tracker?\nNamenode is the single point of failure in HDFS so when Namenode is down your cluster will set off.\n33) Explain how indexing in HDFS is done?\nHadoop has a unique way of indexing. Once the data is stored as per the block size, the HDFS will keep on storing the last part of the data which say where the next part of the data will be.\n34) Explain is it possible to search for files using wildcards?\nYes, it is possible to search for files using wildcards.\n35) List out Hadoop’s three configuration files?\nThe three configuration files are\n core-site.xml mapred-site.xml hdfs-site.xml  36) Explain how can you check whether Namenode is working beside using the jps command?\nBesides using the jps command, to check whether Namenode are working you can also use\n/etc/init.d/hadoop-0.20-namenode status.\n37) Explain what is “map” and what is \u0026ldquo;reducer\u0026rdquo; in Hadoop?\nIn Hadoop, a map is a phase in HDFS query solving. A map reads data from an input location, and outputs a key value pair according to the input type.\nIn Hadoop, a reducer collects the output generated by the mapper, processes it, and creates a final output of its own.\n38) In Hadoop, which file controls reporting in Hadoop?\nIn Hadoop, the hadoop-metrics.properties file controls reporting.\n39) For using Hadoop list the network requirements?\nFor using Hadoop the list of network requirements are:\n Password-less SSH connection Secure Shell (SSH) for launching server processes  40) Mention what is rack awareness?\nRack awareness is the way in which the namenode determines on how to place blocks based on the rack definitions.\n41) Explain what is a Task Tracker in Hadoop?\nA Task Tracker in Hadoop is a slave node daemon in the cluster that accepts tasks from a JobTracker. It also sends out the heartbeat messages to the JobTracker, every few minutes, to confirm that the JobTracker is still alive.\n42) Mention what daemons run on a master node and slave nodes?\n Daemons run on Master node is \u0026ldquo;NameNode\u0026rdquo; Daemons run on each Slave nodes are “Task Tracker” and \u0026ldquo;Data\u0026rdquo;  43) Explain how can you debug Hadoop code?\nThe popular methods for debugging Hadoop code are:\n By using web interface provided by Hadoop framework By using Counters  44) Explain what is storage and compute nodes?\n The storage node is the machine or computer where your file system resides to store the processing data The compute node is the computer or machine where your actual business logic will be executed.  45) Mention what is the use of Context Object?\nThe Context Object enables the mapper to interact with the rest of the Hadoop\nsystem. It includes configuration data for the job, as well as interfaces which allow it to emit output.\n46) Mention what is the next step after Mapper or MapTask?\nThe next step after Mapper or MapTask is that the output of the Mapper are sorted, and partitions will be created for the output.\n47) Mention what is the number of default partitioner in Hadoop?\nIn Hadoop, the default partitioner is a “Hash” Partitioner.\n48) Explain what is the purpose of RecordReader in Hadoop?\nIn Hadoop, the RecordReader loads the data from its source and converts it into (key, value) pairs suitable for reading by the Mapper.\n49) Explain how is data partitioned before it is sent to the reducer if no custom partitioner is defined in Hadoop?\nIf no custom partitioner is defined in Hadoop, then a default partitioner computes a hash value for the key and assigns the partition based on the result.\n50) Explain what happens when Hadoop spawned 50 tasks for a job and one of the task failed?\nIt will restart the task again on some other TaskTracker if the task fails more than the defined limit.\n51) Mention what is the best way to copy files between HDFS clusters?\nThe best way to copy files between HDFS clusters is by using multiple nodes and the distcp command, so the workload is shared.\n52) Mention what is the difference between HDFS and NAS?\nHDFS data blocks are distributed across local drives of all machines in a cluster while NAS data is stored on dedicated hardware.\n53) Mention how Hadoop is different from other data processing tools?\nIn Hadoop, you can increase or decrease the number of mappers without worrying about the volume of data to be processed.\n54) Mention what job does the conf class do?\nJob conf class separate different jobs running on the same cluster. It does the job level settings such as declaring a job in a real environment.\n55) Mention what is the Hadoop MapReduce APIs contract for a key and value class?\nFor a key and value class, there are two Hadoop MapReduce APIs contract\n The value must be defining the org.apache.hadoop.io.Writable interface The key must be defining the org.apache.hadoop.io.WritableComparable interface  56) Mention what are the three modes in which Hadoop can be run?\nThe three modes in which Hadoop can be run are\n Pseudo distributed mode Standalone (local) mode Fully distributed mode  57) Mention what does the text input format do?\nThe text input format will create a line object that is an hexadecimal number. The value is considered as a whole line text while the key is considered as a line object. The mapper will receive the value as ‘text’ parameter while key as ‘longwriteable’ parameter.\n58) Mention how many InputSplits is made by a Hadoop Framework?\nHadoop will make 5 splits\n 1 split for 64K files 2 split for 65mb files 2 splits for 127mb files  59) Mention what is distributed cache in Hadoop?\nDistributed cache in Hadoop is a facility provided by MapReduce framework. At the time of execution of the job, it is used to cache file. The Framework copies the necessary files to the slave node before the execution of any task at that node.\n60) Explain how does Hadoop Classpath plays a vital role in stopping or starting in Hadoop daemons?\nClasspath will consist of a list of directories containing jar files to stop or start daemons.\n14 top tools of BigData 14 Top tools of Big data\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/14-top-tools-of-bigdata/",
	"title": "14 Top-tools-BigData",
	"tags": [],
	"description": "",
	"content": " Today\u0026rsquo;s market is flooded with an array of Big Data tools. They bring cost efficiency, better time management into the data analytical tasks. Here is the list of best big data tools with their key features and download links.\n1) Hadoop:\nThe Apache Hadoop software library is a big data framework. It allows distributed processing of large data sets across clusters of computers. It is designed to scale up from single servers to thousands of machines.\nFeatures:\n Authentication improvements when using HTTP proxy server Specification for Hadoop Compatible Filesystem effort Support for POSIX-style filesystem extended attributes It offers robust ecosystem that is well suited to meet the analytical needs of developer It brings Flexibility In Data Processing It allows for faster data Processing  Download link: https://hadoop.apache.org/releases.html\n2) HPCC:\nHPCC is a big data tool developed by LexisNexis Risk Solution. It delivers on a single platform, a single architecture and a single programming language for data processing.\nFeatures:\n Highly efficient accomplish big data tasks with far less code. Offers high redundancy and availability It can be used both for complex data processing on a Thor cluster Graphical IDE for simplifies development, testing and debugging It automatically optimizes code for parallel processing Provide enhance scalability and performance ECL code compiles into optimized C++, and it can also extend using C++ libraries  Download link: https://hpccsystems.com/try-now\n3) Storm:\nStorm is a free and open source big data computation system. It offers distributed real-time, fault-tolerant processing system. With real-time computation capabilities.\nFeatures:\n It benchmarked as processing one million 100 byte messages per second per node It uses parallel calculations that run across a cluster of machines It will automatically restart in case a node dies. The worker will be restarted on another node Storm guarantees that each unit of data will be processed at least once or exactly once Once deployed Storm is surely easiest tool for Bigdata analysis  Download link: http://storm.apache.org/downloads.html\n4) Qubole:\nQubole Data is Autonomous Big data management platform. It is self-managed, self-optimizing tool which allows the data team to focus on business outcomes.\nFeatures:\n Single Platform for every use case Open-source Engines, optimized for the Cloud Comprehensive Security, Governance, and Compliance Provides actionable Alerts, Insights, and Recommendations to optimize reliability, performance, and costs Automatically enacts policies to avoid performing repetitive manual actions  Download link: https://www.qubole.com/\n5) Cassandra:\nThe Apache Cassandra database is widely used today to provide an effective management of large amounts of data.\nFeatures:\n Support for replicating across multiple data centers by providing lower latency for users Data is automatically replicated to multiple nodes for fault-tolerance It is most suitable for applications that can\u0026rsquo;t afford to lose data, even when an entire data center is down Cassandra offers support contracts and services are available from third parties  Download link: http://cassandra.apache.org/download/\n6) Statwing:\nStatwing is an easy-to-use statistical tool. It was built by and for big data analysts. Its modern interface chooses statistical tests automatically.\nFeatures:\n Explore any data in seconds Statwing helps to clean data, explore relationships, and create charts in minutes It allows creating histograms, scatterplots, heatmaps, and bar charts that export to Excel or PowerPoint It also translates results into plain English, so analysts unfamiliar with statistical analysis  Download link: https://www.statwing.com/\n7) CouchDB:\nCouchDB stores data in JSON documents that can be accessed web or query using JavaScript. It offers distributed scaling with fault-tolerant storage. It allows accessing data by defining the Couch Replication Protocol.\nFeatures:\n CouchDB is a single-node database that works like any other database It allows running a single logical database server on any number of servers It makes use of the ubiquitous HTTP protocol and JSON data format Easy replication of a database across multiple server instances Easy interface for document insertion, updates, retrieval and deletion JSON-based document format can be translatable across different languages  Download link: http://couchdb.apache.org/\n8) Pentaho:\nPentaho provides big data tools to extract, prepare and blend data. It offers visualizations and analytics that change the way to run any business. This Big data tool allows turning big data into big insights.\nFeatures:\n Data access and integration for effective data visualization It empowers users to architect big data at the source and stream them for accurate analytics Seamlessly switch or combine data processing with in-cluster execution to get maximum processing Allow checking data with easy access to analytics, including charts, visualizations, and reporting Supports wide spectrum of big data sources by offering unique capabilities  Download link: http://www.pentaho.com/download\n9) Flink:\nApache Flink is an open-source stream processing Big data tool. It is distributed, high-performing, always-available, and accurate data streaming applications.\nFeatures:\n Provides results that are accurate, even for out-of-order or late-arriving data It is stateful and fault-tolerant and can recover from failures It can perform at a large scale, running on thousands of nodes Has good throughput and latency characteristics This big data tool supports stream processing and windowing with event time semantics It supports flexible windowing based on time, count, or sessions to data-driven windows It supports a wide range of connectors to third-party systems for data sources and sinks  Download link: https://flink.apache.org/\n10) Cloudera:\nCloudera is the fastest, easiest and highly secure modern big data platform. It allows anyone to get any data across any environment within single, scalable platform.\nFeatures:\n High-performance analytics It offers provision for multi-cloud Deploy and manage Cloudera Enterprise across AWS, Microsoft Azure and Google Cloud Platform Spin up and terminate clusters, and only pay for what is needed when need it Developing and training data models Reporting, exploring, and self-servicing business intelligence Delivering real-time insights for monitoring and detection Conducting accurate model scoring and serving  Download link: https://www.cloudera.com/\n11) Openrefine:\nOpen Refine is a powerful big data tool. It helps to work with messy data, cleaning it and transforming it from one format into another. It also allows extending it with web services and external data.\nFeatures:\n OpenRefine tool help you explore large data sets with ease It can be used to link and extend your dataset with various webservices Import data in various formats Explore datasets in a matter of seconds Apply basic and advanced cell transformations Allows to deal with cells that contain multiple values Create instantaneous links between datasets Use named-entity extraction on text fields to automatically identify topics Perform advanced data operations with the help of Refine Expression Language  Download link: http://openrefine.org/download.html\n12) Rapidminer:\nRapidMiner is an open source big data tool. It is used for data prep, machine learning, and model deployment. It offers a suite of products to build new data mining processes and setup predictive analysis.\nFeatures:\n Allow multiple data management methods GUI or batch processing Integrates with in-house databases Interactive, shareable dashboards Big Data predictive analytics Remote analysis processing Data filtering, merging, joining and aggregating Build, train and validate predictive models Store streaming data to numerous databases Reports and triggered notifications  Download link: https://my.rapidminer.com/nexus/account/index.html#downloads\n13) DataCleaner:\nDataCleaner is a data quality analysis application and a solution platform. It has strong data profiling engine. It is extensible and thereby adds data cleansing, transformations, matching, and merging.\nFeature:\n Interactive and explorative data profiling Fuzzy duplicate record detection Data transformation and standardization Data validation and reporting Use of reference data to cleanse data Master the data ingestion pipeline in Hadoop data lake Ensure that rules about the data are correct before user spends thier time on the processing Find the outliers and other devilish details to either exclude or fix the incorrect data  Download link: http://datacleaner.org/\n14) Kaggle:\nKaggle is the world\u0026rsquo;s largest big data community. It helps organizations and researchers to post their data \u0026amp; statistics. It is the best place to analyze data seamlessly.\nFeatures:\n The best place to discover and seamlessly analyze open data Search box to find open datasets Contribute to the open data movement and connect with other data enthusiasts  Download link: https://www.kaggle.com/\n15) Hive:\nHive is an open source-software big data too. It allows programmers analyze large data sets on Hadoop. It helps with querying and managing large datasets real fast.\nFeatures:\n It Supports SQL like query language for interaction and Data modeling It compiles language with two main tasks map, and reducer It allows defining these tasks using Java or Python Hive designed for managing and querying only structured data Hive\u0026rsquo;s SQL-inspired language separates the user from the complexity of Map Reduce programming It offers Java Database Connectivity (JDBC) interface  Download link: https://hive.apache.org/downloads.html\n15 top bigData analytics tool 15 top bigData analytics tool\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/15-top-bigdata-analytics-tool/",
	"title": "15 Top-BigData-Analytics-tools",
	"tags": [],
	"description": "",
	"content": " Big Data Analytics software is widely used in providing meaningful analysis of a large set of data. This software helps in finding current market trends, customer preferences, and other information.\nHere are the 11 Top Big Data Analytics Tools with key feature and download links.\n1) Microsoft HDInsight:\nAzure HDInsight is a Spark and Hadoop service in the cloud. It provides big data cloud offerings in two categories, Standard and Premium. It provides an enterprise-scale cluster for the organization to run their big data workloads.\nFeatures:\n Reliable analytics with an industry-leading SLA It offers enterprise-grade security and monitoring Protect data assets and extend on-premises security and governance controls to the cloud High-productivity platform for developers and scientists Integration with leading productivity applications Deploy Hadoop in the cloud without purchasing new hardware or paying other up-front costs  Download link: https://azure.microsoft.com/en-in/free/\n2) Skytree:\nSkytree is a big data analytics tool that empowers data scientists to build more accurate models faster. It offers accurate predictive machine learning models that are easy to use.\nFeatures:\n Highly Scalable Algorithms Artificial Intelligence for Data Scientists It allows data scientists to visualize and understand the logic behind ML decisions Skytree via the easy-to-adopt GUI or programmatically in Java Model Interpretability It is designed to solve robust predictive problems with data preparation capabilities Programmatic and GUI Access  Download link: http://www.skytree.net/\n3) Talend:\nTalend is a big data tool that simplifies and automates big data integration. Its graphical wizard generates native code. It also allows big data integration, master data management and checks data quality.\nFeatures:\n Accelerate time to value for big data projects Simplify ETL \u0026amp; ELT for big data Talend Big Data Platform simplifies using MapReduce and Spark by generating native code Smarter data quality with machine learning and natural language processing Agile DevOps to speed up big data projects Streamline all the DevOps processes  Download Link: https://www.talend.com/download/\n4) Splice Machine:\nSplice Machine is a big data analytic tool. Their architecture is portable across public clouds such as AWS, Azure, and Google.\nFeatures:\n It can dynamically scale from a few to thousands of nodes to enable applications at every scale The Splice Machine optimizer automatically evaluates every query to the distributed HBase regions Reduce management, deploy faster, and reduce risk Consume fast streaming data, develop, test and deploy machine learning models  Download link: https://www.splicemachine.com\n5) Spark:\nApache Spark is a powerful open source big data analytics tool. It offers over 80 high-level operators that make it easy to build parallel apps. It is used at a wide range of organizations to process large datasets.\nFeatures:\n It helps to run an application in Hadoop cluster, up to 100 times faster in memory, and ten times faster on disk It offers lighting Fast Processing Support for Sophisticated Analytics Ability to Integrate with Hadoop and Existing Hadoop Data It provides built-in APIs in Java, Scala, or Python  Download link: https://spark.apache.org/downloads.html\n6) Plotly:\nPlotly is an analytics tool that lets users create charts and dashboards to share online.\nFeatures:\n Easily turn any data into eye-catching and informative graphics It provides audited industries with fine-grained information on data provenance Plotly offers unlimited public file hosting through its free community plan  Download link: https://plot.ly/\n7) Apache SAMOA:\nApache SAMOA is a big data analytics tool. It enables development of new ML algorithms. It provides a collection of distributed algorithms for common data mining and machine learning tasks.\nDownload link: https://samoa.incubator.apache.org/\n8) Lumify:\nLumify is a big data fusion, analysis, and visualization platform. It helps users to discover connections and explore relationships in their data via a suite of analytic options.\nFeatures:\n It provides both 2D and 3D graph visualizations with a variety of automatic layouts It provides a variety of options for analyzing the links between entities on the graph It comes with specific ingest processing and interface elements for textual content, images, and videos It spaces feature allows you to organize work into a set of projects, or workspaces It is built on proven, scalable big data technologies  Download link: http://www.altamiracorp.com/index.php/lumify/\n9) Elasticsearch:\nElasticsearch is a JSON-based Big data search and analytics engine. It is a distributed, RESTful search and analytics engine for solving numbers of use cases. It offers horizontal scalability, maximum reliability, and easy management.\nFeatures:\n It allows combine many types of searches such as structured, unstructured, geo, metric, etc Intuitive APIs for monitoring and management give complete visibility and control It uses standard RESTful APIs and JSON. It also builds and maintains clients in many languages like Java, Python, NET, and Groovy Real-time search and analytics features to work big data by using the Elasticsearch-Hadoop It gives an enhanced experience with security, monitoring, reporting, and machine learning features  Download link: https://www.elastic.co/downloads/elasticsearch\n10) R-Programming:\nR is a language for statistical computing and graphics. It also used for big data analysis. It provides a wide variety of statistical tests.\nFeatures:\n Effective data handling and storage facility, It provides a suite of operators for calculations on arrays, in particular, matrices, It provides coherent, integrated collection of big data tools for data analysis It provides graphical facilities for data analysis which display either on-screen or on hardcopy  Download link: https://www.r-project.org/\n11) IBM SPSS Modeler:\nIBM SPSS Modeler is a predictive big data analytics platform. It offers predictive models and delivers to individuals, groups, systems and the enterprise. It has a range of advanced algorithms and analysis techniques.\nFeatures:\n Discover insights and solve problems faster by analyzing structured and unstructured data Use an intuitive interface for everyone to learn You can select from on-premises, cloud and hybrid deployment options Quickly choose the best performing algorithm based on model performance  Download link: https://www.ibm.com/us-en/marketplace/spss-modeler/purchase#product-header-top\nCongratulations ! You have completed this tutorial. You can go over again from below 01 BigData Intro 01 Introduction\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/hadoop_setup_3nodes/",
	"title": "3nodes-Hadoop-installation-setup",
	"tags": [],
	"description": "",
	"content": " Setup 3 nodes Hadoop Hadoop Introduction Hadoop is an open-source Apache project that allows creation of parallel processing applications on large data sets, distributed across networked nodes. It\u0026rsquo;s composed of the Hadoop Distributed File System (HDFS™) that handles scalability and redundancy of data across nodes, and Hadoop YARN: a framework for job scheduling that executes data processing tasks on all nodes.\nBefore You Begin  Follow the Getting Started guide to create three (3) nodes. They\u0026rsquo;ll be referred to throughout this guide as node-master, node1 and node2. It\u0026rsquo;s recommended that you set the hostname of each node to match this naming convention.  Run the steps in this guide from the node-master unless otherwise specified.\n Follow the Securing Your Server guide to harden the three servers. Create a normal user for the install, and a user called hadoop for any Hadoop daemons. Do not create SSH keys for hadoop users. SSH keys will be addressed in a later section. Install the JDK using the appropriate guide for your distribution, Debian, CentOS or Ubuntu, or grab the latest JDK from Oracle. The steps below use example IPs for each node. Adjust each example according to your configuration:   - node-master: 192.0.2.1  node1: 192.0.2.2 node2: 192.0.2.3   This guide is written for a non-root user. Commands that require elevated privileges are prefixed with sudo. If you’re not familiar with the sudo command, see the [Users and Groups](https://github.com/linode/docs/blob/master/docs/tools-reference/linux-users-and-groups) guide. All commands in this guide are run with the *hadoop* user if not specified otherwise.  Architecture of a Hadoop Cluster Before configuring the master and slave nodes, it\u0026rsquo;s important to understand the different components of a Hadoop cluster.\nA master node keeps knowledge about the distributed file system, like the inode table on an ext3 filesystem, and schedules resources allocation. node-master will handle this role in this guide, and host two daemons:\n The NameNode: manages the distributed file system and knows where stored data blocks inside the cluster are. The ResourceManager: manages the YARN jobs and takes care of scheduling and executing processes on slave nodes.  Slave nodes store the actual data and provide processing power to run the jobs. They\u0026rsquo;ll be node1 and node2, and will host two daemons:\n The DataNode manages the actual data physically stored on the node; it\u0026rsquo;s named, NameNode. The NodeManager manages execution of tasks on the node.  Configure the System Create Host File on Each Node For each node to communicate with its names, edit the /etc/hosts file to add the IP address of the three servers. Don\u0026rsquo;t forget to replace the sample IP with your IP:\nfile \u0026ldquo;/etc/hosts\u0026rdquo; \u0026gt; 192.0.2.1 node-master 192.0.2.2 node1 192.0.2.3 node2 file\nDistribute Authentication Key-pairs for the Hadoop User\nThe master node will use an ssh-connection to connect to other nodes with key-pair authentication, to manage the cluster.\n Login to node-master as the hadoop user, and generate an ssh-key:  ssh-keygen -b 4096\n Copy the key to the other nodes. It\u0026rsquo;s good practice to also copy the key to the node-master itself, so that you can also use it as a DataNode if needed. Type the following commands, and enter the hadoop user\u0026rsquo;s password when asked. If you are prompted whether or not to add the key to known hosts, enter yes:  ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@node-master ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@node1 ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@node2\nDownload and Unpack Hadoop Binaries\nLogin to node-master as the hadoop user, download the Hadoop tarball from Hadoop project page, and unzip it:\ncd wget http://apache.mindstudios.com/hadoop/common/hadoop-2.8.1/hadoop-2.8.1.tar.gz tar -xzf hadoop-2.8.1.tar.gz mv hadoop-2.8.1 hadoop\nSet Environment Variables\n Add Hadoop binaries to your PATH. Edit /home/hadoop/.profile and add the following line:  file \u0026ldquo;/home/hadoop/.profile\u0026rdquo; shell \u0026gt; PATH=/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:$PATH\nfile\nConfigure the Master Node\nConfiguration will be done on node-master and replicated to other nodes.\nSet JAVA_HOME\n Get your Java installation path. If you installed open-jdk from your package manager, you can get the path with the command:  update-alternatives \u0026ndash;display java\nTake the value of the current link and remove the trailing /bin/java. For example on Debian, the link is /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java, so JAVA_HOME should be /usr/lib/jvm/java-8-openjdk-amd64/jre.\nIf you installed java from Oracle, JAVA_HOME is the path where you unzipped the java archive.\n Edit ~/hadoop/etc/hadoop/hadoop-env.sh and replace this line:  export JAVA_HOME=${JAVA_HOME}\nwith your actual java installation path. For example on a Debian with open-jdk-8:\nfile \u0026ldquo;~/hadoop/etc/hadoop/hadoop-env.sh\u0026rdquo; shell \u0026gt;}} export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre\nfile\nSet NameNode Location\nOn each node update ~/hadoop/etc/hadoop/core-site.xml you want to set the NameNode location to node-master on port 9000:\nfile \u0026ldquo;~/hadoop/etc/hadoop/core-site.xml\u0026rdquo; xml \u0026gt;}}\n  fs.default.name hdfs://node-master:9000  \nfile\nSet path for HDFS Edit hdfs-site.conf:\nfile \u0026ldquo;~/hadoop/etc/hadoop/hdfs-site.xml\u0026rdquo; xml \u0026gt;}} dfs.namenode.name.dir /home/hadoop/data/nameNode\n dfs.datanode.data.dir /home/hadoop/data/dataNode \n dfs.replication 1 \nfile\nThe last property, dfs.replication, indicates how many times data is replicated in the cluster. You can set 2 to have all the data duplicated on the two nodes. Don\u0026rsquo;t enter a value higher than the actual number of slave nodes.\nSet YARN as Job Scheduler  In ~/hadoop/etc/hadoop/, rename mapred-site.xml.template to mapred-site.xml:  cd ~/hadoop/etc/hadoop mv mapred-site.xml.template mapred-site.xml\n Edit the file, setting yarn as the default framework for MapReduce operations:  file \u0026ldquo;~/hadoop/etc/hadoop/mapred-site.xml\u0026rdquo; xml \u0026gt;}}\nmapreduce.framework.name yarn\nfile\nConfigure YARN\nEdit yarn-site.xml:\nfile\u0026rdquo;~/hadoop/etc/hadoop/yarn-site.xml\u0026rdquo; xml \u0026gt;}} yarn.acl.enable 0\n yarn.resourcemanager.hostname node-master \n yarn.nodemanager.aux-services mapreduce_shuffle \nfile\nConfigure Slaves\nThe file slaves is used by startup scripts to start required daemons on all nodes. Edit ~/hadoop/etc/hadoop/slaves to be:\nfile\u0026rdquo;~/hadoop/etc/hadoop/slaves\u0026rdquo; resource \u0026gt;}} node1 node2\nfile\nConfigure Memory Allocation Memory allocation can be tricky on low RAM nodes because default values are not suitable for nodes with less than 8GB of RAM. This section will highlight how memory allocation works for MapReduce jobs, and provide a sample configuration for 2GB RAM nodes.\nThe Memory Allocation Properties\nA YARN job is executed with two kind of resources:  An Application Master (AM) is responsible for monitoring the application and coordinating distributed executors in the cluster. Some executors that are created by the AM actually run the job. For a MapReduce jobs, they\u0026rsquo;ll perform map or reduce operation, in parallel.  Both are run in containers on slave nodes. Each slave node runs a NodeManager daemon that\u0026rsquo;s responsible for container creation on the node. The whole cluster is managed by a ResourceManager that schedules container allocation on all the slave-nodes, depending on capacity requirements and current charge.\nFour types of resource allocations need to be configured properly for the cluster to work. These are:\n How much memory can be allocated for YARN containers on a single node. This limit should be higher than all the others; otherwise, container allocation will be rejected and applications will fail. However, it should not be the entire amount of RAM on the node.  This value is configured in yarn-site.xml with yarn.nodemanager.resource.memory-mb.\n How much memory a single container can consume and the minimum memory allocation allowed. A container will never be bigger than the maximum, or else allocation will fail and will always be allocated as a multiple of the minimum amount of RAM.  Those values are configured in yarn-site.xml with yarn.scheduler.maximum-allocation-mb and yarn.scheduler.minimum-allocation-mb.\n How much memory will be allocated to the ApplicationMaster. This is a constant value that should fit in the container maximum size.  This is configured in mapred-site.xml with yarn.app.mapreduce.am.resource.mb.\n How much memory will be allocated to each map or reduce operation. This should be less than the maximum size.  This is configured in mapred-site.xml with properties mapreduce.map.memory.mb and mapreduce.reduce.memory.mb.\nThe relationship between all those properties can be seen in the following figure:\nSample Configuration for 2GB Nodes For 2GB nodes, a working configuration may be:\n   Property Value     yarn.nodemanager.resource.memory-mb 1536   yarn.scheduler.maximum-allocation-mb 1536   yarn.scheduler.minimum-allocation-mb 128   yarn.app.mapreduce.am.resource.mb 512   mapreduce.map.memory.mb 256   mapreduce.reduce.memory.mb 256     Edit /home/hadoop/hadoop/etc/hadoop/yarn-site.xml and add the following lines:  file\u0026rdquo;~/hadoop/etc/hadoop/yarn-site.xml\u0026rdquo; xml \u0026gt;}}\nyarn.nodemanager.resource.memory-mb 1536 yarn.scheduler.maximum-allocation-mb 1536 yarn.scheduler.minimum-allocation-mb 128 yarn.nodemanager.vmem-check-enabled false\nfile\nThe last property disables virtual-memory checking and can prevent containers from being allocated properly on JDK8.\n Edit /home/hadoop/hadoop/etc/hadoop/mapred-site.xml and add the following lines:  file\u0026rdquo;~/hadoop/etc/hadoop/mapred-site.xml\u0026rdquo; xml \u0026gt;}}\nyarn.app.mapreduce.am.resource.mb 512 mapreduce.map.memory.mb 256 mapreduce.reduce.memory.mb 256\nfile\nDuplicate Config Files on Each Node  Copy the hadoop binaries to slave nodes:  cd /home/hadoop/ scp hadoop-.tar.gz node1:/home/hadoop scp hadoop-.tar.gz node2:/home/hadoop\n Connect to node1 via ssh. A password isn\u0026rsquo;t required, thanks to the ssh keys copied above:  ssh node1\n Unzip the binaries, rename the directory, and exit node1 to get back on the node-master:  tar -xzf hadoop-2.8.1.tar.gz mv hadoop-2.8.1 hadoop exit\n Repeat steps 2 and 3 for node2. Copy the Hadoop configuration files to the slave nodes:  for node in node1 node2; do scp ~/hadoop/etc/hadoop/* $node:/home/hadoop/hadoop/etc/hadoop/; done\nFormat HDFS HDFS needs to be formatted like any classical file system. On node-master, run the following command:\nhdfs namenode -format\nYour Hadoop installation is now configured and ready to run.\nRun and monitor HDFS This section will walk through starting HDFS on NameNode and DataNodes, and monitoring that everything is properly working and interacting with HDFS data.\nStart and Stop HDFS  Start the HDFS by running the following script from node-master:  start-dfs.sh\nIt\u0026rsquo;ll start NameNode and SecondaryNameNode on node-master, and DataNode on node1 and node2, according to the configuration in the slaves config file.\n Check that every process is running with the jps command on each node. You should get on node-master (PID will be different):  21922 Jps 21603 NameNode 21787 SecondaryNameNode\nand on node1 and node2:\n19728 DataNode 19819 Jps\n To stop HDFS on master and slave nodes, run the following command from node-master:  stop-dfs.sh\nMonitor your HDFS Cluster  You can get useful information about running your HDFS cluster with the hdfs dfsadmin command. Try for example:  hdfs dfsadmin -report\nThis will print information (e.g., capacity and usage) for all running DataNodes. To get the description of all available commands, type:\nhdfs dfsadmin -help\n You can also automatically use the friendlier web user interface. Point your browser to http://node-master-IP:50070and you\u0026rsquo;ll get a user-friendly monitoring console.  Put and Get Data to HDFS Writing and reading to HDFS is done with command hdfs dfs. First, manually create your home directory. All other commands will use a path relative to this default home directory:\nhdfs dfs -mkdir -p /user/hadoop\nLet\u0026rsquo;s use some textbooks from the Gutenberg project as an example.\n Create a books directory in HDFS. The following command will create it in the home directory, /user/hadoop/books:  hdfs dfs -mkdir books\n Grab a few books from the Gutenberg project:  cd /home/hadoop wget -O alice.txt https://www.gutenberg.org/files/11/11-0.txt wget -O holmes.txt https://www.gutenberg.org/ebooks/1661.txt.utf-8 wget -O frankenstein.txt https://www.gutenberg.org/ebooks/84.txt.utf-8\n Put the three books through HDFS, in the booksdirectory:  hdfs dfs -put alice.txt holmes.txt frankenstein.txt books\n List the contents of the book directory:  hdfs dfs -ls books\n Move one of the books to the local filesystem:  hdfs dfs -get books/alice.txt\n You can also directly print the books from HDFS:  hdfs dfs -cat books/alice.txt\nThere are many commands to manage your HDFS. For a complete list, you can look at the Apache HDFS shell documentation, or print help with:\nhdfs dfs -help\nRun YARN HDFS is a distributed storage system, it doesn\u0026rsquo;t provide any services for running and scheduling tasks in the cluster. This is the role of the YARN framework. The following section is about starting, monitoring, and submitting jobs to YARN.\nStart and Stop YARN\n Start YARN with the script:  start-yarn.sh\n Check that everything is running with the jps command. In addition to the previous HDFS daemon, you should see a ResourceManager on node-master, and a NodeManager on node1 and node2. To stop YARN, run the following command on node-master:  stop-yarn.sh\nMonitor YARN\n The yarn command provides utilities to manage your YARN cluster. You can also print a report of running nodes with the command:  yarn node -list\nSimilarly, you can get a list of running applications with command:\nyarn application -list\nTo get all available parameters of the yarn command, see Apache YARN documentation.\n As with HDFS, YARN provides a friendlier web UI, started by default on port 8088 of the Resource Manager. Point your browser to http://node-master-IP:8088 and browse the UI:  Submit MapReduce Jobs to YARN\nYarn jobs are packaged into jar files and submitted to YARN for execution with the command yarn jar. The Hadoop installation package provides sample applications that can be run to test your cluster. You\u0026rsquo;ll use them to run a word count on the three books previously uploaded to HDFS.\n Submit a job with the sample jar to YARN. On node-master, run:  yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar wordcount \u0026ldquo;books/*\u0026rdquo; output\nThe last argument is where the output of the job will be saved - in HDFS.\n After the job is finished, you can get the result by querying HDFS with hdfs dfs -ls output. In case of a success, the output will resemble:  Found 2 items -rw-r\u0026ndash;r\u0026ndash; 1 hadoop supergroup 0 2017-10-11 14:09 output/_SUCCESS -rw-r\u0026ndash;r\u0026ndash; 1 hadoop supergroup 269158 2017-10-11 14:09 output/part-r-00000\n Print the result with:  hdfs dfs -cat output/part-r-00000\nNext Steps\nNow that you have a YARN cluster up and running, you can:\n Learn how to code your own YARN jobs with Apache documentation. Install Spark on top on your YARN cluster with Linode Spark guide.  "
},
{
	"uri": "https://shengzhenfu.github.io/getting-start/configuration/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": "When building the website, you can set a theme by using --theme option. We suggest you to edit your configuration file and set the theme by default. Example with config.toml format. Import sample config from sample site to Hugo root:\n$ cp themes/docdock/exampleSite/config.toml .  Change following config.toml line as needed, depending on method above:\ntheme = \u0026quot;\u0026lt;hugo-theme-docdock-dir-name\u0026gt;\u0026quot;  Comment out following line, so default themes/ will be used:\n# themesdir = \u0026quot;../..\u0026quot;  Activate search If not already present, add the follow lines to the config.toml file.\n[outputs] home = [ \u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] LUNRJS search index file will be generated on content changes.\n(Bonus) Create empty file .gitkeep inside public/ and add following to .gitignore. This way it will keep repo smaller and won\u0026rsquo;t bring build result files and errors to remote checkout places:\n/public/* !/public/.gitkeep  Preview site $ hugo server  to browse site on http://localhost:1313\nYour website\u0026rsquo;s content Find out how to create and organize your content quickly and intuitively.\n"
},
{
	"uri": "https://shengzhenfu.github.io/content-organisation/customize-style/",
	"title": "Customize website look and feel",
	"tags": [],
	"description": "",
	"content": " You can change the style and behavior of the theme without touching it.\n inject your own html, css or js into the page overide existing css or js with your own files  No needs to copy the entire theme to customize some parts Bellow are solutions to avoid copying the entire theme into your own codebase.\n Add custom CSS and JS or HTML into the \u0026lt;head\u0026gt; part of each page : Create a custom header partial layouts/partials/custom-head.html\n  content/ layouts/  partials/  custom-head.html     write your own content like (an example from @nzbart):\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;/css/custom.css\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;/js/custom.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Then overrode the style your want to change in static/css/custom.css (in this case, to avoid altering the casing of titles):\nh2 { text-transform: none; } And executed some additional JavaScript from static/js/custom.js (note that jQuery is already loaded by the theme):\nfunction tweakPage() { // make some changes here } $(tweakPage)  now feel free to add the JS, CSS, HTML code you want :)\nAdd custom HTML at the end of the body part of each page : Create a custom-footer.html into a layouts/partials folder next to the content folder\n  content/ layouts/  partials/  custom-footer.html     now feel free to add the JS, CSS, HTML code you want :)\nOveride existing CSS or JS Create the matching file in your static folder, hugo will use yours instead of the theme\u0026rsquo;s one. Example :\ncreate a theme.css and place it into static/css/ to fully overide docdock\u0026rsquo;s theme.css\n"
},
{
	"uri": "https://shengzhenfu.github.io/create-page/",
	"title": "Create Page-Doc hub",
	"tags": ["tag1", "tag2"],
	"description": "",
	"content": " Hugo-theme-docdock defines two types of pages. Default and Slide.\n Default is the common page like the current one you are reading. Slide is a page that use the full screen to display its markdown content as a reveals.js presentation. HomePage is a special content that will be displayed as home page content.  To tell Hugo-theme-docdock to consider a page as a slide, just add a type=\u0026quot;slide\u0026quot;in then frontmatter of your file.  read more on page as slide\nHugo-theme-docdock provides archetypes to help you create this kind of pages.\nFront Matter Each Hugo page has to define a Front Matter in yaml, toml or json.\nHugo-theme-docdock uses the following parameters on top of the existing ones :\n+++ # Type of content, set \u0026quot;slide\u0026quot; to display it fullscreen with reveal.js type=\u0026quot;page\u0026quot; # Creator's Display name creatordisplayname = \u0026quot;Valere JEANTET\u0026quot; # Creator's Email creatoremail = \u0026quot;valere.jeantet@gmail.com\u0026quot; # LastModifier's Display name lastmodifierdisplayname = \u0026quot;Valere JEANTET\u0026quot; # LastModifier's Email lastmodifieremail = \u0026quot;valere.jeantet@gmail.com\u0026quot; +++  Ordering Hugo provides a flexible way to handle order for your pages.\nThe simplest way is to use weight parameter in the front matter of your page.\n Read more on content organization\n"
},
{
	"uri": "https://shengzhenfu.github.io/big-data/",
	"title": "Big - Data",
	"tags": [],
	"description": "",
	"content": " Docs about Big data Let\u0026rsquo;s get started to explore Big data technology  it’s that simple\n 3nodes-Hadoop-installation-setup Setup 3 nodes Hadoop Hadoop Introduction Hadoop is an open-source Apache project that allows creation of parallel processing applications on large data sets, distributed across networked nodes. It\u0026rsquo;s composed of the Hadoop Distributed File System (HDFS™) that handles scalability and redundancy of data across nodes, and Hadoop YARN: a framework for job scheduling that executes data processing tasks on all nodes. Before You Begin Follow the Getting Started guide to create three (3) nodes.\n  Learning Big Data A comprehensive guide to Big Data\n  "
},
{
	"uri": "https://shengzhenfu.github.io/content-organisation/",
	"title": "Content Organisation - Doc hub",
	"tags": [],
	"description": "",
	"content": " With Hugo, pages are the core of your site. Organize your site like any other Hugo project. Magic occurs with the nested sections implemention done in v0.22 of hugo (congrats @bep).\nWith docdock, Each content page composes the menu, they shape the structure of your website.\nTo link pages to each other, place them in a folders hierarchy\ncontent ├── level-one │ ├── level-two │ │ ├── level-three │ │ │ ├── level-four │ │ │ │ ├── _index.md │ │ │ │ ├── page-4-a.md │ │ │ │ ├── page-4-b.md │ │ │ │ └── page-4-c.md │ │ │ ├── _index.md │ │ │ ├── page-3-a.md │ │ │ ├── page-3-b.md │ │ │ └── page-3-c.md │ │ ├── _index.md │ │ ├── page-2-a.md │ │ ├── page-2-b.md │ │ └── page-2-c.md │ ├── _index.md │ ├── page-1-a.md │ ├── page-1-b.md │ └── page-1-c.md ├── _index.md └── page-top.md  _index.md is required in each folder, it\u0026rsquo;s your \u0026ldquo;folder home page\u0026rdquo; Add header to a menu entry in the page frontmatter, add a head param to insert any HTML code before the menu entry:\nexample to display a \u0026ldquo;Hello\u0026rdquo;\n+++ title = \u0026quot;Github repo\u0026quot; head =\u0026quot;\u0026lt;label\u0026gt;Hello\u0026lt;/label\u0026gt; \u0026quot; +++  Add icon to a menu entry in the page frontmatter, add a pre param to insert any HTML code before the menu label:\nexample to display a github icon\n+++ title = \u0026quot;Github repo\u0026quot; pre =\u0026quot;\u0026lt;i class='fa fa-github'\u0026gt;\u0026lt;/i\u0026gt; \u0026quot; +++   \" name = \"Github repo\" +++ -- Look at the menu \u0026ldquo;Create Page/About images\u0026rdquo; which redirects to \u0026ldquo;Shortcodes/image -- Order sibling menu/page entries in your frontmatter add weight param with a number to order.\n+++ title=\u0026quot;My page\u0026quot; weight = 4 +++  add ordersectionsby = \u0026quot;title\u0026quot; in your config.toml to order menu entries by title Hide a menu entry in your frontmatter add hidden=true param.\n+++ title=\u0026quot;My page\u0026quot; hidden = true +++  Unfolded menu entry by default One or more menuentries can be displayed unfolded by default. (like the \u0026ldquo;Getting start\u0026rdquo; menu entry in this website)\nin your frontmatter add alwaysopen=true param. example :\ntitle = \u0026quot;Getting start\u0026quot; description = \u0026quot;\u0026quot; weight = 1 alwaysopen = true  Folder structure and file name Content organization is your content folder structure.\nHomepage Find out how to customize homepage\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/",
	"title": "Shortcodes - Doc hub",
	"tags": [],
	"description": "",
	"content": "A bunch of Shortcodes are available with this theme :\n alert The alert shortcode allows you to highlight information in your page.\n  attachments The Attachments shortcode displays a list of files attached to a page.\n  button Display an actionable button in your page.\n  children List the child pages of a page\n  excerpt The Excerpt shortcode is used to mark a part of a page\u0026#39;s content for re-use.\n  excerpt-include The Excerpt Include shortcode is used to display \u0026lsquo;excerpted\u0026rsquo; (that is, a segment of) content from one page in another. Before you can use this shortcode, the excerpt must have been defined using the Excerpt shortcode. Note that you can have more than one Excerpt Include shortcode on a page (although you can have only one Excerpt shortcode on a page). Usage Parameter Default Description filename required Type the filename of the page that contains the excerpt to be displayed.\n  expand Displays an expandable/collapsible section of text on your page\n  icon Display an icon.\n  mermaid Flowchart example Show code ... {{\u0026lt;mermaid align=\u0026quot;left\u0026quot;\u0026gt;}} graph LR; A[Hard edge] --\u0026gt;|Link text| B(Round edge) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result one] C --\u0026gt;|Two| E[Result two] {{\u0026lt; /mermaid \u0026gt;}} mermaid.initialize({startOnLoad:true}); graph LR; A[Hard edge] --|Link text| B(Round edge) B -- C{Decision} C --|One| D[Result one] C --|Two| E[Result two] With sub-graphs and some style Show code... {{\u0026lt;mermaid align=\u0026quot;left\u0026quot;\u0026gt;}} graph LR; X --\u0026gt; Y linkStyle 0 stroke:#f00,stroke-width:4px; Y --\u0026gt; Z Z --\u0026gt; X linkStyle 1,2 interpolate basis stroke:#0f0,stroke-width:2px; X --\u0026gt; A1 subgraph right A2 --\u0026gt; B2 B2 --\u0026gt; C2 end subgraph left A1 --\u0026gt; B1 B1 --\u0026gt; C1 end C1 --\u0026gt; X Z --\u0026gt; A2 C2 --\u0026gt; Z style Y fill:#f9f,stroke:#333,stroke-width:4px classDef left fill:#ccf,stroke:#f66,stroke-width:2px,stroke-dasharray: 5, 5 class A1,B1,C1 left {{\u0026lt; /mermaid \u0026gt;}} mermaid.\n  notice Disclaimers to help you structure your page\n  panel Allow you to highlight information or put it in a box.\n  revealjs present content as a reveal.js slide\n  "
},
{
	"uri": "https://shengzhenfu.github.io/search/",
	"title": "About the Search Engine - Doc hub",
	"tags": [],
	"description": "",
	"content": " Activate search If not already present, add the follow lines to the config.toml file.\n[outputs] home = [ \u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] Docdock theme uses the last improvement available in hugo version 20+ to generate a json index file ready to be consumed by lunr.js javascript search engine.\n hugo generate lunrjs index.json at the root of public folder if the site only has one language or within each language subfolder. When you build the site with hugo server, hugo generates it internally and of course it don\u0026rsquo;t show up in the filesystem "
},
{
	"uri": "https://shengzhenfu.github.io/create-page/homepage/",
	"title": "Home page",
	"tags": ["tag1", "tag2"],
	"description": "",
	"content": "To tell Hugo-theme-docdock to consider a page as homepage\u0026rsquo;s content, just create a content file named _index.md in content folder.\n"
},
{
	"uri": "https://shengzhenfu.github.io/create-page/page-images/",
	"title": "About images",
	"tags": [],
	"description": "",
	"content": " Images have a similar syntax to links but include a preceding exclamation point.\n![agence](https://github.com/vjeantet/vjeantet.fr/raw/master/static/images/sgthon/C.jpg)  Resizing image Add HTTP parameters width and/or height to the link image to resize the image. Values are CSS values (default is auto).\n![Hackathon](https://github.com/vjeantet/vjeantet.fr/raw/master/static/images/sgthon/C.jpg?height=80px)  Add CSS classes Add a HTTP classes parameter to the link image to add CSS classes. shadow and border are available but you could define other ones.\n![s](https://github.com/vjeantet/vjeantet.fr/raw/master/static/images/sgthon/C.jpg?classes=border,shadow)  "
},
{
	"uri": "https://shengzhenfu.github.io/create-page/myslide/",
	"title": "My Slide ! fullscreen",
	"tags": [],
	"description": "",
	"content": " In the morning Getting up  Turn off alarm Get out of bed  Breakfast  Eat eggs Drink coffee  In the evening Dinner  Eat spaghetti Drink wine  Going to sleep  Get in bed Count sheep  "
},
{
	"uri": "https://shengzhenfu.github.io/create-page/page-slide/",
	"title": "Present a Slide",
	"tags": [],
	"description": "",
	"content": " A basic md content page can be rendered as a reveal.js presentation full screen.\nYou can, also, embed presentation in a page as a small box, using the revealjs shortcode in your md file. Formating Use your common Markdown syntax you use in Hugo, don\u0026rsquo;t forget, you can put html tags too.\nSpecial syntax (in html comment) is available for adding attributes to Markdown elements. This is useful for fragments, amongst other things.\n Please read the  doc from hakimel\nOptions In the frontmatter of your page file, set type and revealOptions params\nYour content will be served as a fullscreen revealjs presentation and revealOptions will be used to ajust its behaviour.\n+++ title = \u0026quot;Test slide\u0026quot; type=\u0026quot;slide\u0026quot; theme = \u0026quot;league\u0026quot; [revealOptions] transition= 'concave' controls= true progress= true history= true center= true +++  read more about reveal options here\nSlide Delimiters When creating the content for your slideshow presentation within content markdown file you need to be able to distinguish between one slide and the next. This is achieved very simply using a convention within Markdown that indicates the start of each new slide.\nAs both horizontal and vertical slides are supported by reveal.js each has it\u0026rsquo;s own unique delimiter.\nTo denote the start of a horizontal slide simply add the following delimiter (dashes) in your Markdown:\n---  To denote the start of a vertical slide simply add the following delimiter (underscores) in your Markdown:\n___  By using a combination of horizontal and vertical slides you can customize the navigation within your slideshow presentation. Typically vertical slides are used to present information below a top-level horizontal slide.\nFor example, a very simple slideshow presentation can be created as follows\n+++ title = \u0026quot;test\u0026quot; date = \u0026quot;2017-04-24T18:36:24+02:00\u0026quot; type=\u0026quot;slide\u0026quot; theme = \u0026quot;league\u0026quot; [revealOptions] transition= 'concave' controls= true progress= true history= true center= true +++ # In the morning ___ ## Getting up - Turn off alarm - Get out of bed ___ ## Breakfast - Eat eggs - Drink coffee --- # In the evening ___ ## Dinner - Eat spaghetti - Drink wine ___ ## Going to sleep - Get in bed - Count sheep   click here to view this page rendered\n"
},
{
	"uri": "https://shengzhenfu.github.io/_footer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Doc hub @Shengzhen.Fu\n"
},
{
	"uri": "https://shengzhenfu.github.io/_header/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Doc hub @Shengzhen.Fu\n"
},
{
	"uri": "https://shengzhenfu.github.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Doc hub Welcome to visit Doc hub, hope you enjoy your time here\n"
},
{
	"uri": "https://shengzhenfu.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://shengzhenfu.github.io/credits/",
	"title": "Credits",
	"tags": [],
	"description": "contributors and packages used by hugo-theme-docdock",
	"content": " github contributors contributors:\nPackages and libraries  Bootstrap - front-end framework mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; JavaScript-autoComplete - An extremely lightweight and powerful vanilla JavaScript completion suggester. clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support reveal-js - The HTML Presentation Framework  Tooling  Netlify - Continuous deployement and hosting of this documentation Hugo    "
},
{
	"uri": "https://shengzhenfu.github.io/content-organisation/customize-style/disable/",
	"title": "Disable features",
	"tags": [],
	"description": "",
	"content": " You can disable feature in docdock by changing some params in config.toml\nhide Next / Prev Chevrons [params] disableNavChevron = true  hide Search box in side menu [params] disableSearch = true  hide the  icon in side bar [params] disableHomeIcon = true  "
},
{
	"uri": "https://shengzhenfu.github.io/showcase/",
	"title": "Docdock-built Sites",
	"tags": [],
	"description": "Hugo-built Sites with docdock theme",
	"content": " https://invincible.site/ by @shazic https://bitfan.io/ by @vjeantet "
},
{
	"uri": "https://shengzhenfu.github.io/big-data/learning-bigdata/",
	"title": "Learning Big Data",
	"tags": [],
	"description": "A comprehensive guide to Big Data",
	"content": "In this collection of Big Data documents, you will learn from the beginning and have a comprehensive understanding after total 15 topics.\nEnjoy your time in Big data :)\n"
},
{
	"uri": "https://shengzhenfu.github.io/tags/tag1/",
	"title": "Tag1",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://shengzhenfu.github.io/tags/tag2/",
	"title": "Tag2",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://shengzhenfu.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://shengzhenfu.github.io/content-organisation/customize-style/themestyle/",
	"title": "Theme styles",
	"tags": [],
	"description": "change theme style",
	"content": " In site configuration file, you can set a subtheme name of this theme to load a specific css.\nadd a param themeStyle = \u0026quot;STYLE_NAME\u0026quot; in the [params] part of config.toml file.\nStyle \u0026ldquo;original\u0026rdquo; [params] themeStyle = \u0026quot;original\u0026quot;  Style \u0026ldquo;flex\u0026rdquo; (work in progress) [params] themeStyle = \u0026quot;flex\u0026quot;  "
},
{
	"uri": "https://shengzhenfu.github.io/content-organisation/customize-style/theme-variants/",
	"title": "Theme variants",
	"tags": [],
	"description": "change theme style/colors",
	"content": " In site configuration file, you can set a variant name of this theme to load a specific css, with different color specifications.\nadd a param themeVariant = \u0026quot;VARIANT_NAME\u0026quot; in the [params] part of config.toml file.\nAvailable variants change only colors at this moment Variant \u0026ldquo;gray\u0026rdquo; [params] themeVariant = \u0026quot;gray\u0026quot;  Variant \u0026ldquo;gold\u0026rdquo; [params] themeVariant = \u0026quot;gold\u0026quot;  Variant \u0026ldquo;green\u0026rdquo; [params] themeVariant = \u0026quot;green\u0026quot;`  "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/alert/",
	"title": "alert",
	"tags": [],
	"description": "The alert shortcode allows you to highlight information in your page.",
	"content": " The alert shortcode allow you to highlight information in your page. They create a colored box surrounding your text, like this:\nThis is an alert ! Usage    Parameter Default Description     theme info success, info,warning,danger    Tips : setting only the theme as argument works too : {{%alert warning%}} instead of {{%alert theme=\u0026quot;warning\u0026quot;%}}\n Basic examples {{% alert theme=\u0026quot;info\u0026quot; %}}**this** is a text{{% /alert %}} {{% alert theme=\u0026quot;success\u0026quot; %}}**Yeahhh !** is a text{{% /alert %}} {{% alert theme=\u0026quot;warning\u0026quot; %}}**Be carefull** is a text{{% /alert %}} {{% alert theme=\u0026quot;danger\u0026quot; %}}**Beware !** is a text{{% /alert %}}  this is an info Yeahhh ! is an success Be carefull is a warning Beware ! is a danger "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/attachments/",
	"title": "attachments",
	"tags": [],
	"description": "The Attachments shortcode displays a list of files attached to a page.",
	"content": " The Attachments shortcode displays a list of files attached to a page. Example :   Attachments   ISO27001.pdf  (471 kB)   screen_dash.jpg  (162 kB)   知足.pdf  (134 kB)    Usage The shortcurt lists files found in a specific folder. Currently, it support two implementations for pages\n If your page is a markdown file, attachements must be place in a folder named like your page and ending with .files.\n  content  _index.md page.files  attachment.pdf  page.md    If your page is a folder, attachements must be place in a nested \u0026lsquo;files\u0026rsquo; folder.\n  content  _index.md page  index.md files  attachment.pdf       That\u0026rsquo;s all !\nTip : Look at this documentation source code on github parameters    Parameter Default Description     title \u0026ldquo;Attachments\u0026rdquo; List\u0026rsquo;s title   pattern \u0026rdquo;.*\u0026rdquo; A regular expressions, used to filter the attachments by file name. The pattern parameter value must be regular expressions.\nFor example:\n To match a file suffix of \u0026lsquo;jpg\u0026rsquo;, use .*jpg (not *.jpg). To match file names ending in \u0026lsquo;jpg\u0026rsquo; or \u0026lsquo;png\u0026rsquo;, use .*(jpg|png)       Demo List of attachments ending in pdf or mp4 {{%attachments title=\u0026quot;Related files\u0026quot; pattern=\u0026quot;.*(pdf|mp4)\u0026quot;/%}}  renders as\n  Related files   ISO27001.pdf  (471 kB)   知足.pdf  (134 kB)   "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/button/",
	"title": "button",
	"tags": [],
	"description": "Display an actionable button in your page.",
	"content": " Display an actionable button in your page.\n This is a warning button   Usage    Parameter Default Description     href \u0026rdquo;\u0026rdquo; The location href to link to   align \u0026ldquo;center\u0026rdquo; horizontal align button on page   theme primary default, primary , success,info,warning,danger    The inner text you place in short code will be displayed as the button text\nDemo {{\u0026lt; button href=\u0026quot;https://google.com\u0026quot; \u0026gt;}} go to google {{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026quot;https://google.com\u0026quot; theme=\u0026quot;success\u0026quot; \u0026gt;}} Success {{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026quot;https://google.com\u0026quot; theme=\u0026quot;info\u0026quot; \u0026gt;}} Info {{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026quot;https://google.com\u0026quot; theme=\u0026quot;warning\u0026quot; \u0026gt;}} Warning {{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026quot;https://google.com\u0026quot; theme=\u0026quot;danger\u0026quot; \u0026gt;}} Danger ! {{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026quot;https://google.com\u0026quot; theme=\u0026quot;default\u0026quot; \u0026gt;}} Danger ! {{\u0026lt; /button \u0026gt;}}   go to google  Success  Info  Warning  Danger !  Danger !  "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/",
	"title": "children",
	"tags": [],
	"description": "List the child pages of a page",
	"content": " Use the children shortcode to list the child pages of a page and the further descendants (children\u0026rsquo;s children). By default, the shortcode displays links to the child pages.\nUsage    Parameter Default Description     page current Specify the page name (section name) to display children for   style \u0026ldquo;li\u0026rdquo; Choose the style used to display descendants. It could be any HTML tag name, use \u0026ldquo;card\u0026rdquo; to display children pages as cards   showhidden \u0026ldquo;false\u0026rdquo; When true, child pages hidden from the menu will be displayed   description \u0026ldquo;false\u0026rdquo; Allows you to include a short text under each page in the list.when no description exists for the page, children shortcode takes the first 70 words of your content. read more info about summaries on gohugo.io   depth 1 Enter a number to specify the depth of descendants to display. For example, if the value is 2, the shortcode will display 2 levels of child pages. Tips: set 999 to get all descendants    sort none Sort Children By\nWeight - to sort on menu orderName - to sort alphabetically on menu labelIdentifier - to sort alphabetically on identifier set in frontmatterURL - URL    Demo {{% children %}}   page 1   page 2   page 3   page test   {{% children description=\u0026quot;true\u0026quot; %}}   page 1 This is a demo child page\n  page 2 Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  page 3 This is a demo child page\n  page test This is a page test\n  {{% children depth=\u0026quot;3\u0026quot; showhidden=\u0026quot;true\u0026quot; %}}   page 1    page 1-1    page 1-1-1     page 2  \n page test 3    \n page 3   \n page 4   \n page test    \n{{% children style=\u0026quot;h2\u0026quot; depth=\u0026quot;3\u0026quot; description=\u0026quot;true\u0026quot; %}}   page 1 This is a demo child page\n  page 1-1 Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod\n  page 1-1-1 This is a demo child page\n  page 2 Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  page test 3 This is a page test\n  page 3 This is a demo child page\n  page test This is a page test\n  {{% children style=\u0026quot;div\u0026quot; depth=\u0026quot;999\u0026quot; %}}   page 1   page 1-1   page 1-1-1   page 1-1-1-1   page 1-1-1-1-1   page 2   page test 3   page 3   page test   {{% children style=\u0026quot;card\u0026quot; depth=\u0026quot;2\u0026quot; description=\u0026quot;true\u0026quot; %}}   page 1 This is a demo child page\n  page 1-1 Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod\n  page 2 Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  page test 3 This is a page test\n  page 3 This is a demo child page\n  page test This is a page test\n  "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/excerpt/",
	"title": "excerpt",
	"tags": [],
	"description": "The Excerpt shortcode is used to mark a part of a page&#39;s content for re-use.",
	"content": " The Excerpt shortcode is used to mark a part of a page\u0026rsquo;s content for re-use. Defining an excerpt enables other shortcodes, such as the excerpt-include shortcode, to display the marked content elsewhere.\nYou can only define one excerpt per page. In other words, you can only add the Excerpt shortcode once to a page. Usage    Parameter Default Description     hidden \u0026ldquo;false\u0026rdquo; Controls whether the page content contained in the Excerpt shortcode placeholder is displayed on the page.Note that this option affects only the page that contains the Excerpt shortcode. It does not affect any pages where the content is reused.     Demo {{%excerpt%}} Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation **ullamco** laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in _reprehenderit in voluptate_ cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. {{% /excerpt%}}  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nSee re use example with excerpt-include shortcode "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/excerpt-include/",
	"title": "excerpt-include",
	"tags": [],
	"description": "",
	"content": " The Excerpt Include shortcode is used to display \u0026lsquo;excerpted\u0026rsquo; (that is, a segment of) content from one page in another. Before you can use this shortcode, the excerpt must have been defined using the Excerpt shortcode. Note that you can have more than one Excerpt Include shortcode on a page (although you can have only one Excerpt shortcode on a page). Usage    Parameter Default Description     filename required Type the filename of the page that contains the excerpt to be displayed.Path is relative to the content folder   panel none Determines whether docDock will display a panel around the excerpted content. The panel includes the given panel\u0026rsquo;s value and the border of the panel. By default, the panel and title are not shown.    Demo The paragraph below shows an example of an Excerpt Include shortcode, containing content from an excerpt which we have defined on the Excerpt shortcode page. On the Excerpt Include shortcode below, we have set the options to show both the title of the page and the panel surrounding the content.\n{{%excerpt-include filename=\u0026quot;shortcodes/excerpt.md\u0026quot; panel=\u0026quot;From excerpt page\u0026quot; /%}}  From excerpt page Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.    Don\u0026rsquo;t create an excerpt file in the /layouts/shortcodes/ folder. In this example, shortcodes/filename.md is part of the exampleSite/content folder, and shortcodes is just a path in the /content.\n "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/expand/",
	"title": "expand",
	"tags": [],
	"description": "Displays an expandable/collapsible section of text on your page",
	"content": " The Expand shortcode displays an expandable/collapsible section of text on your page. Here is an example\n  Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  Usage this shortcode takes exactly one optional parameter to define the text that appears next to the expand/collapse icon. (default is \u0026ldquo;Expand me\u0026hellip;\u0026rdquo;)\n{{%expand \u0026quot;Is this docdock theme rocks ?\u0026quot; %}}Yes !.{{% /expand%}}    Is this docdock theme rocks ?   Yes !  \nDemo {{%expand%}}Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.{{% /expand%}}    Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/icon/",
	"title": "icon",
	"tags": [],
	"description": "Display an icon.",
	"content": " Display an icon like   \nIt uses :\n glyphicons library (bootstrap).more info here fontawesome library more info here  This icon shortcode will display an icon in your page.\nUsage    Parameter Default Description     name required name of icon (see bellow)   size none size of icon, medium, xx-small, x-small, small, large, x-large, xx-large, 11px, 2em, 20%\u0026hellip;.    Tips : setting only the name as argument works too : {{\u0026lt;icon film\u0026gt;}} instead of {{\u0026lt;icon name=\u0026quot;film\u0026quot;\u0026gt;}}\n Demo {{\u0026lt; icon name=\u0026quot;film\u0026quot; size=\u0026quot;large\u0026quot; \u0026gt;}}   .\nicons available    asterisk   plus   euro   eur   minus   cloud   envelope   pencil   glass   music   search   heart   star   star-empty   user   film   th-large   th   th-list   ok   remove   zoom-in   zoom-out   off   signal   cog   trash   home   file   time   road   download-alt   download   upload   inbox   play-circle   repeat   refresh   list-alt   lock   flag   headphones   volume-off   volume-down   volume-up   qrcode   barcode   tag   tags   book   bookmark   print   camera   font   bold   italic   text-height   text-width   align-left   align-center   align-right   align-justify   list   indent-left   indent-right   facetime-video   picture   map-marker   adjust   tint   edit   share   check   move   step-backward   fast-backward   backward   play   pause   stop   forward   fast-forward   step-forward   eject   chevron-left   chevron-right   plus-sign   minus-sign   remove-sign   ok-sign   question-sign   info-sign   screenshot   remove-circle   ok-circle   ban-circle   arrow-left   arrow-right   arrow-up   arrow-down   share-alt   resize-full   resize-small   exclamation-sign   gift   leaf   fire   eye-open   eye-close   warning-sign   plane   calendar   random   comment   magnet   chevron-up   chevron-down   retweet   shopping-cart   folder-close   folder-open   resize-vertical   resize-horizontal   hdd   bullhorn   bell   certificate   thumbs-up   thumbs-down   hand-right   hand-left   hand-up   hand-down   circle-arrow-right   circle-arrow-left   circle-arrow-up   circle-arrow-down   globe   wrench   tasks   filter   briefcase   fullscreen   dashboard   paperclip   heart-empty   link   phone   pushpin   usd   gbp   sort   sort-by-alphabet   sort-by-alphabet-alt   sort-by-order   sort-by-order-alt   sort-by-attributes   sort-by-attributes-alt   unchecked   expand   collapse-down   collapse-up   log-in   flash   log-out   new-window   record   save   open   saved   import   export   send   floppy-disk   floppy-saved   floppy-remove   floppy-save   floppy-open   credit-card   transfer   cutlery   header   compressed   earphone   phone-alt   tower   stats   sd-video   hd-video   subtitles   sound-stereo   sound-dolby   sound-5-1   sound-6-1   sound-7-1   copyright-mark   registration-mark   cloud-download   cloud-upload   tree-conifer   tree-deciduous   cd   save-file   open-file   level-up   copy   paste   alert   equalizer   king   queen   pawn   bishop   knight   baby-formula   tent   blackboard   bed   apple   erase   hourglass   lamp   duplicate   piggy-bank   scissors   bitcoin   btc   xbt   yen   jpy   ruble   rub   scale   ice-lolly   ice-lolly-tasted   education   option-horizontal   option-vertical   menu-hamburger   modal-window   oil   grain   sunglasses   text-size   text-color   text-background   object-align-top   object-align-bottom   object-align-horizontal   object-align-left   object-align-vertical   object-align-right   triangle-right   triangle-left   triangle-bottom   triangle-top   console   superscript   subscript   menu-left   menu-right   menu-down   menu-up  Web Application Icons    fa-address-book   fa-address-book-o   fa-address-card   fa-address-card-o   fa-adjust   fa-american-sign-language-interpreting   fa-anchor   fa-archive   fa-area-chart   fa-arrows   fa-arrows-h   fa-arrows-v   fa-asl-interpreting (alias)   fa-assistive-listening-systems   fa-asterisk   fa-at   fa-audio-description   fa-automobile (alias)   fa-balance-scale   fa-ban   fa-bank (alias)   fa-bar-chart   fa-bar-chart-o (alias)   fa-barcode   fa-bars   fa-bath   fa-bathtub (alias)   fa-battery (alias)   fa-battery-0 (alias)   fa-battery-1 (alias)   fa-battery-2 (alias)   fa-battery-3 (alias)   fa-battery-4 (alias)   fa-battery-empty   fa-battery-full   fa-battery-half   fa-battery-quarter   fa-battery-three-quarters   fa-bed   fa-beer   fa-bell   fa-bell-o   fa-bell-slash   fa-bell-slash-o   fa-bicycle   fa-binoculars   fa-birthday-cake   fa-blind   fa-bluetooth   fa-bluetooth-b   fa-bolt   fa-bomb   fa-book   fa-bookmark   fa-bookmark-o   fa-braille   fa-briefcase   fa-bug   fa-building   fa-building-o   fa-bullhorn   fa-bullseye   fa-bus   fa-cab (alias)   fa-calculator   fa-calendar   fa-calendar-check-o   fa-calendar-minus-o   fa-calendar-o   fa-calendar-plus-o   fa-calendar-times-o   fa-camera   fa-camera-retro   fa-car   fa-caret-square-o-down   fa-caret-square-o-left   fa-caret-square-o-right   fa-caret-square-o-up   fa-cart-arrow-down   fa-cart-plus   fa-cc   fa-certificate   fa-check   fa-check-circle   fa-check-circle-o   fa-check-square   fa-check-square-o   fa-child   fa-circle   fa-circle-o   fa-circle-o-notch   fa-circle-thin   fa-clock-o   fa-clone   fa-close (alias)   fa-cloud   fa-cloud-download   fa-cloud-upload   fa-code   fa-code-fork   fa-coffee   fa-cog   fa-cogs   fa-comment   fa-comment-o   fa-commenting   fa-commenting-o   fa-comments   fa-comments-o   fa-compass   fa-copyright   fa-creative-commons   fa-credit-card   fa-credit-card-alt   fa-crop   fa-crosshairs   fa-cube   fa-cubes   fa-cutlery   fa-dashboard (alias)   fa-database   fa-deaf   fa-deafness (alias)   fa-desktop   fa-diamond   fa-dot-circle-o   fa-download   fa-drivers-license (alias)   fa-drivers-license-o (alias)   fa-edit (alias)   fa-ellipsis-h   fa-ellipsis-v   fa-envelope   fa-envelope-o   fa-envelope-open   fa-envelope-open-o   fa-envelope-square   fa-eraser   fa-exchange   fa-exclamation   fa-exclamation-circle   fa-exclamation-triangle   fa-external-link   fa-external-link-square   fa-eye   fa-eye-slash   fa-eyedropper   fa-fax   fa-feed (alias)   fa-female   fa-fighter-jet   fa-file-archive-o   fa-file-audio-o   fa-file-code-o   fa-file-excel-o   fa-file-image-o   fa-file-movie-o (alias)   fa-file-pdf-o   fa-file-photo-o (alias)   fa-file-picture-o (alias)   fa-file-powerpoint-o   fa-file-sound-o (alias)   fa-file-video-o   fa-file-word-o   fa-file-zip-o (alias)   fa-film   fa-filter   fa-fire   fa-fire-extinguisher   fa-flag   fa-flag-checkered   fa-flag-o   fa-flash (alias)   fa-flask   fa-folder   fa-folder-o   fa-folder-open   fa-folder-open-o   fa-frown-o   fa-futbol-o   fa-gamepad   fa-gavel   fa-gear (alias)   fa-gears (alias)   fa-gift   fa-glass   fa-globe   fa-graduation-cap   fa-group (alias)   fa-hand-grab-o (alias)   fa-hand-lizard-o   fa-hand-paper-o   fa-hand-peace-o   fa-hand-pointer-o   fa-hand-rock-o   fa-hand-scissors-o   fa-hand-spock-o   fa-hand-stop-o (alias)   fa-handshake-o   fa-hard-of-hearing (alias)   fa-hashtag   fa-hdd-o   fa-headphones   fa-heart   fa-heart-o   fa-heartbeat   fa-history   fa-home   fa-hotel (alias)   fa-hourglass   fa-hourglass-1 (alias)   fa-hourglass-2 (alias)   fa-hourglass-3 (alias)   fa-hourglass-end   fa-hourglass-half   fa-hourglass-o   fa-hourglass-start   fa-i-cursor   fa-id-badge   fa-id-card   fa-id-card-o   fa-image (alias)   fa-inbox   fa-industry   fa-info   fa-info-circle   fa-institution (alias)   fa-key   fa-keyboard-o   fa-language   fa-laptop   fa-leaf   fa-legal (alias)   fa-lemon-o   fa-level-down   fa-level-up   fa-life-bouy (alias)   fa-life-buoy (alias)   fa-life-ring   fa-life-saver (alias)   fa-lightbulb-o   fa-line-chart   fa-location-arrow   fa-lock   fa-low-vision   fa-magic   fa-magnet   fa-mail-forward (alias)   fa-mail-reply (alias)   fa-mail-reply-all (alias)   fa-male   fa-map   fa-map-marker   fa-map-o   fa-map-pin   fa-map-signs   fa-meh-o   fa-microchip   fa-microphone   fa-microphone-slash   fa-minus   fa-minus-circle   fa-minus-square   fa-minus-square-o   fa-mobile   fa-mobile-phone (alias)   fa-money   fa-moon-o   fa-mortar-board (alias)   fa-motorcycle   fa-mouse-pointer   fa-music   fa-navicon (alias)   fa-newspaper-o   fa-object-group   fa-object-ungroup   fa-paint-brush   fa-paper-plane   fa-paper-plane-o   fa-paw   fa-pencil   fa-pencil-square   fa-pencil-square-o   fa-percent   fa-phone   fa-phone-square   fa-photo (alias)   fa-picture-o   fa-pie-chart   fa-plane   fa-plug   fa-plus   fa-plus-circle   fa-plus-square   fa-plus-square-o   fa-podcast   fa-power-off   fa-print   fa-puzzle-piece   fa-qrcode   fa-question   fa-question-circle   fa-question-circle-o   fa-quote-left   fa-quote-right   fa-random   fa-recycle   fa-refresh   fa-registered   fa-remove (alias)   fa-reorder (alias)   fa-reply   fa-reply-all   fa-retweet   fa-road   fa-rocket   fa-rss   fa-rss-square   fa-s15 (alias)   fa-search   fa-search-minus   fa-search-plus   fa-send (alias)   fa-send-o (alias)   fa-server   fa-share   fa-share-alt   fa-share-alt-square   fa-share-square   fa-share-square-o   fa-shield   fa-ship   fa-shopping-bag   fa-shopping-basket   fa-shopping-cart   fa-shower   fa-sign-in   fa-sign-language   fa-sign-out   fa-signal   fa-signing (alias)   fa-sitemap   fa-sliders   fa-smile-o   fa-snowflake-o   fa-soccer-ball-o (alias)   fa-sort   fa-sort-alpha-asc   fa-sort-alpha-desc   fa-sort-amount-asc   fa-sort-amount-desc   fa-sort-asc   fa-sort-desc   fa-sort-down (alias)   fa-sort-numeric-asc   fa-sort-numeric-desc   fa-sort-up (alias)   fa-space-shuttle   fa-spinner   fa-spoon   fa-square   fa-square-o   fa-star   fa-star-half   fa-star-half-empty (alias)   fa-star-half-full (alias)   fa-star-half-o   fa-star-o   fa-sticky-note   fa-sticky-note-o   fa-street-view   fa-suitcase   fa-sun-o   fa-support (alias)   fa-tablet   fa-tachometer   fa-tag   fa-tags   fa-tasks   fa-taxi   fa-television   fa-terminal   fa-thermometer (alias)   fa-thermometer-0 (alias)   fa-thermometer-1 (alias)   fa-thermometer-2 (alias)   fa-thermometer-3 (alias)   fa-thermometer-4 (alias)   fa-thermometer-empty   fa-thermometer-full   fa-thermometer-half   fa-thermometer-quarter   fa-thermometer-three-quarters   fa-thumb-tack   fa-thumbs-down   fa-thumbs-o-down   fa-thumbs-o-up   fa-thumbs-up   fa-ticket   fa-times   fa-times-circle   fa-times-circle-o   fa-times-rectangle (alias)   fa-times-rectangle-o (alias)   fa-tint   fa-toggle-down (alias)   fa-toggle-left (alias)   fa-toggle-off   fa-toggle-on   fa-toggle-right (alias)   fa-toggle-up (alias)   fa-trademark   fa-trash   fa-trash-o   fa-tree   fa-trophy   fa-truck   fa-tty   fa-tv (alias)   fa-umbrella   fa-universal-access   fa-university   fa-unlock   fa-unlock-alt   fa-unsorted (alias)   fa-upload   fa-user   fa-user-circle   fa-user-circle-o   fa-user-o   fa-user-plus   fa-user-secret   fa-user-times   fa-users   fa-vcard (alias)   fa-vcard-o (alias)   fa-video-camera   fa-volume-control-phone   fa-volume-down   fa-volume-off   fa-volume-up   fa-warning (alias)   fa-wheelchair   fa-wheelchair-alt   fa-wifi   fa-window-close   fa-window-close-o   fa-window-maximize   fa-window-minimize   fa-window-restore   fa-wrench  Accessibility Icons    fa-american-sign-language-interpreting   fa-asl-interpreting (alias)   fa-assistive-listening-systems   fa-audio-description   fa-blind   fa-braille   fa-cc   fa-deaf   fa-deafness (alias)   fa-hard-of-hearing (alias)   fa-low-vision   fa-question-circle-o   fa-sign-language   fa-signing (alias)   fa-tty   fa-universal-access   fa-volume-control-phone   fa-wheelchair   fa-wheelchair-alt  Hand Icons    fa-hand-grab-o (alias)   fa-hand-lizard-o   fa-hand-o-down   fa-hand-o-left   fa-hand-o-right   fa-hand-o-up   fa-hand-paper-o   fa-hand-peace-o   fa-hand-pointer-o   fa-hand-rock-o   fa-hand-scissors-o   fa-hand-spock-o   fa-hand-stop-o (alias)   fa-thumbs-down   fa-thumbs-o-down   fa-thumbs-o-up   fa-thumbs-up  Transportation Icons    fa-ambulance   fa-automobile (alias)   fa-bicycle   fa-bus   fa-cab (alias)   fa-car   fa-fighter-jet   fa-motorcycle   fa-plane   fa-rocket   fa-ship   fa-space-shuttle   fa-subway   fa-taxi   fa-train   fa-truck   fa-wheelchair   fa-wheelchair-alt  Gender Icons    fa-genderless   fa-intersex (alias)   fa-mars   fa-mars-double   fa-mars-stroke   fa-mars-stroke-h   fa-mars-stroke-v   fa-mercury   fa-neuter   fa-transgender   fa-transgender-alt   fa-venus   fa-venus-double   fa-venus-mars  File Type Icons    fa-file   fa-file-archive-o   fa-file-audio-o   fa-file-code-o   fa-file-excel-o   fa-file-image-o   fa-file-movie-o (alias)   fa-file-o   fa-file-pdf-o   fa-file-photo-o (alias)   fa-file-picture-o (alias)   fa-file-powerpoint-o   fa-file-sound-o (alias)   fa-file-text   fa-file-text-o   fa-file-video-o   fa-file-word-o   fa-file-zip-o (alias)  Spinner Icons    fa-circle-o-notch   fa-cog   fa-gear (alias)   fa-refresh   fa-spinner  Form Control Icons    fa-check-square   fa-check-square-o   fa-circle   fa-circle-o   fa-dot-circle-o   fa-minus-square   fa-minus-square-o   fa-plus-square   fa-plus-square-o   fa-square   fa-square-o  Payment Icons    fa-cc-amex   fa-cc-diners-club   fa-cc-discover   fa-cc-jcb   fa-cc-mastercard   fa-cc-paypal   fa-cc-stripe   fa-cc-visa   fa-credit-card   fa-credit-card-alt   fa-google-wallet   fa-paypal  Chart Icons    fa-area-chart   fa-bar-chart   fa-bar-chart-o (alias)   fa-line-chart   fa-pie-chart  Currency Icons    fa-bitcoin (alias)   fa-btc   fa-cny (alias)   fa-dollar (alias)   fa-eur   fa-euro (alias)   fa-gbp   fa-gg   fa-gg-circle   fa-ils   fa-inr   fa-jpy   fa-krw   fa-money   fa-rmb (alias)   fa-rouble (alias)   fa-rub   fa-ruble (alias)   fa-rupee (alias)   fa-shekel (alias)   fa-sheqel (alias)   fa-try   fa-turkish-lira (alias)   fa-usd   fa-won (alias)   fa-yen (alias)  Text Editor Icons    fa-align-center   fa-align-justify   fa-align-left   fa-align-right   fa-bold   fa-chain (alias)   fa-chain-broken   fa-clipboard   fa-columns   fa-copy (alias)   fa-cut (alias)   fa-dedent (alias)   fa-eraser   fa-file   fa-file-o   fa-file-text   fa-file-text-o   fa-files-o   fa-floppy-o   fa-font   fa-header   fa-indent   fa-italic   fa-link   fa-list   fa-list-alt   fa-list-ol   fa-list-ul   fa-outdent   fa-paperclip   fa-paragraph   fa-paste (alias)   fa-repeat   fa-rotate-left (alias)   fa-rotate-right (alias)   fa-save (alias)   fa-scissors   fa-strikethrough   fa-subscript   fa-superscript   fa-table   fa-text-height   fa-text-width   fa-th   fa-th-large   fa-th-list   fa-underline   fa-undo   fa-unlink (alias)  Directional Icons    fa-angle-double-down   fa-angle-double-left   fa-angle-double-right   fa-angle-double-up   fa-angle-down   fa-angle-left   fa-angle-right   fa-angle-up   fa-arrow-circle-down   fa-arrow-circle-left   fa-arrow-circle-o-down   fa-arrow-circle-o-left   fa-arrow-circle-o-right   fa-arrow-circle-o-up   fa-arrow-circle-right   fa-arrow-circle-up   fa-arrow-down   fa-arrow-left   fa-arrow-right   fa-arrow-up   fa-arrows   fa-arrows-alt   fa-arrows-h   fa-arrows-v   fa-caret-down   fa-caret-left   fa-caret-right   fa-caret-square-o-down   fa-caret-square-o-left   fa-caret-square-o-right   fa-caret-square-o-up   fa-caret-up   fa-chevron-circle-down   fa-chevron-circle-left   fa-chevron-circle-right   fa-chevron-circle-up   fa-chevron-down   fa-chevron-left   fa-chevron-right   fa-chevron-up   fa-exchange   fa-hand-o-down   fa-hand-o-left   fa-hand-o-right   fa-hand-o-up   fa-long-arrow-down   fa-long-arrow-left   fa-long-arrow-right   fa-long-arrow-up   fa-toggle-down (alias)   fa-toggle-left (alias)   fa-toggle-right (alias)   fa-toggle-up (alias)  Video Player Icons    fa-arrows-alt   fa-backward   fa-compress   fa-eject   fa-expand   fa-fast-backward   fa-fast-forward   fa-forward   fa-pause   fa-pause-circle   fa-pause-circle-o   fa-play   fa-play-circle   fa-play-circle-o   fa-random   fa-step-backward   fa-step-forward   fa-stop   fa-stop-circle   fa-stop-circle-o   fa-youtube-play  Brand Icons    fa-500px   fa-adn   fa-amazon   fa-android   fa-angellist   fa-apple   fa-bandcamp   fa-behance   fa-behance-square   fa-bitbucket   fa-bitbucket-square   fa-bitcoin (alias)   fa-black-tie   fa-bluetooth   fa-bluetooth-b   fa-btc   fa-buysellads   fa-cc-amex   fa-cc-diners-club   fa-cc-discover   fa-cc-jcb   fa-cc-mastercard   fa-cc-paypal   fa-cc-stripe   fa-cc-visa   fa-chrome   fa-codepen   fa-codiepie   fa-connectdevelop   fa-contao   fa-css3   fa-dashcube   fa-delicious   fa-deviantart   fa-digg   fa-dribbble   fa-dropbox   fa-drupal   fa-edge   fa-eercast   fa-empire   fa-envira   fa-etsy   fa-expeditedssl   fa-fa (alias)   fa-facebook   fa-facebook-f (alias)   fa-facebook-official   fa-facebook-square   fa-firefox   fa-first-order   fa-flickr   fa-font-awesome   fa-fonticons   fa-fort-awesome   fa-forumbee   fa-foursquare   fa-free-code-camp   fa-ge (alias)   fa-get-pocket   fa-gg   fa-gg-circle   fa-git   fa-git-square   fa-github   fa-github-alt   fa-github-square   fa-gitlab   fa-gittip (alias)   fa-glide   fa-glide-g   fa-google   fa-google-plus   fa-google-plus-circle (alias)   fa-google-plus-official   fa-google-plus-square   fa-google-wallet   fa-gratipay   fa-grav   fa-hacker-news   fa-houzz   fa-html5   fa-imdb   fa-instagram   fa-internet-explorer   fa-ioxhost   fa-joomla   fa-jsfiddle   fa-lastfm   fa-lastfm-square   fa-leanpub   fa-linkedin   fa-linkedin-square   fa-linode   fa-linux   fa-maxcdn   fa-meanpath   fa-medium   fa-meetup   fa-mixcloud   fa-modx   fa-odnoklassniki   fa-odnoklassniki-square   fa-opencart   fa-openid   fa-opera   fa-optin-monster   fa-pagelines   fa-paypal   fa-pied-piper   fa-pied-piper-alt   fa-pied-piper-pp   fa-pinterest   fa-pinterest-p   fa-pinterest-square   fa-product-hunt   fa-qq   fa-quora   fa-ra (alias)   fa-ravelry   fa-rebel   fa-reddit   fa-reddit-alien   fa-reddit-square   fa-renren   fa-resistance (alias)   fa-safari   fa-scribd   fa-sellsy   fa-share-alt   fa-share-alt-square   fa-shirtsinbulk   fa-simplybuilt   fa-skyatlas   fa-skype   fa-slack   fa-slideshare   fa-snapchat   fa-snapchat-ghost   fa-snapchat-square   fa-soundcloud   fa-spotify   fa-stack-exchange   fa-stack-overflow   fa-steam   fa-steam-square   fa-stumbleupon   fa-stumbleupon-circle   fa-superpowers   fa-telegram   fa-tencent-weibo   fa-themeisle   fa-trello   fa-tripadvisor   fa-tumblr   fa-tumblr-square   fa-twitch   fa-twitter   fa-twitter-square   fa-usb   fa-viacoin   fa-viadeo   fa-viadeo-square   fa-vimeo   fa-vimeo-square   fa-vine   fa-vk   fa-wechat (alias)   fa-weibo   fa-weixin   fa-whatsapp   fa-wikipedia-w   fa-windows   fa-wordpress   fa-wpbeginner   fa-wpexplorer   fa-wpforms   fa-xing   fa-xing-square   fa-y-combinator   fa-y-combinator-square (alias)   fa-yahoo   fa-yc (alias)   fa-yc-square (alias)   fa-yelp   fa-yoast   fa-youtube   fa-youtube-play   fa-youtube-square  "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/mermaid/",
	"title": "mermaid",
	"tags": [],
	"description": "",
	"content": " Flowchart example   Show code ...   {{\u0026lt;mermaid align=\u0026quot;left\u0026quot;\u0026gt;}} graph LR; A[Hard edge] --\u0026gt;|Link text| B(Round edge) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result one] C --\u0026gt;|Two| E[Result two] {{\u0026lt; /mermaid \u0026gt;}}    mermaid.initialize({startOnLoad:true}); graph LR; A[Hard edge] --|Link text| B(Round edge) B -- C{Decision} C --|One| D[Result one] C --|Two| E[Result two] \nWith sub-graphs and some style   Show code...   {{\u0026lt;mermaid align=\u0026quot;left\u0026quot;\u0026gt;}} graph LR; X --\u0026gt; Y linkStyle 0 stroke:#f00,stroke-width:4px; Y --\u0026gt; Z Z --\u0026gt; X linkStyle 1,2 interpolate basis stroke:#0f0,stroke-width:2px; X --\u0026gt; A1 subgraph right A2 --\u0026gt; B2 B2 --\u0026gt; C2 end subgraph left A1 --\u0026gt; B1 B1 --\u0026gt; C1 end C1 --\u0026gt; X Z --\u0026gt; A2 C2 --\u0026gt; Z style Y fill:#f9f,stroke:#333,stroke-width:4px classDef left fill:#ccf,stroke:#f66,stroke-width:2px,stroke-dasharray: 5, 5 class A1,B1,C1 left {{\u0026lt; /mermaid \u0026gt;}}    mermaid.initialize({startOnLoad:true}); graph LR; X -- Y linkStyle 0 stroke:#f00,stroke-width:4px; Y -- Z Z -- X linkStyle 1,2 interpolate basis stroke:#0f0,stroke-width:2px; X -- A1 subgraph right A2 -- B2 B2 -- C2 end subgraph left A1 -- B1 B1 -- C1 end C1 -- X Z -- A2 C2 -- Z style Y fill:#f9f,stroke:#333,stroke-width:4px classDef left fill:#ccf,stroke:#f66,stroke-width:2px,stroke-dasharray: 5, 5 class A1,B1,C1 left \nSequence example   Show code ...   {{\u0026lt;mermaid\u0026gt;}} sequenceDiagram participant Alice participant Bob Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good! {{\u0026lt; /mermaid \u0026gt;}}   \nmermaid.initialize({startOnLoad:true}); sequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good! \nGANTT Example   Show code ...   {{\u0026lt;mermaid\u0026gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d {{\u0026lt; /mermaid \u0026gt;}}    mermaid.initialize({startOnLoad:true}); gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d \n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/notice/",
	"title": "notice",
	"tags": [],
	"description": "Disclaimers to help you structure your page",
	"content": " The notice shortcode shows 4 types of disclaimers to help you structure your page.\nNote {{% notice note %}} A notice disclaimer {{% /notice %}}  renders as\nA notice disclaimer\n Info {{% notice info %}} An information disclaimer {{% /notice %}}  renders as\nAn information disclaimer\n Tip {{% notice tip %}} A tip disclaimer {{% /notice %}}  renders as\nA tip disclaimer\n Warning {{% notice warning %}} An warning disclaimer {{% /notice %}}  renders as\nAn warning disclaimer\n "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/children-1/",
	"title": "page 1",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/children-1/children-1-1/",
	"title": "page 1-1",
	"tags": [],
	"description": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/children-1/children-1-1/children-1-1-1/",
	"title": "page 1-1-1",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/children-1/children-1-1/children-1-1-1/children-1-1-1-1/",
	"title": "page 1-1-1-1",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/children-1/children-1-1/children-1-1-1/children-1-1-1-1/children-1-1-1-1-1/",
	"title": "page 1-1-1-1-1",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/children-2/",
	"title": "page 2",
	"tags": [],
	"description": "",
	"content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/children-3/",
	"title": "page 3",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/children-4/",
	"title": "page 4",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page, not displayed in the menu\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/test/",
	"title": "page test",
	"tags": [],
	"description": "This is a page test",
	"content": "This is a test demo child page\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/children/children-2/test3/",
	"title": "page test 3",
	"tags": [],
	"description": "This is a page test",
	"content": "This is a test 3 demo child page\n"
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/panel/",
	"title": "panel",
	"tags": [],
	"description": "Allow you to highlight information or put it in a box.",
	"content": " The panel shortcode Allow you to highlight information or put it in a box. They create a colored box surrounding your text  Usage    Parameter Default Description     header none The title of the panel. If specified, this title will be displayed in its own header row.   footer none the footer of the panel. If specified, this text will be displayed in its own row   theme primary default,primary,info,success,warning,danger    Basic example By default :\n{{% panel %}}this is a panel text{{% /panel %}}  this is a panel text  Panel with heading Easily add a heading container to your panel with header parameter. You may apply any theme.\n{{% panel theme=\u0026quot;danger\u0026quot; header=\u0026quot;panel title\u0026quot; %}}this is a panel text{{% /panel %}}  panel title this is a panel text  {{% panel theme=\u0026quot;success\u0026quot; header=\u0026quot;panel title\u0026quot; %}}this is a panel text{{% /panel %}}  panel title this is a panel text  Panel with footer Wrap a secondary text in footer.\n{{% panel footer=\u0026quot;panel footer\u0026quot; %}}Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.{{% /panel %}}  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n panel footer  Themes Success theme this is a panel text  default theme this is a panel text  primary theme this is a panel text  info theme this is a panel text  warning theme this is a panel text  danger theme this is a panel text  "
},
{
	"uri": "https://shengzhenfu.github.io/shortcodes/revealjs/",
	"title": "revealjs",
	"tags": [],
	"description": "present content as a reveal.js slide",
	"content": " This shortcode will format the enclosed markdow to render it with reveal.js at runtime (client-side)\nRead more on revealjs github repo.\nUsage revealjs can use the following named parameters :\n theme transition controls progress history center  Important Even if the enclosed content is a mardown, use \u0026lt; shortcode notation instead of the % notation  Content formating and slide delimiters read more on this here\nDemo  # In the morning ___ ## Getting up - Turn off alarm - Get out of bed ___ ## Breakfast - Eat eggs - Drink coffee --- # In the evening ___ ## Dinner - Eat spaghetti - Drink wine ___ ## Going to sleep - Get in bed - Count sheep       function initSlides() { Reveal.initialize({ embedded : true, controls : false, center: true ,\thistory: false , progress: \"true\" , transition: \"concave\", dependencies: [ { src: '\\/revealjs\\/lib\\/js\\/classList.js\"', condition: function() { return !document.body.classList; } }, { src: '\\/revealjs\\/plugin\\/markdown\\/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } }, { src: '\\/revealjs\\/plugin\\/markdown\\/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } }, { src: '\\/revealjs\\/plugin\\/highlight\\/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }, { src: '\\/revealjs\\/plugin\\/zoom-js\\/zoom.js', async: true, condition: function() { return !!document.body.classList; } }, { src: '\\/revealjs\\/plugin\\/notes\\/notes.js', async: true, condition: function() { return !!document.body.classList; } } ] }); }   See it fullscreen var toto = document.getElementById('slideContent').innerHTML document.getElementById('slideFrame').contentWindow.document.write(document.getElementById('slideContent').innerHTML); document.getElementById('slideContent').remove(); document.addEventListener(\"DOMContentLoaded\",function(){ setTimeout(function () { document.getElementById('slideFrame').contentWindow.initSlides() ; }, 2000); }); function slideFullScreen() { document.open(); document.write(toto); document.close(); initSlides() }  Source :   Show code ...   {{\u0026lt;revealjs theme=\u0026quot;moon\u0026quot; progress=\u0026quot;true\u0026quot;\u0026gt;}} # In the morning ___ ## Getting up - Turn off alarm - Get out of bed ___ ## Breakfast - Eat eggs - Drink coffee --- # In the evening ___ ## Dinner - Eat spaghetti - Drink wine ___ ## Going to sleep - Get in bed - Count sheep {{\u0026lt;revealjs\u0026gt;}}   \n  click here to view raw content  "
}]